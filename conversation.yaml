local_system:
  commit_id: 6dd202450272931f39ce6c83b45cb02548c3ffb0
  attached_files:
  - src/analyze_grammar.rs
  - src/constraint.rs
  - src/glr/grammar.rs
  - src/gss.rs
  - src/interface.rs
  - src/glr/items.rs
  - python/src/lib.rs
  - src/lib.rs
  - src/glr/mod.rs
  - src/glr/parser.rs
  - src/precompute.rs
  - src/precompute_gss.rs
  - src/glr/table.rs
  - src/glr/tests.rs
  - src/tokenizer_combinators.rs
  - src/trie.rs
  - src/utils.rs
system:
  message: You are a helpful assistant.
  tools:
    attach:
      type: function
      name: attach
      description: Attach a set of files into the conversation as a user message.
        Fails if uncommitted changes exist.
      input_schema:
        type: array
        items: string
    diff_attachments:
      type: function
      name: diff_attachments
      description: Produce a user message containing the diff of attachments from
        the previous commit to the current one. Fails if uncommitted changes exist.
      input_schema: 'null'
    code_block_to_file:
      type: function
      name: code_block_to_file
      description: Write the content of specified code blocks from the last assistant
        message into separate files. Fails if uncommitted changes exist. Fails if
        a code block with the given ID doesn't exist.
      input_schema:
        type: object
        fields:
          commit_message: string
          actions:
            type: array
            items:
              type: object
              fields:
                code_block_id: string
                filename: string
              required:
              - code_block_id
              - filename
        required:
        - commit_message
        - actions
    delete_files:
      type: function
      name: delete_files
      description: Deletes the specified files. Fails if uncommitted changes exist.
      input_schema:
        type: array
        items: string
  model: gemini/gemini-2.0-flash-exp
  prefill_model: anthropic/claude-3-5-haiku-20241022
messages:
- tool_calls:
  - id: 1ed358d9-924a-423c-9114-ce3cd1891475
    name: attach
    input:
    - src/analyze_grammar.rs
    - src/constraint.rs
    - src/glr/grammar.rs
    - src/gss.rs
    - src/interface.rs
    - src/glr/items.rs
    - python/src/lib.rs
    - src/lib.rs
    - src/glr/mod.rs
    - src/glr/parser.rs
    - src/precompute.rs
    - src/precompute_gss.rs
    - src/glr/table.rs
    - src/glr/tests.rs
    - src/tokenizer_combinators.rs
    - src/trie.rs
    - src/utils.rs
  role: system
- tool_results:
  - tool_call_id: 1ed358d9-924a-423c-9114-ce3cd1891475
    content: "```rust\nuse std::collections::{BTreeMap, BTreeSet};\nuse crate::glr::grammar::{prod,\
      \ NonTerminal, Production, Symbol};\n\npub fn validate(productions: &[Production])\
      \ -> Result<(), String> {\n    // Ensure all nonterminals have a productions\n\
      \    let mut lhs_nonterms: BTreeSet<NonTerminal> = BTreeSet::new();\n    let\
      \ mut rhs_nonterms: BTreeSet<NonTerminal> = BTreeSet::new();\n\n    for prod\
      \ in productions {\n        lhs_nonterms.insert(prod.lhs.clone());\n       \
      \ for symbol in &prod.rhs {\n            if let Symbol::NonTerminal(nt) = symbol\
      \ {\n                rhs_nonterms.insert(nt.clone());\n            }\n     \
      \   }\n    }\n\n    let missing_nonterms: BTreeSet<_> = rhs_nonterms.difference(&lhs_nonterms).collect();\n\
      \    if !missing_nonterms.is_empty() {\n        let missing_nonterm_strings:\
      \ BTreeSet<_> = missing_nonterms.into_iter().map(|nt| nt.0.clone()).collect();\n\
      \        return Err(format!(\"Nonterminals missing a production: {:?}. All RHS\
      \ nonterminals: {:?}. All LHS nonterminals: {:?}\", missing_nonterm_strings,\
      \ rhs_nonterms, lhs_nonterms));\n    }\n\n    Ok(())\n}\n\npub fn drop_dead(productions:\
      \ &[Production]) -> Vec<Production> {\n    // todo: this function is broken\n\
      \    // Ensure all nonterminals have a productions\n    let mut nt_reachables:\
      \ BTreeMap<&NonTerminal, BTreeSet<&NonTerminal>> = BTreeMap::new();\n\n    for\
      \ prod in productions {\n        let mut rhs_nonterms = BTreeSet::new();\n \
      \       for symbol in &prod.rhs {\n            if let Symbol::NonTerminal(nt)\
      \ = symbol {\n                rhs_nonterms.insert(nt);\n            }\n    \
      \    }\n        nt_reachables.insert(&prod.lhs, rhs_nonterms);\n    }\n\n  \
      \  loop {\n        let mut changed = false;\n        for (nt, reachables) in\
      \ nt_reachables.clone() {\n            let old_len = nt_reachables[nt].len();\n\
      \            for reachable in reachables {\n                if let Some(reachable_reachables)\
      \ = nt_reachables.get(reachable).cloned() {\n                    nt_reachables.get_mut(nt).unwrap().extend(reachable_reachables);\n\
      \                }\n            }\n            if nt_reachables[nt].len() !=\
      \ old_len {\n                changed = true;\n            }\n        }\n   \
      \     if !changed {\n            break;\n        }\n    }\n\n    let start_prod\
      \ = &productions[0];\n    let mut reachable_from_start = BTreeSet::new();\n\
      \    for symbol in &start_prod.rhs {\n        if let Symbol::NonTerminal(nt)\
      \ = symbol {\n            reachable_from_start.insert(nt);\n            if let\
      \ Some(nt_reachables) = nt_reachables.get(nt).cloned() {\n                reachable_from_start.extend(nt_reachables);\n\
      \            }\n        }\n    }\n\n    let mut new_productions = vec![start_prod.clone()];\n\
      \    // crate::debug!(2, \"Keeping production {:?}\", start_prod);\n    for\
      \ prod in productions.iter().skip(1) {\n        if reachable_from_start.contains(&prod.lhs)\
      \ {\n            // crate::debug!(2, \"Keeping production {:?}\", prod);\n \
      \           new_productions.push(prod.clone())\n        } else {\n         \
      \   // crate::debug!(2, \"Removing production {:?}\", prod);\n        }\n  \
      \  }\n    crate::debug!(2, \"Dropped {} productions\", productions.len() - new_productions.len());\n\
      \n    new_productions\n}\n\n```\n\n```rust\nuse crate::glr::parser::{GLRParser,\
      \ GLRParserState, InsertWith, ParseState, ParseStateKey};\nuse crate::glr::table::{StateID,\
      \ TerminalID};\nuse crate::{precompute, debug};\nuse crate::precompute::{LLMTokenID,\
      \ Token, TokenID, Tokenizer};\nuse bitvec::prelude::*;\nuse bimap::BiBTreeMap;\n\
      use std::collections::{BTreeMap, BTreeSet};\nuse std::sync::{Arc, Mutex};\n\
      use crate::trie::TrieNode;\n\n/// Type alias for a token represented as a byte\
      \ vector\ntype LLMToken = Vec<u8>;\n\n/// Type alias for mapping LLM tokens\
      \ to their unique identifiers\ntype LLMTokenMap = BiBTreeMap<Vec<u8>, LLMTokenID>;\n\
      \n/// Represents a constraint on grammar generation\n#[derive(Debug, Clone)]\n\
      pub struct GrammarConstraint<T: Tokenizer> {\n    pub(crate) tokenizer: T,\n\
      \    pub(crate) parser: GLRParser,\n    pub precomputed: BTreeMap<StateID, TrieNode<TokenID,\
      \ (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>,\n\
      \    pub(crate) max_llm_token_id: usize,\n}\n\n/// Represents the current state\
      \ of a grammar constraint\n#[derive(Debug, Clone)]\npub struct GrammarConstraintState<T:\
      \ Tokenizer> {\n    parent: GrammarConstraint<T>,\n    states: Vec<(ParseState,\
      \ BTreeSet<StateID>)>,\n}\n\nimpl<T: Tokenizer> GrammarConstraint<T> {\n   \
      \ /// Creates a new grammar constraint\n    pub fn new(\n        tokenizer:\
      \ T, \n        parser: GLRParser, \n        llm_tokens: LLMTokenMap, \n    \
      \    eof_llm_token_id: usize, \n        max_llm_token_id: usize\n    ) -> Self\
      \ {\n        let mut precomputed = precompute::precompute(&tokenizer, &llm_tokens,\
      \ LLMTokenID(eof_llm_token_id), max_llm_token_id);\n        precompute::precompute_add_eof(&mut\
      \ precomputed, LLMTokenID(eof_llm_token_id), parser.eof_terminal_id.0, max_llm_token_id);\n\
      \n        Self {\n            tokenizer,\n            parser,\n            precomputed,\n\
      \            max_llm_token_id,\n        }\n    }\n\n    /// Initializes the\
      \ grammar constraint state\n    pub fn init(self) -> GrammarConstraintState<T>\
      \ {\n        let parser_initial_state = self.parser.init_parse_state();\n  \
      \      let tokenizer_initial_state_id = StateID(self.tokenizer.initial_state_id());\n\
      \n        GrammarConstraintState {\n            parent: self,\n            states:\
      \ vec![(parser_initial_state, BTreeSet::from([tokenizer_initial_state_id]))],\n\
      \        }\n    }\n}\n\nimpl<'a, T: Tokenizer> GrammarConstraintState<T> {\n\
      \    /// Generates a mask of valid next tokens\n    pub fn get_mask(&self) ->\
      \ BitVec {\n        let mut result = bitvec![0; self.parent.max_llm_token_id\
      \ + 1];\n\n        for (parse_state, tokenizer_state_ids) in &self.states {\n\
      \            for tokenizer_state in tokenizer_state_ids {\n                let\
      \ token_sequence_map = &self.parent.precomputed[tokenizer_state];\n        \
      \        TrieNode::special_map(\n                    Arc::new(Mutex::new(token_sequence_map.clone())),\n\
      \                    vec![parse_state.clone()],\n                    |current_parse_states,\
      \ token_id, _dst_node| {\n                        let mut glr_parse_state =\
      \ self.parent.parser.init_glr_parser_from_parse_states(current_parse_states.clone());\n\
      \                        glr_parse_state.step(TerminalID(*token_id));\n    \
      \                    glr_parse_state.active_states\n                    },\n\
      \                    |parse_states: Vec<Vec<ParseState>>| {\n              \
      \          let all_parse_states: Vec<ParseState> = parse_states.into_iter().flatten().collect();\n\
      \                        let mut new_glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(all_parse_states);\n\
      \                        new_glr_parse_state.merge_active_states();\n      \
      \                  new_glr_parse_state.active_states\n                    },\n\
      \                    |(_, bitsets, maybe_clean_end_bitset), current_parse_states|\
      \ {\n                        let mut glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(current_parse_states.clone());\n\
      \                        if glr_parse_state.is_ok() {\n                    \
      \        for (possible_next_grammar_token, bitset) in bitsets {\n          \
      \                      let mut new_glr_parse_state = glr_parse_state.clone();\n\
      \                                let possible_next_grammar_token_id = TerminalID(*possible_next_grammar_token);\n\
      \                                new_glr_parse_state.step(possible_next_grammar_token_id);\n\
      \n                                if new_glr_parse_state.is_ok() {\n       \
      \                             result |= bitset;\n                          \
      \      }\n                            }\n                            if let\
      \ Some(bitset) = maybe_clean_end_bitset {\n                                result\
      \ |= bitset;\n                            }\n                        }\n   \
      \                 },\n                );\n            }\n        }\n       \
      \ result\n    }\n\n    /// Commits a token to the current state\n    pub fn\
      \ commit(&mut self, llm_token_id: LLMTokenID) {\n        let mut new_states:\
      \ BTreeMap<(ParseStateKey, BTreeSet<StateID>), ParseState> = BTreeMap::new();\n\
      \        \n        for (parse_state, tokenizer_state_ids) in &self.states {\n\
      \            for tokenizer_state_id in tokenizer_state_ids {\n             \
      \   // todo: should be able to do the below loop more efficiently by optimising\
      \ the precomputed\n                //  stuff for earlier llm token lookup\n\
      \                TrieNode::special_map(\n                    Arc::new(Mutex::new(self.parent.precomputed[tokenizer_state_id].clone())),\n\
      \                    vec![parse_state.clone()],\n                    // todo:\
      \ it's messy that we need to access the value in dst_node here.\n          \
      \          |current_parse_states, token_id, _dst_node| {\n                 \
      \       // todo: this is introducing redundancy... ?\n                     \
      \   let mut glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(current_parse_states.clone());\n\
      \                        glr_parse_state.step(TerminalID(*token_id));\n    \
      \                    glr_parse_state.active_states\n                    },\n\
      \                    |parse_states: Vec<Vec<ParseState>>| {\n              \
      \          let all_parse_states: Vec<ParseState> = parse_states.into_iter().flatten().collect();\n\
      \                        let mut new_glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(all_parse_states);\n\
      \                        new_glr_parse_state.merge_active_states();\n      \
      \                  new_glr_parse_state.active_states\n                    },\n\
      \                    |(llm_token_id_to_state_id, _, _), current_parse_states|\
      \ {\n                        let mut new_glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(current_parse_states.clone());\n\
      \                        if let Some(info) = llm_token_id_to_state_id.get(&llm_token_id)\
      \ {\n                            for active_parse_state in new_glr_parse_state.active_states\
      \ {\n                                new_states.insert_with(\n             \
      \                       (active_parse_state.key(), BTreeSet::from([info.unwrap_or(StateID(0))])),\n\
      \                                    active_parse_state,\n                 \
      \                   |old, new| {\n                                        old.merge(new);\n\
      \                                    },\n                                );\n\
      \                            }\n                        }\n                \
      \    },\n                )\n            }\n        }\n        \n        self.states\
      \ = new_states.into_iter().map(|((_key, tokenizer_state_ids), parse_state)|\
      \ {\n            (parse_state, tokenizer_state_ids)\n        }).collect();\n\
      \    }\n\n    /// Commits multiple tokens sequentially\n    pub fn commit_many(&mut\
      \ self, llm_token_ids: &[LLMTokenID]) {\n        for &llm_token_id in llm_token_ids\
      \ {\n            self.commit(llm_token_id);\n        }\n    }\n\n    /// Getter\
      \ for precomputed field\n    pub fn get_precomputed(&self) -> &BTreeMap<StateID,\
      \ TrieNode<TokenID, (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID,\
      \ BitVec>, Option<BitVec>)>> {\n        &self.parent.precomputed\n    }\n}\n\
      \n```\n\n```rust\nuse std::collections::{BTreeMap, BTreeSet};\n\n/// Represents\
      \ a non-terminal symbol in a grammar\n#[derive(Debug, Clone, PartialEq, Eq,\
      \ PartialOrd, Ord)]\npub struct NonTerminal(pub String);\n\n/// Represents a\
      \ terminal symbol in a grammar\n#[derive(Debug, Clone, PartialEq, Eq, PartialOrd,\
      \ Ord)]\npub struct Terminal(pub String);\n\n/// Represents a symbol in a grammar\
      \ production\n#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]\npub enum\
      \ Symbol {\n    Terminal(Terminal),\n    NonTerminal(NonTerminal),\n}\n\n///\
      \ Represents a production rule in a grammar\n#[derive(Debug, Clone, PartialEq,\
      \ Eq, PartialOrd, Ord)]\npub struct Production {\n    pub lhs: NonTerminal,\n\
      \    pub rhs: Vec<Symbol>,\n}\n\n/// Creates a non-terminal symbol\npub fn nt(name:\
      \ &str) -> Symbol {\n    Symbol::NonTerminal(NonTerminal(name.to_string()))\n\
      }\n\n/// Creates a terminal symbol\npub fn t(name: &str) -> Symbol {\n    Symbol::Terminal(Terminal(name.to_string()))\n\
      }\n\n/// Creates a production rule\npub fn prod(name: &str, rhs: Vec<Symbol>)\
      \ -> Production {\n    Production {\n        lhs: NonTerminal(name.to_string()),\n\
      \        rhs,\n    }\n}\n\n/// Computes the set of non-terminals that can derive\
      \ an empty string (epsilon)\npub fn compute_epsilon_nonterminals(productions:\
      \ &[Production]) -> BTreeSet<NonTerminal> {\n    let mut epsilon_nonterminals\
      \ = BTreeSet::new();\n    let mut changed = true;\n\n    while changed {\n \
      \       changed = false;\n        for production in productions {\n        \
      \    if production.rhs.is_empty() && !epsilon_nonterminals.contains(&production.lhs)\
      \ {\n                epsilon_nonterminals.insert(production.lhs.clone());\n\
      \                changed = true;\n            } else if production.rhs.iter().all(|symbol|\
      \ {\n                matches!(symbol, Symbol::NonTerminal(nt) if epsilon_nonterminals.contains(nt))\n\
      \            }) && !epsilon_nonterminals.contains(&production.lhs)\n       \
      \     {\n                epsilon_nonterminals.insert(production.lhs.clone());\n\
      \                changed = true;\n            }\n        }\n    }\n\n    epsilon_nonterminals\n\
      }\n\n/// Computes the FIRST sets for each non-terminal\npub fn compute_first_sets(productions:\
      \ &[Production]) -> BTreeMap<NonTerminal, BTreeSet<Terminal>> {\n    let epsilon_nonterminals\
      \ = compute_epsilon_nonterminals(productions);\n    let mut first_sets: BTreeMap<NonTerminal,\
      \ BTreeSet<Terminal>> = BTreeMap::new();\n\n    // Initialize first sets\n \
      \   for production in productions {\n        let lhs = &production.lhs;\n  \
      \      first_sets.entry(lhs.clone()).or_default();\n        \n        for symbol\
      \ in &production.rhs {\n            match symbol {\n                Symbol::Terminal(t)\
      \ => {\n                    first_sets.get_mut(lhs).unwrap().insert(t.clone());\n\
      \                    break;\n                }\n                Symbol::NonTerminal(nt)\
      \ => {\n                    if !epsilon_nonterminals.contains(nt) {\n      \
      \                  break;\n                    }\n                }\n      \
      \      }\n        }\n    }\n\n    let mut changed = true;\n    while changed\
      \ {\n        changed = false;\n\n        for production in productions {\n \
      \           let lhs = &production.lhs;\n            let rhs = &production.rhs;\n\
      \n            let old_size = first_sets.get_mut(lhs).unwrap().len();\n\n   \
      \         for symbol in rhs {\n                if let Symbol::NonTerminal(nt)\
      \ = symbol {\n                    let first_nt = first_sets[nt].clone();\n \
      \                   first_sets.get_mut(lhs).unwrap().extend(first_nt);\n\n \
      \                   if !epsilon_nonterminals.contains(nt) {\n              \
      \          break;\n                    }\n                }\n            }\n\
      \n            if first_sets.get_mut(lhs).unwrap().len() != old_size {\n    \
      \            changed = true;\n            }\n        }\n    }\n\n    first_sets\n\
      }\n\n/// Computes the FOLLOW sets for each non-terminal\npub fn compute_follow_sets(productions:\
      \ &[Production]) -> BTreeMap<NonTerminal, BTreeSet<Terminal>> {\n    let first_sets\
      \ = compute_first_sets(productions);\n    let epsilon_nonterminals = compute_epsilon_nonterminals(productions);\n\
      \    let mut follow_sets: BTreeMap<NonTerminal, BTreeSet<Terminal>> = BTreeMap::new();\n\
      \n    // Initialize follow sets\n    for production in productions {\n     \
      \   follow_sets.entry(production.lhs.clone()).or_default();\n    }\n\n    //\
      \ Add EOF marker to the start symbol\n    if let Some(start_symbol) = productions.first()\
      \ {\n        follow_sets\n            .get_mut(&start_symbol.lhs)\n        \
      \    .unwrap()\n            .insert(Terminal(\"$\".to_string()));\n    }\n\n\
      \    let mut changed = true;\n    while changed {\n        changed = false;\n\
      \n        for production in productions {\n            let lhs = &production.lhs;\n\
      \            let rhs = &production.rhs;\n\n            for (i, symbol) in rhs.iter().enumerate()\
      \ {\n                if let Symbol::NonTerminal(nt) = symbol {\n           \
      \         let old_size = follow_sets.get_mut(nt).unwrap().len();\n\n       \
      \             let mut nullable = true;\n                    for next_symbol\
      \ in &rhs[i + 1..] {\n                        match next_symbol {\n        \
      \                    Symbol::Terminal(t_next) => {\n                       \
      \         follow_sets.get_mut(nt).unwrap().insert(t_next.clone());\n       \
      \                         nullable = false;\n                              \
      \  break;\n                            }\n                            Symbol::NonTerminal(nt_next)\
      \ => {\n                                let first_next = &first_sets[nt_next];\n\
      \                                follow_sets.get_mut(nt).unwrap().extend(first_next.iter().cloned());\n\
      \                                \n                                if !epsilon_nonterminals.contains(nt_next)\
      \ {\n                                    nullable = false;\n               \
      \                     break;\n                                }\n          \
      \                  }\n                        }\n                    }\n\n \
      \                   if nullable {\n                        let follow_lhs =\
      \ follow_sets.get(lhs).unwrap().clone();\n                        follow_sets.get_mut(nt).unwrap().extend(follow_lhs);\n\
      \                    }\n\n                    if follow_sets.get_mut(nt).unwrap().len()\
      \ != old_size {\n                        changed = true;\n                 \
      \   }\n                }\n            }\n        }\n    }\n\n    follow_sets\n\
      }\n\n```\n\n```rust\nuse std::collections::{BTreeMap, HashMap};\nuse std::sync::Arc;\n\
      \n/// Graph-Structured Stack (GSS) Node representing a graph-like stack structure\n\
      #[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]\npub struct GSSNode<T>\
      \ {\n    /// The value stored in the current node\n    value: T,\n    /// Predecessor\
      \ nodes in the stack\n    predecessors: Vec<Arc<GSSNode<T>>>,\n}\n\nimpl<T>\
      \ GSSNode<T> {\n    /// Creates a new GSS node with a single value\n    pub\
      \ fn new(value: T) -> Self {\n        Self {\n            value,\n         \
      \   predecessors: Vec::new(),\n        }\n    }\n\n    /// Creates a GSS node\
      \ from an iterator of values\n    pub fn from_iter<I>(iter: I) -> Self\n   \
      \ where\n        I: IntoIterator<Item = T>,\n    {\n        let mut iter = iter.into_iter();\n\
      \        let mut root = Self::new(iter.next().unwrap());\n        for value\
      \ in iter {\n            root = root.push(value);\n        }\n        root\n\
      \    }\n\n    /// Pushes a new value onto the GSS node, creating a new node\n\
      \    pub fn push(self, value: T) -> Self {\n        let mut new_node = Self::new(value);\n\
      \        new_node.predecessors.push(Arc::new(self));\n        new_node\n   \
      \ }\n\n    /// Returns all predecessor nodes\n    pub fn pop(&self) -> Vec<Arc<Self>>\
      \ {\n        self.predecessors.clone()\n    }\n\n    /// Returns predecessor\
      \ nodes up to a specified depth\n    pub fn popn(&self, n: usize) -> Vec<Arc<Self>>\n\
      \    where\n        T: Clone,\n    {\n        if n == 0 {\n            return\
      \ vec![Arc::new(self.clone())];\n        }\n        let mut nodes = Vec::new();\n\
      \        for popped in self.pop() {\n            nodes.extend(popped.popn(n\
      \ - 1));\n        }\n        nodes\n    }\n\n    /// Returns the current node's\
      \ value\n    pub fn peek(&self) -> &T {\n        &self.value\n    }\n\n    ///\
      \ Flattens the GSS node into all possible paths\n    pub fn flatten(&self) ->\
      \ Vec<Vec<T>>\n    where\n        T: Clone,\n    {\n        let mut result =\
      \ Vec::new();\n        let mut stack = Vec::new();\n        stack.push((self,\
      \ Vec::new()));\n        while let Some((node, mut path)) = stack.pop() {\n\
      \            path.push(node.value.clone());\n            if node.predecessors.is_empty()\
      \ {\n                result.push(path);\n            } else {\n            \
      \    for predecessor in &node.predecessors {\n                    stack.push((predecessor,\
      \ path.clone()));\n                }\n            }\n        }\n        result\n\
      \    }\n\n    /// Flattens multiple GSS nodes into their paths\n    pub fn flatten_bulk(nodes:\
      \ &[Self]) -> Vec<Vec<T>>\n    where\n        T: Clone,\n    {\n        nodes.iter().flat_map(|node|\
      \ node.flatten()).collect()\n    }\n\n    /// Merges two GSS nodes with the\
      \ same value\n    pub fn merge(&mut self, mut other: Self)\n    where\n    \
      \    T: PartialEq,\n    {\n        assert!(self.value == other.value);\n   \
      \     self.predecessors.append(&mut other.predecessors);\n    }\n\n    /// Maps\
      \ the GSS node's values using a transformation function\n    pub fn map<F, U>(&self,\
      \ f: F) -> GSSNode<U>\n    where\n        F: Copy + Fn(&T) -> U,\n    {\n  \
      \      GSSNode {\n            value: f(&self.value),\n            predecessors:\
      \ self.predecessors.clone().into_iter()\n                .map(|node| Arc::new(node.map(f)))\n\
      \                .collect(),\n        }\n    }\n}\n\nimpl<T> Drop for GSSNode<T>\
      \ {\n    fn drop(&mut self) {\n        let mut cur_nodes = std::mem::take(&mut\
      \ self.predecessors);\n        while let Some(node) = cur_nodes.pop() {\n  \
      \          if let Ok(mut inner_node) = Arc::try_unwrap(node) {\n           \
      \     cur_nodes.append(&mut inner_node.predecessors);\n            }\n     \
      \   }\n    }\n}\n\npub trait GSSTrait<T: Clone> {\n    type Peek<'a> where T:\
      \ 'a, Self: 'a;\n    fn peek(&self) -> Self::Peek<'_>;\n    fn push(&self, value:\
      \ T) -> GSSNode<T>;\n    fn pop(&self) -> Vec<Arc<GSSNode<T>>>;\n    fn popn(&self,\
      \ n: usize) -> Vec<Arc<GSSNode<T>>>;\n}\n\nimpl<T: Clone> GSSTrait<T> for GSSNode<T>\
      \ {\n    type Peek<'a> = &'a T where T: 'a;\n\n    fn peek(&self) -> Self::Peek<'_>\
      \ {\n        &self.value\n    }\n\n    fn push(&self, value: T) -> GSSNode<T>\
      \ {\n        let mut new_node = GSSNode::new(value);\n        new_node.predecessors.push(Arc::new(self.clone()));\n\
      \        new_node\n    }\n\n    fn pop(&self) -> Vec<Arc<GSSNode<T>>> {\n  \
      \      self.predecessors.clone()\n    }\n\n    fn popn(&self, n: usize) -> Vec<Arc<GSSNode<T>>>\
      \ {\n        if n == 0 {\n            return vec![Arc::new(self.clone())];\n\
      \        }\n        let mut nodes = Vec::new();\n        for popped in self.pop()\
      \ {\n            nodes.extend(popped.popn(n - 1));\n        }\n        nodes\n\
      \    }\n}\n\nimpl<T: Clone> GSSTrait<T> for Arc<GSSNode<T>> {\n    type Peek<'a>\
      \ = &'a T where T: 'a;\n\n    fn peek(&self) -> Self::Peek<'_> {\n        &self.value\n\
      \    }\n\n    fn push(&self, value: T) -> GSSNode<T> {\n        let mut new_node\
      \ = GSSNode::new(value);\n        new_node.predecessors.push(self.clone());\n\
      \        new_node\n    }\n\n    fn pop(&self) -> Vec<Arc<GSSNode<T>>> {\n  \
      \      self.predecessors.clone()\n    }\n\n    fn popn(&self, n: usize) -> Vec<Arc<GSSNode<T>>>\
      \ {\n        if n == 0 {\n            return vec![self.clone()];\n        }\n\
      \        let mut nodes = Vec::new();\n        for popped in self.pop() {\n \
      \           nodes.extend(popped.popn(n - 1));\n        }\n        nodes\n  \
      \  }\n}\n\nimpl<T: Clone> GSSTrait<T> for Option<Arc<GSSNode<T>>> {\n    type\
      \ Peek<'a> = Option<&'a T> where T: 'a;\n\n    fn peek(&self) -> Self::Peek<'_>\
      \ {\n        self.as_ref().map(|node| node.peek())\n    }\n\n    fn push(&self,\
      \ value: T) -> GSSNode<T> {\n        self.clone().map(|node| node.push(value.clone())).unwrap_or_else(||\
      \ GSSNode::new(value))\n    }\n\n    fn pop(&self) -> Vec<Arc<GSSNode<T>>> {\n\
      \        self.as_ref().map(|node| node.pop()).unwrap_or_default()\n    }\n\n\
      \    fn popn(&self, n: usize) -> Vec<Arc<GSSNode<T>>> {\n        self.as_ref().map(|node|\
      \ node.popn(n)).unwrap_or_default()\n    }\n}\n\nimpl<T: Clone> GSSTrait<T>\
      \ for Option<GSSNode<T>> {\n    type Peek<'a> = Option<&'a T> where T: 'a;\n\
      \n    fn peek(&self) -> Self::Peek<'_> {\n        self.as_ref().map(|node| node.peek())\n\
      \    }\n\n    fn push(&self, value: T) -> GSSNode<T> {\n        self.clone().map(|node|\
      \ node.push(value.clone())).unwrap_or_else(|| GSSNode::new(value))\n    }\n\n\
      \    fn pop(&self) -> Vec<Arc<GSSNode<T>>> {\n        self.as_ref().map(|node|\
      \ node.pop()).unwrap_or_default()\n    }\n\n    fn popn(&self, n: usize) ->\
      \ Vec<Arc<GSSNode<T>>> {\n        self.as_ref().map(|node| node.popn(n)).unwrap_or_default()\n\
      \    }\n}\n\npub trait BulkMerge<T> {\n    fn bulk_merge(&mut self);\n}\n\n\
      impl<T: Clone + Ord> BulkMerge<T> for Vec<Arc<GSSNode<T>>> {\n    fn bulk_merge(&mut\
      \ self) {\n        // todo: should be possible to avoid cloning T in some cases\
      \ by using &T in this map,\n        //  but we need to be careful about lifetimes.\
      \ If we use `node.as_ref().value`, then node\n        //  will go out of bounds\
      \ while the reference to its value is still inside `groups`.\n        let mut\
      \ groups: BTreeMap<T, HashMap<_, Arc<GSSNode<T>>>> = BTreeMap::new();\n    \
      \    for node in self.drain(..) {\n            groups.entry(node.value.clone()).or_default().entry(Arc::as_ptr(&node)).or_insert(node);\n\
      \        }\n        for mut group in groups.into_values() {\n            let\
      \ mut group = group.into_values().collect::<Vec<_>>();\n            let mut\
      \ first = group.pop().unwrap();\n            if group.is_empty() {\n       \
      \         self.push(first);\n            } else {\n                // address\
      \ map\n                let mut predecessors_set: BTreeMap<_, _> = BTreeMap::new();\n\
      \                for sibling in group {\n                    for predecessor\
      \ in &sibling.predecessors {\n                        predecessors_set.insert(Arc::as_ptr(predecessor),\
      \ predecessor.clone());\n                    }\n                }\n        \
      \        let first_mut_ref = Arc::make_mut(&mut first);\n                first_mut_ref.predecessors\
      \ = predecessors_set.into_values().collect();\n                self.push(first);\n\
      \            }\n        }\n    }\n}\n```\n\n```rust\n// src/interface.rs\nuse\
      \ crate::finite_automata::{greedy_group, groups, non_greedy_group, ExprGroup,\
      \ ExprGroups};\nuse crate::finite_automata::{Expr, Regex};\nuse crate::glr::grammar::{NonTerminal,\
      \ Production, Symbol, Terminal};\nuse crate::glr::parser::{GLRParser, ParseState};\n\
      use crate::glr::table::{assign_non_terminal_ids, generate_glr_parser, generate_glr_parser_with_maps,\
      \ NonTerminalID, StateID, TerminalID};\nuse crate::precompute::{precompute,\
      \ LLMTokenID, Token, Tokenizer};\nuse bimap::BiBTreeMap;\nuse std::collections::{BTreeMap,\
      \ BTreeSet, HashSet};\nuse std::fmt::{Debug, Formatter};\nuse kdam::tqdm;\n\
      use crate::analyze_grammar::drop_dead;\nuse crate::constraint::{GrammarConstraint};\n\
      use crate::debug;\n\ntype LLMToken<'a> = &'a [u8];\ntype LLMTokenMap = BiBTreeMap<Vec<u8>,\
      \ LLMTokenID>;\n\n#[derive(Clone)]\npub struct Grammar<T> {\n    pub productions:\
      \ Vec<Production>,\n    pub start_production_id: usize,\n    pub literal_map:\
      \ BTreeMap<String, String>,\n    pub terminal_name_to_group_id: BiBTreeMap<String,\
      \ usize>,\n    pub terminal_expr_to_group_id: BiBTreeMap<Expr, usize>,\n   \
      \ pub tokenizer: T,\n}\n\nimpl<T> Debug for Grammar<T> where T: Debug {\n  \
      \  fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n        writeln!(f,\
      \ \"Grammar:\")?;\n        writeln!(f, \"  Start Production ID: {}\", self.start_production_id)?;\n\
      \        writeln!(f, \"  Productions:\")?;\n\n        for production in &self.productions\
      \ {\n            write!(f, \"    {} -> \", production.lhs.0)?;\n           \
      \ for (i, symbol) in production.rhs.iter().enumerate() {\n                match\
      \ symbol {\n                    Symbol::Terminal(terminal) => {\n          \
      \              write!(f, \"{}\", terminal.0)?;\n                    }\n    \
      \                Symbol::NonTerminal(non_terminal) => {\n                  \
      \      write!(f, \"{}\", non_terminal.0)?;\n                    }\n        \
      \        }\n                if i < production.rhs.len() - 1 {\n            \
      \        write!(f, \" \")?;\n                }\n            }\n            writeln!(f)?;\n\
      \        }\n\n        writeln!(f, \"  Literal Map:\")?;\n        for (literal,\
      \ mangled_name) in &self.literal_map {\n            writeln!(f, \"    {:?}:\
      \ {}\", literal, mangled_name)?;\n        }\n\n        writeln!(f, \"  Terminal\
      \ Map (name to group ID):\")?;\n        for (name, group_id) in &self.terminal_name_to_group_id\
      \ {\n            writeln!(f, \"    {:?}: {}\", name, group_id)?;\n        }\n\
      \n        writeln!(f, \"Tokenizer:\");\n        writeln!(f, \"{:?}\", &self.tokenizer);\n\
      \n        Ok(())\n    }\n}\n\nimpl<T> Grammar<T> {\n    fn mangle_literal(literal:\
      \ &str, tokens: &BTreeMap<String, Expr>) -> String {\n        let mut mangled_name\
      \ = literal.to_string();\n        let mut i = 0;\n        while tokens.contains_key(&mangled_name)\
      \ {\n            mangled_name = format!(\"{}__literal_{}\", literal, i);\n \
      \           i += 1;\n        }\n        mangled_name\n    }\n}\n\n#[derive(Debug,\
      \ Clone, PartialEq, Eq)]\npub enum GrammarExpr {\n    RegexExpr(Expr),\n   \
      \ Ref(String),\n    Sequence(Vec<GrammarExpr>),\n    Choice(Vec<GrammarExpr>),\n\
      \    Optional(Box<GrammarExpr>),\n    Repeat(Box<GrammarExpr>),\n}\n\npub fn\
      \ regex(expr: Expr) -> GrammarExpr {\n    GrammarExpr::RegexExpr(expr)\n}\n\n\
      pub fn r#ref(name: &str) -> GrammarExpr {\n    GrammarExpr::Ref(name.to_string())\n\
      }\n\npub fn sequence(exprs: Vec<GrammarExpr>) -> GrammarExpr {\n    GrammarExpr::Sequence(exprs)\n\
      }\n\npub fn choice(exprs: Vec<GrammarExpr>) -> GrammarExpr {\n    GrammarExpr::Choice(exprs)\n\
      }\n\npub fn optional(expr: GrammarExpr) -> GrammarExpr {\n    GrammarExpr::Optional(Box::new(expr))\n\
      }\n\npub fn repeat(expr: GrammarExpr) -> GrammarExpr {\n    GrammarExpr::Repeat(Box::new(expr))\n\
      }\n\nimpl<T> Grammar<T> {\n    pub fn glr_parser(&self) -> GLRParser {\n   \
      \     generate_glr_parser(&self.productions, self.start_production_id)\n   \
      \ }\n}\n\nimpl Grammar<Regex> {\n    /// Constructs a `Grammar` and `Regex`\
      \ tokenizer from a list of grammar expressions.\n    /// The first non-terminal\
      \ in the list is treated as the start symbol.\n    pub fn from_exprs(exprs:\
      \ Vec<(String, GrammarExpr)>) -> Self {\n        let mut productions = Vec::new();\n\
      \        let mut literal_map = BTreeMap::new();\n        let mut terminal_name_to_group_id\
      \ = BiBTreeMap::new();\n        let mut terminal_expr_to_group_id = BiBTreeMap::new();\n\
      \        let mut next_terminal_id = 0;\n\n        // Add a start production.\n\
      \        // make sure the start production name is not already taken by adding\
      \ apostrophes to it until it's unique.\n        let mut start_production_name\
      \ = \"start'\".to_string();\n        let nonterminals: HashSet<&str> = exprs.iter().map(|(name,\
      \ _)| name.as_str()).collect();\n        while nonterminals.contains(&start_production_name.as_str())\
      \ {\n            start_production_name.push('\\'');\n        }\n        debug!(2,\
      \ \"start_production_name: {:?}\", start_production_name);\n        productions.push(Production\
      \ {\n            lhs: NonTerminal(start_production_name.clone()),\n        \
      \    rhs: vec![Symbol::NonTerminal(NonTerminal(exprs[0].0.clone()))],\n    \
      \    });\n\n        fn convert_expr(\n            expr: &GrammarExpr,\n    \
      \        productions: &mut Vec<Production>,\n            non_terminal_map: &mut\
      \ BiBTreeMap<NonTerminal, NonTerminalID>,\n            next_non_terminal_id:\
      \ &mut usize,\n            literal_map: &mut BTreeMap<String, String>,\n   \
      \         tokens: &mut BTreeMap<String, Expr>,\n            terminal_name_to_group_id:\
      \ &mut BiBTreeMap<String, usize>,\n            // todo: make this `terminal_group_id_to_expr`\
      \ instead\n            terminal_expr_to_group_id: &mut BiBTreeMap<Expr, usize>,\n\
      \            next_terminal_id: &mut usize,\n        ) -> Vec<Symbol> {\n   \
      \         // TODO: define a function that makes us a unique name for an internal\
      \ rule, with an appropriate prefix.\n            //  e.g. Option0, Repeat0,\
      \ etc. Make sure there's no existing rule with that name (and there won't be\
      \ one later either).\n            //  i.e. collect all nonterminals in the grammar\
      \ upfront and pass it to convert_expr.\n            match expr {\n         \
      \       GrammarExpr::RegexExpr(regex_expr) => {\n                    // TODO:\
      \ what if this is already in the map (e.g. the user happens to create a rule\
      \ with name `__regex_0`?\n                    //  We need to generate a unique\
      \ regex name.\n                    if let Some(terminal_id) = terminal_expr_to_group_id.get_by_left(&regex_expr)\
      \ {\n                        vec![Symbol::Terminal(Terminal(format!(\"__regex_{}\"\
      , terminal_id)))]\n                    } else {\n                        //\
      \ Create a unique terminal name for this regex expression\n                \
      \        let terminal_id = *next_terminal_id;\n                        let terminal_name\
      \ = format!(\"__regex_{}\", terminal_id);\n                        terminal_name_to_group_id.insert(terminal_name.clone(),\
      \ terminal_id);\n                        terminal_expr_to_group_id.insert(regex_expr.clone(),\
      \ terminal_id);\n                        tokens.insert(terminal_name.clone(),\
      \ regex_expr.clone());\n                        *next_terminal_id += 1;\n  \
      \                      vec![Symbol::Terminal(Terminal(terminal_name))]\n   \
      \                 }\n                }\n                GrammarExpr::Ref(name)\
      \ => {\n                    vec![Symbol::NonTerminal(NonTerminal(name.clone()))]\n\
      \                }\n                GrammarExpr::Sequence(exprs) => exprs\n\
      \                    .iter()\n                    .flat_map(|e| {\n        \
      \                convert_expr(\n                            e,\n           \
      \                 productions,\n                            non_terminal_map,\n\
      \                            next_non_terminal_id,\n                       \
      \     literal_map,\n                            tokens,\n                  \
      \          terminal_name_to_group_id,\n                            terminal_expr_to_group_id,\n\
      \                            next_terminal_id,\n                        )\n\
      \                    })\n                    .collect(),\n                GrammarExpr::Choice(exprs)\
      \ => {\n                    let new_nonterminal = format!(\"Choice{}\", *next_non_terminal_id);\n\
      \                    let nt = NonTerminal(new_nonterminal.clone());\n\n    \
      \                // TODO: what if this is already in the map (e.g. the user\
      \ happens to create a rule with name `Choice0`?\n                    //  We\
      \ need to generate a unique nonterminal name.\n                    if !non_terminal_map.contains_left(&nt)\
      \ {\n                        non_terminal_map.insert(nt.clone(), NonTerminalID(*next_non_terminal_id));\n\
      \                        *next_non_terminal_id += 1;\n                    }\n\
      \n                    for expr in exprs {\n                        let rhs =\
      \ convert_expr(\n                            expr,\n                       \
      \     productions,\n                            non_terminal_map,\n        \
      \                    next_non_terminal_id,\n                            literal_map,\n\
      \                            tokens,\n                            terminal_name_to_group_id,\n\
      \                            terminal_expr_to_group_id,\n                  \
      \          next_terminal_id,\n                        );\n                 \
      \       productions.push(Production {\n                            lhs: nt.clone(),\n\
      \                            rhs,\n                        });\n           \
      \         }\n\n                    vec![Symbol::NonTerminal(nt)]\n         \
      \       }\n                GrammarExpr::Optional(expr) => {\n              \
      \      // TODO: name the internal rule here Option{} or something rather than\
      \ Choice{}.\n                    convert_expr(\n                        &GrammarExpr::Choice(vec![*expr.clone(),\
      \ GrammarExpr::Sequence(vec![])]),\n                        productions,\n \
      \                       non_terminal_map,\n                        next_non_terminal_id,\n\
      \                        literal_map,\n                        tokens,\n   \
      \                     terminal_name_to_group_id,\n                        terminal_expr_to_group_id,\n\
      \                        next_terminal_id,\n                    )\n        \
      \        }\n                GrammarExpr::Repeat(expr) => {\n               \
      \     // TODO: same as above, make sure it's unique.\n                    let\
      \ nonterminal_id = *next_non_terminal_id;\n                    let nonterminal_name\
      \ = format!(\"Repeat{}\", nonterminal_id);\n                    non_terminal_map.insert(NonTerminal(nonterminal_name.clone()),\
      \ NonTerminalID(nonterminal_id));\n                    *next_non_terminal_id\
      \ += 1;\n                    let rhs = convert_expr(\n                     \
      \   expr,\n                        productions,\n                        non_terminal_map,\n\
      \                        next_non_terminal_id,\n                        literal_map,\n\
      \                        tokens,\n                        terminal_name_to_group_id,\n\
      \                        terminal_expr_to_group_id,\n                      \
      \  next_terminal_id,\n                    );\n                    productions.push(Production\
      \ {\n                        lhs: NonTerminal(nonterminal_name.clone()),\n \
      \                       rhs,\n                    });\n                    vec![Symbol::NonTerminal(NonTerminal(nonterminal_name))]\n\
      \                }\n            }\n        }\n\n        let mut non_terminal_map\
      \ = BiBTreeMap::new();\n        let mut next_non_terminal_id = 0;\n        let\
      \ mut tokens = BTreeMap::new();\n\n        for (name, expr) in tqdm!(exprs.iter())\
      \ {\n            let rhs = convert_expr(\n                expr,\n          \
      \      &mut productions,\n                &mut non_terminal_map,\n         \
      \       &mut next_non_terminal_id,\n                &mut literal_map,\n    \
      \            &mut tokens,\n                &mut terminal_name_to_group_id,\n\
      \                &mut terminal_expr_to_group_id,\n                &mut next_terminal_id,\n\
      \            );\n            productions.push(Production {\n               \
      \ lhs: NonTerminal(name.clone()),\n                rhs,\n            });\n \
      \       }\n\n        // TODO: this is bad. prob remove this.\n        // crate::debug!(2,\
      \ \"Dropping dead productions\");\n        // let productions = drop_dead(&productions);\n\
      \n        let tokenizer_exprs_vec: Vec<ExprGroup> = tokens\n            .into_iter()\n\
      \            .map(|(_, expr)| greedy_group(expr))\n            .collect();\n\
      \        let tokenizer_expr_groups = groups(tokenizer_exprs_vec);\n        debug!(2,\
      \ \"Building tokenizer\");\n        let tokenizer = tokenizer_expr_groups.clone().build();\n\
      \n        debug!(2, \"Done defining grammar\");\n        Self {\n          \
      \  productions,\n            start_production_id: 0,\n            literal_map,\n\
      \            terminal_name_to_group_id,\n            terminal_expr_to_group_id,\n\
      \            tokenizer,\n        }\n    }\n}\n\nimpl<T: Tokenizer> GrammarConstraint<T>\
      \ {\n    pub fn from_grammar(grammar: Grammar<T>, llm_tokens: LLMTokenMap, eof_llm_token_id:\
      \ usize, max_llm_token_id: usize) -> Self {\n        debug!(2, \"GrammarConstraint::from_grammar\"\
      );\n        let terminal_map = grammar.terminal_name_to_group_id.iter().map(|(name,\
      \ group_id)| { (Terminal(name.clone()), TerminalID(*group_id)) }).collect();\n\
      \        let non_terminal_map = assign_non_terminal_ids(&grammar.productions);\n\
      \        debug!(2, \"Generating GLR parser\");\n        let parser = generate_glr_parser_with_maps(&grammar.productions,\
      \ grammar.start_production_id, terminal_map, non_terminal_map);\n\n        debug!(2,\
      \ \"Precomputing\");\n        let mut precomputed = precompute(&grammar.tokenizer,\
      \ &llm_tokens, LLMTokenID(eof_llm_token_id), max_llm_token_id);\n        debug!(2,\
      \ \"precomputed.len(): {}\", precomputed.len());\n        crate::precompute::precompute_add_eof(&mut\
      \ precomputed, LLMTokenID(eof_llm_token_id), parser.eof_terminal_id.0, max_llm_token_id);\n\
      \        // precompute_add_eof(&mut precomputed, LLMTokenID(eof_llm_token_id),\
      \ llm_tokens.len(), max_llm_token_id);\n        debug!(2, \"precomputed.len():\
      \ {}\", precomputed.len());\n        debug!(2, \"Done precomputing\");\n\n \
      \       // // todo: remove this\n        // debug!(2, \"GrammarConstraint::from_grammar\"\
      );\n        // let terminal_map = grammar.terminal_name_to_group_id.iter().map(|(name,\
      \ group_id)| { (Terminal(name.clone()), TerminalID(*group_id)) }).collect();\n\
      \        // let non_terminal_map = assign_non_terminal_ids(&grammar.productions);\n\
      \        // debug!(2, \"Generating GLR parser\");\n        // let parser = generate_glr_parser_with_maps(&grammar.productions,\
      \ grammar.start_production_id, terminal_map, non_terminal_map);\n\n        Self\
      \ {\n            tokenizer: grammar.tokenizer,\n            parser,\n      \
      \      precomputed,\n            max_llm_token_id,\n        }\n    }\n}\n\n\
      #[cfg(test)]\nmod tests {\n    use std::sync::{Arc, Mutex};\n    use bitvec::prelude::*;\n\
      \    use super::*;\n    use crate::finite_automata::eat_u8;\n    use crate::glr::table::generate_glr_parser;\n\
      \    use crate::precompute::{print_precomputed, LLMTokenID};\n    use crate::{choice_fast,\
      \ groups, seq_fast};\n    use crate::tokenizer_combinators::{eat_u8_fast, eat_u8_negation_fast,\
      \ eat_u8_range_fast, repeat0_fast};\n    use crate::trie::TrieNode;\n\n\n  \
      \  fn bitvec_with_capacity_and_values(capacity: usize, values: Vec<usize>) ->\
      \ BitVec {\n        let mut bitvec = BitVec::new();\n        bitvec.resize(capacity,\
      \ false);\n        for value in values {\n            bitvec.set(value, true);\n\
      \        }\n        bitvec\n    }\n\n    #[test]\n    fn test_grammar_from_exprs()\
      \ {\n        let exprs = vec![\n            (\n                \"E\".to_string(),\n\
      \                choice(vec![\n                    sequence(vec![\n        \
      \                r#ref(\"E\"),\n                        regex(eat_u8(b'+')),\n\
      \                        r#ref(\"T\"),\n                    ]),\n          \
      \          r#ref(\"T\"),\n                ]),\n            ),\n            (\n\
      \                \"T\".to_string(),\n                choice(vec![\n        \
      \            sequence(vec![\n                        r#ref(\"T\"),\n       \
      \                 regex(eat_u8(b'*')),\n                        r#ref(\"F\"\
      ),\n                    ]),\n                    r#ref(\"F\"),\n           \
      \     ]),\n            ),\n            (\n                \"F\".to_string(),\n\
      \                choice(vec![\n                    sequence(vec![\n        \
      \                regex(eat_u8(b'(')),\n                        r#ref(\"E\"),\n\
      \                        regex(eat_u8(b')')),\n                    ]),\n   \
      \                 regex(eat_u8(b'i')),\n                ]),\n            ),\n\
      \        ];\n\n        let grammar = Grammar::from_exprs(exprs.clone());\n \
      \       debug!(2, \"{:?}\", &grammar);\n\n        let parser = grammar.glr_parser();\n\
      \        debug!(2, \"{:?}\", &parser);\n\n        let llm_tokens: Vec<Vec<u8>>\
      \ = vec![b\"i\".to_vec(), b\"+\".to_vec(), b\"*\".to_vec(), b\"(\".to_vec(),\
      \ b\")\".to_vec(), b\"(i\".to_vec(), b\"+i\".to_vec()];\n        let llm_token_map:\
      \ LLMTokenMap = llm_tokens.iter().enumerate().map(|(i, token)| (token.clone(),\
      \ LLMTokenID(i))).collect();\n        let eof_llm_token_id = llm_tokens.len();\n\
      \        let max_llm_token_id = llm_tokens.len();\n        let grammar_constraint\
      \ = GrammarConstraint::from_grammar(grammar, llm_token_map.clone(), eof_llm_token_id,\
      \ max_llm_token_id);\n        let mut grammar_constraint_state = grammar_constraint.init();\n\
      \n        macro_rules! llm_token_vec {\n            ($($token:expr),* $(,)?)\
      \ => {\n                vec![\n                    $(\n                    \
      \    llm_token_map.get_by_left(&$token.to_vec()).unwrap().0,\n             \
      \       )*\n                ]\n            }\n        }\n\n        for (tokenizer_state,\
      \ root) in grammar_constraint_state.get_precomputed() {\n            debug!(3,\
      \ \"Tokenizer state: {}\", tokenizer_state.0);\n            for node in TrieNode::all_nodes(Arc::new(Mutex::new(root.clone())))\
      \ {\n                debug!(3, \"Node address: {:p}, value: {:?}\", Arc::as_ptr(&node),\
      \ node.try_lock().unwrap().value);\n                // print edge values and\
      \ destination addresses\n                for (edge, dest) in node.try_lock().unwrap().children()\
      \ {\n                    debug!(3, \"    Edge value: {:?}, destination address:\
      \ {:p}\", edge, Arc::as_ptr(&dest));\n                }\n            }\n   \
      \     }\n\n        // Get the mask.\n        // The valid LLM tokens initially\
      \ are [\"i\", \"(\", \"(i\"].\n        let mask = grammar_constraint_state.get_mask();\n\
      \        let expected_mask = bitvec_with_capacity_and_values(llm_tokens.len()\
      \ + 1, llm_token_vec!(b\"i\", b\"(\", b\"(i\"));\n        assert_eq!(mask, expected_mask);\n\
      \n        // Simulate generating from a LLM with the grammar constraint.\n \
      \       // We may have some 'prefill' we want to pass to the parser before we\
      \ generate the first new LLM token.\n        // Let's say the prefill is \"\
      (i+i*i\".\n        // This would be best tokenized as [\"(i\", \"+\", \"i\"\
      , \"*\", \"i\"].\n        //\n        // Take note of the ambiguity in the LLM\
      \ tokens; we could the prefill as [\"(\", \"i\", \"+\", \"i\", \"*\", \"i\"\
      ],\n        // i.e. break the \"(i\" token into \"(\" and \"i\". But that's\
      \ a waste of a token.\n        // A good LLM tokenizer would greedily emit the\
      \ longest possible token at each step.\n        let prefill: Vec<_> = llm_token_vec!(b\"\
      (i\", b\"+i\", b\"*\", b\"i\").into_iter().map(|token_id| LLMTokenID(token_id)).collect();\n\
      \        grammar_constraint_state.commit_many(&prefill);\n\n        // Get the\
      \ mask.\n        // The valid LLM tokens right now are [\"+\", \"*\", \")\"\
      , \"+i)\"].\n        let mask = grammar_constraint_state.get_mask();\n     \
      \   let expected_mask = bitvec_with_capacity_and_values(llm_tokens.len() + 1,\
      \ llm_token_vec!(b\"+\", b\"*\", b\")\", b\"+i\"));\n        assert_eq!(mask,\
      \ expected_mask);\n\n        // Finish it\n        let terminals: Vec<_> = llm_token_vec!(b\"\
      )\").into_iter().map(|token_id| LLMTokenID(token_id)).collect();\n        grammar_constraint_state.commit_many(&terminals);\n\
      \        let mask = grammar_constraint_state.get_mask();\n        let mut expected_mask\
      \ = bitvec_with_capacity_and_values(llm_tokens.len() + 1, llm_token_vec!(b\"\
      +\", b\"*\", b\"+i\"));\n        // Add the EOF token\n        expected_mask.set(llm_tokens.len(),\
      \ true);\n        assert_eq!(mask, expected_mask);\n\n    }\n\n    #[test]\n\
      \    fn test_grammar_from_exprs_simple() {\n        let exprs = vec![\n    \
      \        (\n                \"E\".to_string(),\n                sequence(vec![\n\
      \                    regex(eat_u8(b'a')),\n                    regex(eat_u8(b'b')),\n\
      \                ]),\n            ),\n        ];\n\n        let grammar = Grammar::from_exprs(exprs.clone());\n\
      \        dbg!(&grammar);\n\n        let parser = grammar.glr_parser();\n   \
      \     dbg!(&parser);\n\n        let llm_tokens: Vec<Vec<u8>> = vec![b\"a\".to_vec(),\
      \ b\"b\".to_vec()];\n        let llm_token_map: LLMTokenMap = llm_tokens.iter().enumerate().map(|(i,\
      \ token)| (token.clone(), LLMTokenID(i))).collect();\n        let eof_llm_token_id\
      \ = llm_tokens.len();\n        let max_llm_token_id = llm_tokens.len();\n  \
      \      let grammar_constraint = GrammarConstraint::from_grammar(grammar, llm_token_map.clone(),\
      \ eof_llm_token_id, max_llm_token_id);\n        let mut grammar_constraint_state\
      \ = grammar_constraint.init();\n\n        for (tokenizer_state, root) in grammar_constraint_state.get_precomputed()\
      \ {\n            debug!(1, \"Tokenizer state: {}\", tokenizer_state.0);\n  \
      \          for node in TrieNode::all_nodes(Arc::new(Mutex::new(root.clone())))\
      \ {\n                debug!(1, \"Node address: {:p}, value: {:?}\", Arc::as_ptr(&node),\
      \ node.try_lock().unwrap().value);\n                // print edge values and\
      \ destination addresses\n                for (edge, dest) in node.try_lock().unwrap().children()\
      \ {\n                    debug!(1, \"    Edge value: {:?}, destination address:\
      \ {:p}\", edge, Arc::as_ptr(&dest));\n                }\n            }\n   \
      \     }\n\n        macro_rules! llm_token_vec {\n            ($($token:expr),*\
      \ $(,)?) => {\n                vec![\n                    $(\n             \
      \           llm_token_map.get_by_left(&$token.to_vec()).unwrap().0,\n      \
      \              )*\n                ]\n            }\n        }\n\n        //\
      \ Get the mask.\n        let mask = grammar_constraint_state.get_mask();\n \
      \       let expected_mask = bitvec_with_capacity_and_values(llm_tokens.len()\
      \ + 1, llm_token_vec!(b\"a\"));\n        assert_eq!(mask, expected_mask);\n\n\
      \        // Commit \"a\"\n        let terminals: Vec<_> = llm_token_vec!(b\"\
      a\").into_iter().map(|token_id| LLMTokenID(token_id)).collect();\n        grammar_constraint_state.commit_many(&terminals);\n\
      \n        // Get the mask.\n        let mask = grammar_constraint_state.get_mask();\n\
      \        let expected_mask = bitvec_with_capacity_and_values(llm_tokens.len()\
      \ + 1, llm_token_vec!(b\"b\"));\n        assert_eq!(mask, expected_mask);\n\
      \    }\n\n    #[test]\n    fn test_grammar_from_exprs_very_simple() {\n    \
      \    let exprs = vec![\n            (\n                \"E\".to_string(),\n\
      \                regex(eat_u8(b'a')),\n            ),\n        ];\n\n      \
      \  let grammar = Grammar::from_exprs(exprs.clone());\n        dbg!(&grammar);\n\
      \n        let parser = grammar.glr_parser();\n        dbg!(&parser);\n\n   \
      \     let llm_tokens: Vec<Vec<u8>> = vec![b\"a\".to_vec()];\n        let llm_token_map:\
      \ LLMTokenMap = llm_tokens.iter().enumerate().map(|(i, token)| (token.clone(),\
      \ LLMTokenID(i))).collect();\n        let eof_llm_token_id = llm_tokens.len();\n\
      \        let max_llm_token_id = llm_tokens.len();\n        let grammar_constraint\
      \ = GrammarConstraint::from_grammar(grammar, llm_token_map.clone(), eof_llm_token_id,\
      \ max_llm_token_id);\n        let mut grammar_constraint_state = grammar_constraint.init();\n\
      \n        print_precomputed(&grammar_constraint_state.get_precomputed());\n\n\
      \        for (tokenizer_state, root) in grammar_constraint_state.get_precomputed()\
      \ {\n            debug!(1, \"Tokenizer state: {}\", tokenizer_state.0);\n  \
      \          for node in TrieNode::all_nodes(Arc::new(Mutex::new(root.clone())))\
      \ {\n                debug!(1, \"Node address: {:p}, value: {:?}\", Arc::as_ptr(&node),\
      \ node.try_lock().unwrap().value);\n                // print edge values and\
      \ destination addresses\n                for (edge, dest) in node.try_lock().unwrap().children()\
      \ {\n                    debug!(1, \"    Edge value: {:?}, destination address:\
      \ {:p}\", edge, Arc::as_ptr(&dest));\n                }\n            }\n   \
      \     }\n\n        macro_rules! llm_token_vec {\n            ($($token:expr),*\
      \ $(,)?) => {\n                vec![\n                    $(\n             \
      \           llm_token_map.get_by_left(&$token.to_vec()).unwrap().0,\n      \
      \              )*\n                ]\n            }\n        }\n\n        //\
      \ Get the mask.\n        let mask = grammar_constraint_state.get_mask();\n \
      \       let expected_mask = bitvec_with_capacity_and_values(llm_tokens.len()\
      \ + 1, llm_token_vec!(b\"a\"));\n        assert_eq!(mask, expected_mask);\n\n\
      \        // Commit \"a\"\n        let terminals: Vec<_> = llm_token_vec!(b\"\
      a\").into_iter().map(|token_id| LLMTokenID(token_id)).collect();\n        grammar_constraint_state.commit_many(&terminals);\n\
      \n        // Get the mask.\n        let mask = grammar_constraint_state.get_mask();\n\
      \        let mut expected_mask = bitvec_with_capacity_and_values(llm_tokens.len()\
      \ + 1, llm_token_vec!());\n        // Add the EOF token\n        expected_mask.set(llm_tokens.len(),\
      \ true);\n        assert_eq!(mask, expected_mask);\n    }\n\n    #[test]\n \
      \   fn test_precompute_for_python_name_token() {\n        // ignore = rep(choice([\n\
      \        //     eat_u8(ord(\" \")),\n        //     seq([eat_u8(ord(\"#\")),\
      \ rep(eat_u8_negation(ord(\"\\n\"))), eat_u8(ord(\"\\n\"))]),\n        // ]))\n\
      \        // digit = choice([eat_u8(c) for c in range(ord(\"0\"), ord(\"9\")\
      \ + 1)])\n        // alph_lower = choice([eat_u8(c) for c in range(ord(\"a\"\
      ), ord(\"z\") + 1)])\n        // alph_upper = choice([eat_u8(c) for c in range(ord(\"\
      A\"), ord(\"Z\") + 1)])\n        //\n        // name_start = choice([\n    \
      \    //     alph_lower,\n        //     alph_upper,\n        //     eat_u8(ord(\"\
      _\"))\n        // ])\n        // name_middle = choice([\n        //     name_start,\n\
      \        //     digit,\n        // ])\n        let ignore = repeat0_fast(choice_fast!(eat_u8_fast(b'\
      \ '), seq_fast!(eat_u8_fast(b'#'), repeat0_fast(eat_u8_negation_fast(b'\\n')),\
      \ eat_u8_fast(b'\\n'))));\n\n        let digit = eat_u8_range_fast(b'0', b'9');\n\
      \        let alph_lower = eat_u8_range_fast(b'a', b'z');\n        let alph_upper\
      \ = eat_u8_range_fast(b'A', b'Z');\n\n        let name_start = choice_fast!(alph_lower,\
      \ alph_upper, eat_u8_fast(b'_'));\n        let name_middle = choice_fast!(name_start.clone(),\
      \ digit);\n        let name = seq_fast!(ignore, name_start, repeat0_fast(seq_fast!(name_middle)));\n\
      \n        let tokenizer = name.build();\n        dbg!(&tokenizer);\n\n     \
      \   // // Define LLM tokens\n        let llm_tokens: Vec<Vec<u8>> = (0..2).map(|i|\
      \ format!(\"abcdefghijk{}\", i).as_bytes().to_vec()).collect();\n        let\
      \ llm_tokens_slices: Vec<&[u8]> = llm_tokens.iter().map(|token| &token[..]).collect();\n\
      \        let llm_token_map: LLMTokenMap = llm_tokens.iter().enumerate().map(|(i,\
      \ token)| (token.clone(), LLMTokenID(i))).collect();\n        let eof_llm_token_id\
      \ = llm_tokens.len();\n        let max_llm_token_id = llm_tokens.len();\n  \
      \      let precomputed = precompute(&tokenizer, &llm_token_map, LLMTokenID(eof_llm_token_id),\
      \ max_llm_token_id);\n        print_precomputed(&precomputed);\n        println!(\"\
      Done precomputing\");\n        // print_precomputed(&precomputed);\n    }\n\n\
      \    #[test]\n    fn test_precompute_explosion() {\n        // let tokenizer\
      \ = groups![\n        //     seq_fast![eat_u8(b'a'), eat_u8(b'a'), eat_u8(b'a')],\n\
      \        //     seq_fast![eat_u8(b'a'), eat_u8(b'b'), eat_u8(b'a')],\n     \
      \   //     seq_fast![eat_u8(b'b'), eat_u8(b'a'), eat_u8(b'a')],\n        //\
      \     seq_fast![eat_u8(b'b'), eat_u8(b'a'), eat_u8(b'b')],\n        //     seq_fast![eat_u8(b'a'),\
      \ eat_u8(b'a')],\n        //     seq_fast![eat_u8(b'b'), eat_u8(b'a')],\n  \
      \      //     seq_fast![eat_u8(b'b'), eat_u8(b'b')],\n        //     repeat0_fast(seq_fast![eat_u8(b'a'),\
      \ eat_u8(b'a')]),\n        //     repeat0_fast(eat_u8(b'a')),\n        //  \
      \   // eat_u8(b'a'),\n        //     eat_u8(b'a'),\n        //     eat_u8(b'b'),\n\
      \        // ].build();\n        let tokenizer = groups![\n            eat_u8(b'a'),\n\
      \            eat_u8(b'a'),\n        ].build();\n\n        // Define the LLM\
      \ token as 32 'a's\n        // let llm_tokens: Vec<Vec<u8>> = vec![b\"ab\".to_vec(),\
      \ b\"aaaaaaaa\".to_vec(), b\"babababa\".to_vec(), b\"aabbaabb\".to_vec()];\n\
      \        let llm_tokens: Vec<Vec<u8>> = vec![b\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\
      .to_vec()];\n        let llm_token_map: LLMTokenMap = llm_tokens.iter().enumerate().map(|(i,\
      \ token)| (token.clone(), LLMTokenID(i))).collect();\n        let eof_llm_token_id\
      \ = llm_tokens.len();\n        let max_llm_token_id = llm_tokens.len();\n  \
      \      let precomputed = precompute(&tokenizer, &llm_token_map, LLMTokenID(eof_llm_token_id),\
      \ max_llm_token_id);\n        print_precomputed(&precomputed);\n        println!(\"\
      Done precomputing\");\n    }\n}\n\n```\n\n```rust\nuse crate::glr::grammar::{Production,\
      \ Symbol};\nuse std::collections::{BTreeMap, BTreeSet, VecDeque};\n\n\n#[derive(Debug,\
      \ Clone, PartialEq, Eq, PartialOrd, Ord)]\npub struct Item {\n    pub production:\
      \ Production,\n    pub dot_position: usize,\n}\n\npub fn compute_closure(items:\
      \ &BTreeSet<Item>, productions: &[Production]) -> BTreeSet<Item> {\n    crate::debug!(0,\
      \ \"Computing closure\");\n    let mut closure = items.clone();\n    let mut\
      \ worklist: VecDeque<Item> = items.iter().cloned().collect();\n\n    while let\
      \ Some(item) = worklist.pop_front() {\n        if let Some(Symbol::NonTerminal(nt))\
      \ = item.production.rhs.get(item.dot_position) {\n            for prod in productions.iter().filter(|p|\
      \ p.lhs == *nt) {\n                let new_item = Item {\n                 \
      \   production: prod.clone(),\n                    dot_position: 0,\n      \
      \          };\n                // Directly add the new item without checking\
      \ for existence\n                if closure.insert(new_item.clone()) {\n   \
      \                 worklist.push_back(new_item);\n                }\n       \
      \     }\n        }\n    }\n\n    crate::debug!(1, \"Done computing closure\"\
      );\n    closure\n}\n\n/// Computes the GOTO function for a set of LR(0) items.\n\
      pub fn compute_goto(items: &BTreeSet<Item>) -> BTreeSet<Item> {\n    let mut\
      \ result = BTreeSet::new();\n    for item in items {\n        if item.dot_position\
      \ < item.production.rhs.len() {\n            result.insert(Item {\n        \
      \        production: item.production.clone(),\n                dot_position:\
      \ item.dot_position + 1,\n            });\n        }\n    }\n    result\n}\n\
      \n/// Splits a set of LR(0) items based on the symbol after the dot.\npub fn\
      \ split_on_dot(items: &BTreeSet<Item>) -> BTreeMap<Option<Symbol>, BTreeSet<Item>>\
      \ {\n    let mut result: BTreeMap<Option<Symbol>, BTreeSet<Item>> = BTreeMap::new();\n\
      \    for item in items {\n        result\n            .entry(item.production.rhs.get(item.dot_position).cloned())\n\
      \            .or_default()\n            .insert(item.clone());\n    }\n    result\n\
      }\n// src/glr/items.rs\n\n```\n\n```rust\n// python/src/lib.rs\nuse sep1::finite_automata::{Expr\
      \ as RegexExpr, ExprGroups as RegexGroups, greedy_group, non_greedy_group, groups\
      \ as regex_groups, _choice as regex_choice, eat_u8, eat_u8_negation, eat_u8_set,\
      \ eps, opt, prec, rep, rep1, _seq as regex_seq};\nuse sep1::finite_automata::Regex;\n\
      use pyo3::prelude::*;\nuse pyo3::types::{PyByteArray, PyBytes, PyDict};\nuse\
      \ sep1::glr::grammar::{NonTerminal, Production, Symbol, Terminal};\nuse sep1::glr::parser::GLRParser;\n\
      use sep1::glr::table::{generate_glr_parser, StateID};\nuse sep1::interface::{Grammar,\
      \ GrammarExpr, choice as grammar_choice, optional as grammar_optional, regex\
      \ as grammar_regex, repeat as grammar_repeat, r#ref as grammar_ref, sequence\
      \ as grammar_sequence};\nuse sep1::constraint::{GrammarConstraint, GrammarConstraintState};\n\
      use sep1::precompute::{print_precomputed, LLMTokenID, Tokenizer};\nuse std::collections::{BTreeMap,\
      \ BTreeSet};\nuse bimap::BiBTreeMap;\nuse numpy::{IntoPyArray, PyArray1, ToPyArray};\n\
      use sep1::u8set::U8Set;\n\n#[pyclass]\n#[derive(Clone)]\nstruct PyGrammarExpr\
      \ {\n    inner: GrammarExpr,\n}\n\n#[pymethods]\nimpl PyGrammarExpr {\n    #[staticmethod]\n\
      \    fn r#ref(name: &str) -> PyResult<Self> {\n        Ok(Self {\n         \
      \   inner: grammar_ref(name),\n        })\n    }\n\n    #[staticmethod]\n  \
      \  fn sequence(exprs: Vec<PyGrammarExpr>) -> Self {\n        Self {\n      \
      \      inner: grammar_sequence(exprs.into_iter().map(|e| e.inner).collect()),\n\
      \        }\n    }\n\n    #[staticmethod]\n    fn choice(exprs: Vec<PyGrammarExpr>)\
      \ -> Self {\n        Self {\n            inner: grammar_choice(exprs.into_iter().map(|e|\
      \ e.inner).collect()),\n        }\n    }\n\n    #[staticmethod]\n    fn optional(expr:\
      \ PyGrammarExpr) -> Self {\n        Self {\n            inner: grammar_optional(expr.inner),\n\
      \        }\n    }\n\n    #[staticmethod]\n    fn repeat(expr: PyGrammarExpr)\
      \ -> Self {\n        Self {\n            inner: grammar_repeat(expr.inner),\n\
      \        }\n    }\n\n    #[staticmethod]\n    fn regex(regex: PyRegexExpr) ->\
      \ Self {\n        Self {\n            inner: grammar_regex(regex.inner)\n  \
      \      }\n    }\n}\n\n#[pyclass]\n#[derive(Clone)]\nstruct PyRegexExpr {\n \
      \   inner: RegexExpr,\n}\n\n#[pymethods]\nimpl PyRegexExpr {\n    #[staticmethod]\n\
      \    fn eat_u8(c: u8) -> Self {\n        Self { inner: eat_u8(c) }\n    }\n\n\
      \    #[staticmethod]\n    fn eat_u8_negation(c: u8) -> Self {\n        Self\
      \ { inner: eat_u8_negation(c) }\n    }\n\n    #[staticmethod]\n    fn rep(expr:\
      \ PyRegexExpr) -> Self {\n        Self { inner: rep(expr.inner) }\n    }\n\n\
      \    #[staticmethod]\n    fn rep1(expr: PyRegexExpr) -> Self {\n        Self\
      \ { inner: rep1(expr.inner) }\n    }\n\n    #[staticmethod]\n    fn opt(expr:\
      \ PyRegexExpr) -> Self {\n        Self { inner: opt(expr.inner) }\n    }\n\n\
      \    #[staticmethod]\n    fn prec(precedence: isize, expr: PyRegexExpr) -> PyRegexGroup\
      \ {\n        PyRegexGroup { inner: prec(precedence, expr.inner) }\n    }\n\n\
      \    #[staticmethod]\n    fn eps() -> Self {\n        Self { inner: eps() }\n\
      \    }\n\n    #[staticmethod]\n    fn seq(exprs: Vec<PyRegexExpr>) -> Self {\n\
      \        Self { inner: regex_seq(exprs.into_iter().map(|e| e.inner).collect())\
      \ }\n    }\n\n    #[staticmethod]\n    fn choice(exprs: Vec<PyRegexExpr>) ->\
      \ Self {\n        Self { inner: regex_choice(exprs.into_iter().map(|e| e.inner).collect())\
      \ }\n    }\n\n    fn build(&self) -> PyResult<PyRegex> {\n        Ok(PyRegex\
      \ { inner: self.inner.clone().build() })\n    }\n}\n\n#[pyclass]\n#[derive(Clone)]\n\
      struct PyRegexGroup {\n    inner: sep1::finite_automata::ExprGroup,\n}\n\n#[pymethods]\n\
      impl PyRegexGroup {\n    #[staticmethod]\n    fn greedy_group(expr: PyRegexExpr)\
      \ -> Self {\n        Self { inner: greedy_group(expr.inner) }\n    }\n\n   \
      \ #[staticmethod]\n    fn non_greedy_group(expr: PyRegexExpr) -> Self {\n  \
      \      Self { inner: non_greedy_group(expr.inner) }\n    }\n}\n\n#[pyclass]\n\
      #[derive(Clone)]\nstruct PyRegexGroups {\n    inner: RegexGroups,\n}\n\n#[pymethods]\n\
      impl PyRegexGroups {\n    #[staticmethod]\n    fn groups(groups: Vec<PyRegexGroup>)\
      \ -> Self {\n        Self {\n            inner: regex_groups(groups.into_iter().map(|g|\
      \ g.inner).collect()),\n        }\n    }\n\n    fn build(&self) -> PyResult<PyRegex>\
      \ { // &self, not self\n        Ok(PyRegex { inner: self.inner.clone().build()\
      \ }) // clone the inner RegexExpr\n    }\n}\n\n#[pyclass]\n#[derive(Clone)]\n\
      pub struct PyRegex {\n    inner: Regex,\n}\n\n#[pymethods]\nimpl PyRegex {\n\
      \    // Add methods here as needed to expose Regex functionality to Python\n\
      }\n\n\n#[pyclass]\n#[derive(Clone)]\npub struct PyGrammar {\n    inner: Grammar<Regex>,\n\
      }\n\n#[pymethods]\nimpl PyGrammar {\n    #[new]\n    fn new(exprs: Vec<(String,\
      \ PyGrammarExpr)>) -> Self {\n        let inner = Grammar::from_exprs(exprs.into_iter().map(|(s,\
      \ e)| (s, e.inner)).collect());\n        Self { inner }\n    }\n\n    fn glr_parser(&self)\
      \ -> PyGLRParser {\n        PyGLRParser { inner: self.inner.glr_parser() }\n\
      \    }\n\n    fn print(&self) {\n        println!(\"{:?}\", self.inner)\n  \
      \  }\n}\n\n#[pyclass]\n#[derive(Clone)]\npub struct PyGLRParser {\n    inner:\
      \ GLRParser,\n}\n\n#[pyclass]\n#[derive(Clone)]\npub struct PyGrammarConstraint\
      \ {\n    inner: GrammarConstraint<Regex>,\n}\n\n#[pymethods]\nimpl PyGrammarConstraint\
      \ {\n    #[new]\n    fn new(py: Python, grammar: PyGrammar, token_to_id: &PyDict,\
      \ eof_llm_token_id: usize, max_llm_token_id: usize) -> PyResult<Self> {\n  \
      \      // Convert the Python dictionary into a BiBTreeMap\n        let mut llm_token_map:\
      \ BiBTreeMap<Vec<u8>, LLMTokenID> = BiBTreeMap::new();\n        for (key, value)\
      \ in token_to_id.iter() {\n            let token = key.extract::<&[u8]>()?;\n\
      \            let id = value.extract::<usize>()?;\n            llm_token_map.insert(token.to_vec(),\
      \ LLMTokenID(id));\n        }\n\n        let inner = GrammarConstraint::from_grammar(grammar.inner,\
      \ llm_token_map, eof_llm_token_id, max_llm_token_id);\n        Ok(Self { inner\
      \ })\n    }\n\n    fn print(&self) {\n        print_precomputed(&self.inner.precomputed);\n\
      \    }\n}\n\n\n#[pyclass]\npub struct PyGrammarConstraintState {\n    inner:\
      \ GrammarConstraintState<Regex>,\n}\n\n#[pymethods]\nimpl PyGrammarConstraintState\
      \ {\n    #[new]\n    fn new(grammar_constraint: PyGrammarConstraint) -> Self\
      \ {\n        Self { inner: grammar_constraint.inner.init() }\n    }\n\n    fn\
      \ get_mask<'py>(&self, py: Python<'py>) -> PyResult<Bound<'py, PyArray1<bool>>>\
      \ { // Correct return type\n        let bitset = self.inner.get_mask();\n  \
      \      let bools: Vec<bool> = bitset.iter().map(|bit_ref| *bit_ref).collect();\n\
      \        let array = bools.into_pyarray_bound(py); // Correct usage\n      \
      \  Ok(array)\n    }\n\n    fn commit(&mut self, llm_token_id: usize) {\n   \
      \     self.inner.commit(LLMTokenID(llm_token_id));\n    }\n}\n\n\n\n/// A Python\
      \ module implemented in Rust.\n#[pymodule]\nfn _sep1(m: &Bound<'_, PyModule>)\
      \ -> PyResult<()> {\n    m.add_class::<PyGrammarExpr>()?;\n    m.add_class::<PyRegexExpr>()?;\n\
      \    m.add_class::<PyRegexGroup>()?;\n    m.add_class::<PyRegexGroups>()?;\n\
      \    m.add_class::<PyGrammar>()?;\n    m.add_class::<PyGrammarConstraint>()?;\n\
      \    m.add_class::<PyGrammarConstraintState>()?;\n    Ok(())\n}\n```\n\n```rust\n\
      extern crate core;\n\npub mod frozenset;\npub mod charmap;\npub mod precompute;\n\
      pub mod tokenizer_combinators;\npub mod u8set;\npub mod finite_automata;\nmod\
      \ gss;\npub mod glr;\npub mod constraint;\npub mod interface;\nmod precompute_gss;\n\
      mod trie;\nmod utils;\nmod analyze_grammar;\n\n```\n\n```rust\npub mod table;\n\
      pub mod grammar;\npub mod items;\npub mod parser;\nmod tests;\n```\n\n```rust\n\
      use crate::gss::BulkMerge;\nuse crate::glr::grammar::{NonTerminal, Production,\
      \ Symbol, Terminal};\nuse crate::glr::items::Item;\nuse crate::glr::table::{NonTerminalID,\
      \ ProductionID, Stage7ShiftsAndReduces, Stage7Table, StateID, TerminalID};\n\
      use crate::gss::{GSSNode, GSSTrait};\n\nuse bimap::BiBTreeMap;\nuse std::collections::{BTreeMap,\
      \ BTreeSet};\nuse std::fmt::{Debug, Display, Formatter};\nuse std::sync::Arc;\n\
      use crate::debug;\n\n#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]\n\
      pub struct ParseState {\n    pub stack: Arc<GSSNode<StateID>>,\n    pub action_stack:\
      \ Option<Arc<GSSNode<Action>>>,\n    pub status: ParseStatus,\n}\n\n#[derive(Debug,\
      \ Clone, PartialEq, Eq, PartialOrd, Ord)]\npub enum Action {\n    Shift(TerminalID),\n\
      \    Reduce { production_id: ProductionID, len: usize, nonterminal_id: NonTerminalID\
      \ },\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub\
      \ enum ParseStatus {\n    Active,\n    Inactive(StopReason),\n}\n\n#[derive(Debug,\
      \ Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub enum StopReason {\n   \
      \ ActionNotFound,\n    GotoNotFound,\n}\n\n\n// TODO: should this *really* derive\
      \ `Clone`? Users probably shouldn't clone this, should they?\n#[derive(Clone)]\n\
      pub struct GLRParser {\n    pub stage_7_table: Stage7Table,\n    pub productions:\
      \ Vec<Production>,\n    pub terminal_map: BiBTreeMap<Terminal, TerminalID>,\n\
      \    pub non_terminal_map: BiBTreeMap<NonTerminal, NonTerminalID>,\n    pub\
      \ item_set_map: BiBTreeMap<BTreeSet<Item>, StateID>,\n    pub start_state_id:\
      \ StateID,\n    pub eof_terminal_id: TerminalID,\n}\n\nimpl GLRParser {\n  \
      \  pub fn new(\n        stage_7_table: Stage7Table,\n        productions: Vec<Production>,\n\
      \        terminal_map: BiBTreeMap<Terminal, TerminalID>,\n        non_terminal_map:\
      \ BiBTreeMap<NonTerminal, NonTerminalID>,\n        item_set_map: BiBTreeMap<BTreeSet<Item>,\
      \ StateID>,\n        start_state_id: StateID,\n        eof_terminal_id: TerminalID,\n\
      \    ) -> Self {\n        Self {\n            stage_7_table,\n            productions,\n\
      \            terminal_map,\n            non_terminal_map,\n            item_set_map,\n\
      \            start_state_id,\n            eof_terminal_id,\n        }\n    }\n\
      \n    pub fn init_glr_parser(&self) -> GLRParserState {\n        GLRParserState\
      \ {\n            parser: self,\n            active_states: vec![self.init_parse_state()],\n\
      \            inactive_states: BTreeMap::new(),\n            input_pos: 0,\n\
      \        }\n    }\n    \n    pub fn init_glr_parser_from_parse_state(&self,\
      \ parse_state: ParseState) -> GLRParserState {\n        GLRParserState {\n \
      \           parser: self,\n            active_states: vec![parse_state],\n \
      \           inactive_states: BTreeMap::new(),\n            input_pos: 0,\n \
      \       }\n    }\n\n    pub fn init_glr_parser_from_parse_states(&self, parse_states:\
      \ Vec<ParseState>) -> GLRParserState {\n        GLRParserState {\n         \
      \   parser: self,\n            active_states: parse_states,\n            inactive_states:\
      \ BTreeMap::new(),\n            input_pos: 0,\n        }\n    }\n\n    pub fn\
      \ init_parse_state(&self) -> ParseState {\n        ParseState {\n          \
      \  stack: Arc::new(GSSNode::new(self.start_state_id)),\n            action_stack:\
      \ None,\n            status: ParseStatus::Active,\n        }\n    }\n\n    pub\
      \ fn parse(&self, input: &[TerminalID]) -> GLRParserState {\n        let mut\
      \ state = self.init_glr_parser();\n        state.parse(input);\n        state\n\
      \    }\n}\n\n\nimpl Debug for GLRParser {\n    fn fmt(&self, f: &mut Formatter<'_>)\
      \ -> std::fmt::Result {\n        // Use Display\n        write!(f, \"{}\", self)\n\
      \    }\n}\n\nimpl Display for GLRParser {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>)\
      \ -> std::fmt::Result {\n        let stage_7_table = &self.stage_7_table;\n\
      \        let terminal_map = &self.terminal_map;\n        let non_terminal_map\
      \ = &self.non_terminal_map;\n        let item_set_map = &self.item_set_map;\n\
      \n        writeln!(f, \"Parse Table:\")?;\n        writeln!(f, \"  Start State:\
      \ {}\", self.start_state_id.0)?;\n        for (&state_id, row) in stage_7_table.iter().collect::<BTreeMap<_,\
      \ _>>() {\n            writeln!(f, \"  State {}:\", state_id.0)?;\n\n      \
      \      writeln!(f, \"    Items:\")?;\n            let item_set = item_set_map.get_by_right(&state_id).unwrap();\n\
      \            for item in item_set {\n                write!(f, \"      - {}\
      \ ->\", item.production.lhs.0)?;\n                for (i, symbol) in item.production.rhs.iter().enumerate()\
      \ {\n                    if i == item.dot_position {\n                     \
      \   write!(f, \" •\")?;\n                    }\n                    match symbol\
      \ {\n                        Symbol::Terminal(terminal) => {\n             \
      \               write!(f, \" {:?}\", terminal.0)?;\n                       \
      \ }\n                        Symbol::NonTerminal(non_terminal) => {\n      \
      \                      write!(f, \" {}\", non_terminal.0)?;\n              \
      \          }\n                    }\n                }\n                if item.dot_position\
      \ == item.production.rhs.len() {\n                    write!(f, \" •\")?;\n\
      \                }\n                writeln!(f)?;\n            }\n\n       \
      \     writeln!(f, \"    Actions:\")?;\n            for (&terminal_id, action)\
      \ in &row.shifts_and_reduces {\n                let terminal = terminal_map.get_by_right(&terminal_id).unwrap();\n\
      \                match action {\n                    Stage7ShiftsAndReduces::Shift(next_state_id)\
      \ => {\n                        writeln!(f, \"      - {:?} -> Shift {}\", terminal.0,\
      \ next_state_id.0)?;\n                    }\n                    Stage7ShiftsAndReduces::Reduce\
      \ { production_id, nonterminal_id: nonterminal, len } => {\n               \
      \         let nt = non_terminal_map.get_by_right(nonterminal).unwrap();\n  \
      \                      writeln!(f, \"      - {:?} -> Reduce {} (len {})\", terminal.0,\
      \ nt.0, len)?;\n                    }\n                    Stage7ShiftsAndReduces::Split\
      \ { shift, reduces } => {\n                        writeln!(f, \"      - {:?}\
      \ -> Conflict:\", terminal.0)?;\n                        if let Some(shift_state)\
      \ = shift {\n                            writeln!(f, \"        - Shift {}\"\
      , shift_state.0)?;\n                        }\n                        for (len,\
      \ nt_id_to_prod_ids) in reduces {\n                            writeln!(f, \"\
      \        - Reduce (len {}):\", len)?;\n                            for (nt_id,\
      \ prod_ids) in nt_id_to_prod_ids {\n                                let nt =\
      \ non_terminal_map.get_by_right(nt_id).unwrap();\n                         \
      \       for prod_id in prod_ids {\n                                    let prod\
      \ = self.productions.get(prod_id.0).unwrap();\n                            \
      \        writeln!(f, \"          - {} -> {}\", nt.0, prod.lhs.0)?;\n       \
      \                         }\n                            }\n\n             \
      \           }\n                    }\n                }\n            }\n\n \
      \           writeln!(f, \"    Gotos:\")?;\n            for (&non_terminal_id,\
      \ &next_state_id) in &row.gotos {\n                let non_terminal = non_terminal_map.get_by_right(&non_terminal_id).unwrap();\n\
      \                writeln!(f, \"      - {} -> {}\", non_terminal.0, next_state_id.0)?;\n\
      \            }\n        }\n\n        writeln!(f, \"\\nTerminal Map (name to\
      \ terminal ID):\")?;\n        for (terminal, terminal_id) in terminal_map {\n\
      \            writeln!(f, \"  {} -> {}\", terminal.0, terminal_id.0)?;\n    \
      \    }\n\n        writeln!(f, \"\\nNon-Terminal Map:\")?;\n        for (non_terminal,\
      \ non_terminal_id) in non_terminal_map {\n            writeln!(f, \"  {} ->\
      \ {}\", non_terminal.0, non_terminal_id.0)?;\n        }\n\n        Ok(())\n\
      \    }\n}\n\n\n#[derive(Clone)]\npub struct GLRParserState<'a> {\n    pub parser:\
      \ &'a GLRParser,\n    pub active_states: Vec<ParseState>,\n    pub inactive_states:\
      \ BTreeMap<usize, Vec<ParseState>>,\n    pub input_pos: usize,\n}\n\n\n\nimpl<'a>\
      \ GLRParserState<'a> {\n    pub fn parse(&mut self, input: &[TerminalID]) {\n\
      \        self.parse_part(input);\n        self.parse_eof();\n    }\n\n    pub\
      \ fn parse_part(&mut self, input: &[TerminalID]) {\n        for &token_id in\
      \ input {\n            self.step(token_id);\n        }\n    }\n\n    pub fn\
      \ parse_eof(&mut self) {\n        self.step(self.parser.eof_terminal_id);\n\
      \    }\n\n\n    pub fn step(&mut self, token_id: TerminalID) {\n        let\
      \ mut next_active_states = Vec::new();\n        let mut inactive_states = Vec::new();\n\
      \n        while let Some(state) = self.active_states.pop() {\n            let\
      \ stack = state.stack;\n            let action_stack = state.action_stack;\n\
      \            let state_id = *stack.peek();\n\n            let row = self.parser.stage_7_table.get(&state_id).unwrap();\n\
      \n            if let Some(action) = row.shifts_and_reduces.get(&token_id) {\n\
      \                match action {\n                    Stage7ShiftsAndReduces::Shift(next_state_id)\
      \ => {\n                        debug!(3, \"Shifting\");\n                 \
      \       let new_stack = stack.push(*next_state_id);\n                      \
      \  let new_actions = action_stack.push(Action::Shift(token_id));\n         \
      \               next_active_states.push(ParseState {\n                     \
      \       stack: Arc::new(new_stack),\n                            action_stack:\
      \ Some(Arc::new(new_actions)),\n                            status: ParseStatus::Active,\n\
      \                        });\n                    }\n                    Stage7ShiftsAndReduces::Reduce\
      \ { production_id, nonterminal_id: nonterminal, len } => {\n               \
      \         debug!(3, \"Reducing by production {:?} with len {}\", production_id,\
      \ len);\n                        let mut popped_stack_nodes = stack.popn(*len);\n\
      \                        popped_stack_nodes.bulk_merge();\n                \
      \        for stack_node in popped_stack_nodes {\n                          \
      \  let revealed_state = *stack_node.peek();\n                            let\
      \ goto_row = self.parser.stage_7_table.get(&revealed_state).unwrap();\n\n  \
      \                          if let Some(&goto_state) = goto_row.gotos.get(nonterminal)\
      \ {\n                                debug!(3, \"Going to state {:?}\", goto_state);\n\
      \                                let new_stack = stack_node.push(goto_state);\n\
      \                                let new_actions = action_stack.clone().push(Action::Reduce\
      \ { production_id: *production_id, len: *len, nonterminal_id: *nonterminal });\n\
      \                                self.active_states.push(ParseState {\n    \
      \                                stack: Arc::new(new_stack),\n             \
      \                       action_stack: Some(Arc::new(new_actions)),\n       \
      \                             status: ParseStatus::Active,\n               \
      \                 });\n                            } else {\n              \
      \                  inactive_states.push(ParseState {\n                     \
      \               stack: stack_node,\n                                    action_stack:\
      \ None,\n                                    status: ParseStatus::Inactive(StopReason::GotoNotFound),\n\
      \                                });\n                            }\n      \
      \                  }\n                    }\n                    Stage7ShiftsAndReduces::Split\
      \ { shift, reduces } => {\n                        debug!(3, \"Split\");\n \
      \                       if let Some(shift_state) = shift {\n               \
      \             let new_stack = stack.push(*shift_state);\n                  \
      \          let new_actions = action_stack.clone().push(Action::Shift(token_id));\n\
      \n                            next_active_states.push(ParseState {\n       \
      \                         stack: Arc::new(new_stack),\n                    \
      \            action_stack: Some(Arc::new(new_actions)),\n                  \
      \              status: ParseStatus::Active,\n                            });\n\
      \                        }\n\n                        for (len, nt_ids) in reduces\
      \ {\n                            let mut popped_stack_nodes = stack.popn(*len);\n\
      \                            popped_stack_nodes.bulk_merge();\n            \
      \                for (nt_id, prod_ids) in nt_ids {\n                       \
      \         for stack_node in &popped_stack_nodes {\n                        \
      \            let revealed_state = *stack_node.peek();\n                    \
      \                let goto_row = self.parser.stage_7_table.get(&revealed_state).unwrap();\n\
      \                                    if let Some(&goto_state) = goto_row.gotos.get(nt_id)\
      \ {\n                                        let new_stack = Arc::new(stack_node.push(goto_state));\n\
      \                                        for prod_id in prod_ids {\n       \
      \                                     let new_actions = action_stack.clone().push(Action::Reduce\
      \ { production_id: *prod_id, len: *len, nonterminal_id: *nt_id });\n       \
      \                                     self.active_states.push(ParseState {\n\
      \                                                stack: new_stack.clone(),\n\
      \                                                action_stack: Some(Arc::new(new_actions)),\n\
      \                                                status: ParseStatus::Active,\n\
      \                                            });\n                         \
      \               }\n                                    } else {\n          \
      \                              inactive_states.push(ParseState {\n         \
      \                                   stack: stack_node.clone(),\n           \
      \                                 action_stack: action_stack.clone(),\n    \
      \                                        status: ParseStatus::Inactive(StopReason::GotoNotFound),\n\
      \                                        });\n                             \
      \       }\n                                }\n                            }\n\
      \                        }\n                    }\n                }\n     \
      \       } else {\n                inactive_states.push(ParseState {\n      \
      \              stack,\n                    action_stack,\n                 \
      \   status: ParseStatus::Inactive(StopReason::ActionNotFound),\n           \
      \     });\n            }\n        }\n        self.active_states = next_active_states;\n\
      \        self.inactive_states.insert(self.input_pos, inactive_states);\n\n \
      \       if token_id != self.parser.eof_terminal_id {\n            self.input_pos\
      \ += 1;\n        }\n    }\n\n    pub fn merge_active_states(&mut self) {\n \
      \       let mut active_state_map: BTreeMap<ParseStateKey, ParseState> = BTreeMap::new();\n\
      \n        let mut new_active_states = Vec::new();\n\n        for mut state in\
      \ std::mem::take(&mut self.active_states) {\n            let key = state.key();\n\
      \            if let Some(existing) = active_state_map.get_mut(&key) {\n    \
      \            Arc::make_mut(&mut existing.stack).merge(state.stack.as_ref().clone());\n\
      \                if let Some(existing_action_stack) = existing.action_stack.as_mut()\
      \ {\n                    Arc::make_mut(existing_action_stack).merge(state.action_stack.unwrap().as_ref().clone());\n\
      \                }\n            } else {\n                active_state_map.insert(key,\
      \ state.clone());\n                new_active_states.push(state);\n        \
      \    }\n        }\n        self.active_states = new_active_states;\n    }\n\n\
      \    pub fn fully_matches(&self) -> bool {\n        !self.fully_matching_states().is_empty()\n\
      \    }\n\n    pub fn fully_matching_states(&self) -> Vec<&ParseState> {\n  \
      \      self.inactive_states.get(&self.input_pos).map_or(vec![], |states| {\n\
      \            states.iter().filter(|state| state.status == ParseStatus::Inactive(StopReason::GotoNotFound)).collect()\n\
      \        })\n    }\n\n    pub fn is_ok(&self) -> bool {\n        !self.active_states.is_empty()\
      \ || self.fully_matches()\n    }\n}\n\n#[derive(Debug, Clone, PartialEq, Eq,\
      \ PartialOrd, Ord)]\npub struct ParseStateKey {\n    stack: StateID,\n    action_stack:\
      \ Option<Action>,\n}\n\nimpl ParseState {\n    pub fn key(&self) -> ParseStateKey\
      \ {\n        ParseStateKey {\n            stack: *self.stack.peek(),\n     \
      \       action_stack: self.action_stack.peek().cloned(),\n        }\n    }\n\
      \n    pub fn merge(&mut self, other: ParseState) {\n        assert_eq!(self.key(),\
      \ other.key());\n        Arc::make_mut(&mut self.stack).merge(Arc::unwrap_or_clone(other.stack));\n\
      \        match (&mut self.action_stack, other.action_stack) {\n            (Some(a),\
      \ Some(b)) => {\n                Arc::make_mut(a).merge(Arc::unwrap_or_clone(b));\n\
      \            }\n            (None, None) => {}\n            _ => unreachable!(),\n\
      \        }\n    }\n}\n\npub trait InsertWith<K, V> {\n    fn insert_with<F:\
      \ FnOnce(&mut V, V)>(&mut self, k: K, v: V, combine: F);\n}\n\nimpl<K, V> InsertWith<K,\
      \ V> for BTreeMap<K, V> where K: Eq + Ord {\n    fn insert_with<F: FnOnce(&mut\
      \ V, V)>(&mut self, k: K, v: V, combine: F) {\n        match self.entry(k) {\n\
      \            std::collections::btree_map::Entry::Occupied(mut occupied) => {\n\
      \                let value = occupied.get_mut();\n                combine(value,\
      \ v);\n            }\n            std::collections::btree_map::Entry::Vacant(vacant)\
      \ => {\n                vacant.insert(v);\n            }\n        }\n    }\n\
      }\n\n```\n\n```rust\nuse crate::finite_automata::{GroupID, Regex};\nuse crate::glr;\n\
      use crate::glr::table::StateID;\nuse std::collections::{BTreeMap, BTreeSet};\n\
      use std::sync::{Arc, Mutex};\nuse bitvec::prelude::BitVec;\nuse kdam::tqdm;\n\
      use crate::trie::{TrieNode};\nuse bimap::BiBTreeMap;\nuse bitvec::bitvec;\n\
      use crate::debug;\n\npub type TokenID = usize;\n\n#[derive(Debug, Clone, Copy,\
      \ PartialEq, Eq, PartialOrd, Ord, Hash)]\npub struct LLMTokenID(pub usize);\n\
      \n/// Represents a token with its ID and width.\n#[derive(Clone, Debug, PartialEq,\
      \ Eq, PartialOrd, Ord)]\npub struct Token {\n    pub id: GroupID,\n    pub width:\
      \ usize,\n}\n\n/// Represents the result of executing the tokenizer from a specific\
      \ state.\npub struct ExecuteResult {\n    pub matches: Vec<Token>,\n    pub\
      \ new_state: Option<usize>,\n}\n\n// TODO: get rid of this trait. Just implement\
      \ it directly on the Tokenizer struct.\n/// Trait defining the tokenizer behavior.\n\
      pub trait Tokenizer: Sized {\n    /// Returns the initial state ID.\n    fn\
      \ initial_state_id(&self) -> usize;\n\n    /// Executes the tokenizer on the\
      \ given text starting from the specified state.\n    /// Returns all possible\
      \ next tokens (**not** a sequence of tokens).\n    fn execute_from_state(&self,\
      \ text: &[u8], state: usize) -> ExecuteResult;\n\n    /// Returns the list of\
      \ tokens accessible from the given state.\n    fn tokens_accessible_from_state(&self,\
      \ state: usize) -> Vec<TokenID>;\n\n    /// Returns the maximum state ID in\
      \ the DFA.\n    fn max_state(&self) -> usize;\n\n    /// Executes the tokenizer\
      \ on the entire string and returns all possible token sequences and final states.\n\
      \    fn execute_all_from_state(\n        &self,\n        text: &[u8],\n    \
      \    state: usize,\n        state_map_root_arc: Arc<Mutex<TrieNode<GroupID,\
      \ (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>>,\n\
      \        llm_token_id: LLMTokenID,\n        max_llm_token_id: usize,\n    )\
      \ {\n        // (position, state) -> [node]\n        let mut queue: BTreeMap<(usize,\
      \ Option<usize>), BTreeMap<_, _>> = BTreeMap::new();\n\n        let mut queue_positions:\
      \ BTreeMap<*const Mutex<TrieNode<GroupID, (BTreeMap<LLMTokenID, Option<StateID>>,\
      \ BTreeMap<TokenID, BitVec>, Option<BitVec>)>>, (usize, Option<usize>)> = BTreeMap::new();\n\
      \        let mut new_nodes_for_positions: BTreeMap<(usize, Option<usize>), Arc<Mutex<TrieNode<GroupID,\
      \ (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>>>\
      \ = BTreeMap::new();\n\n        let root = state_map_root_arc.clone();\n\n \
      \       // Initialize the queue with the starting state\n        queue.insert((0,\
      \ Some(state)), BTreeMap::from([(Arc::as_ptr(&root), root.clone())]));\n   \
      \     queue_positions.insert(Arc::as_ptr(&root), (0, Some(state)));\n\n    \
      \    while let Some(((position, maybe_state), nodes)) = queue.pop_first() {\n\
      \            for (_, node) in nodes {\n                if position == text.len()\
      \ {\n                    let mut node_guard = node.try_lock().unwrap();\n  \
      \                  assert!(!node_guard.value.0.contains_key(&llm_token_id));\n\
      \                    node_guard.value.0.insert(llm_token_id, maybe_state.map(StateID));\n\
      \                    if let Some(state) = maybe_state {\n                  \
      \      for possible_grammar_token_id in &self.tokens_accessible_from_state(state)\
      \ {\n                            node_guard.value.1.entry(*possible_grammar_token_id).or_insert_with(||\
      \ {\n                                let mut bitset = BitVec::new();\n     \
      \                           bitset.resize(max_llm_token_id + 1, false);\n  \
      \                              bitset\n                            }).set(llm_token_id.0,\
      \ true);\n                        }\n                    } else {\n        \
      \                node_guard.value.2.get_or_insert_with(|| {\n              \
      \              let mut bitset = BitVec::new();\n                           \
      \ bitset.resize(max_llm_token_id + 1, false);\n                            bitset\n\
      \                        }).set(llm_token_id.0, true);\n                   \
      \ }\n                    continue;\n                }\n\n                let\
      \ remaining_text = &text[position..];\n                let execute_result =\
      \ self.execute_from_state(remaining_text, maybe_state.unwrap_or(0));\n\n   \
      \             // Process all matches\n                for token in &execute_result.matches\
      \ {\n                    let new_position = position + token.width;\n      \
      \              assert_ne!(token.width, 0);\n                    assert!(new_position\
      \ <= text.len());\n                    let new_state: Option<usize> = None;\n\
      \                    let mut node_guard = node.try_lock().unwrap();\n      \
      \              if let Some(child) = node_guard.get(&token.id) {\n          \
      \              let child_ptr = Arc::as_ptr(&child);\n                      \
      \  if let Some(&(child_position, child_state)) = queue_positions.get(&child_ptr)\
      \ {\n                            if (child_position, child_state) != (new_position,\
      \ new_state) {\n                                // Child exists and is already\
      \ queued with different position or state\n                                //\
      \ Need to replace the child with a clone\n                                let\
      \ new_child = node_guard.replace_child_with_clone(&token.id);\n            \
      \                    queue_positions.insert(Arc::as_ptr(&new_child), (new_position,\
      \ new_state));\n                                queue.entry((new_position, new_state)).or_default().insert(Arc::as_ptr(&new_child),\
      \ new_child.clone());\n                            }\n                     \
      \   } else {\n                            // Child exists but is not already\
      \ queued\n                            // Need to add it to the queue\n     \
      \                       queue_positions.insert(child_ptr, (new_position, new_state));\n\
      \                            queue.entry((new_position, new_state)).or_default().insert(child_ptr,\
      \ child.clone());\n                        }\n                    } else {\n\
      \                        if let Some(new_node) = new_nodes_for_positions.get(&(new_position,\
      \ new_state)) {\n                            // A new node already exists for\
      \ this position and state\n                            // Add an edge from the\
      \ current node to the new node\n                            node_guard.insert(token.id,\
      \ new_node.clone());\n                        } else {\n                   \
      \         // A new node does not exist for this position and state\n       \
      \                     // Create a new node and add an edge from the current\
      \ node to the new node\n                            let new_node = Arc::new(Mutex::new(TrieNode::new((BTreeMap::new(),\
      \ BTreeMap::new(), None))));\n                            new_nodes_for_positions.insert((new_position,\
      \ new_state), new_node.clone());\n                            node_guard.insert(token.id,\
      \ new_node.clone());\n                            queue_positions.insert(Arc::as_ptr(&new_node),\
      \ (new_position, new_state));\n                            queue.entry((new_position,\
      \ new_state)).or_default().insert(Arc::as_ptr(&new_node), new_node.clone());\n\
      \                        }\n                    }\n                }\n     \
      \       }\n        }\n    }\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq,\
      \ PartialOrd, Ord)]\npub struct TokenizerStateInfoForLLMToken {\n    pub tokenizer_state_id:\
      \ usize,\n    pub position_in_llm_token: usize,\n    pub dirty_end_state: Option<StateID>,\n\
      \    // todo: do we even need this?\n    pub clean_end: bool,\n}\n\n/// Precomputes\
      \ a map from state -> token sequence -> LLM token -> state.\npub fn precompute<'a>(\n\
      \    tokenizer: &impl Tokenizer,\n    llm_token_map: &BiBTreeMap<Vec<u8>, LLMTokenID>,\n\
      \    eof_llm_token_id: LLMTokenID,\n    max_llm_token_id: usize,\n) -> BTreeMap<StateID,\
      \ TrieNode<TokenID, (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID,\
      \ BitVec>, Option<BitVec>)>> {\n    let mut result: BTreeMap<StateID, TrieNode<GroupID,\
      \ _>> = BTreeMap::new();\n\n    // Ensure the tokenizer doesn't match on empty\
      \ strings\n    debug!(2, \"Ensuring tokenizer doesn't match on empty strings\"\
      );\n    let execute_result = tokenizer.execute_from_state(&[], 0);\n    if !execute_result.matches.is_empty()\
      \ {\n        panic!(\"Tokenizer should not match on empty string. If it did,\
      \ there would be infinitely many possible token sequences for any LLM token.\"\
      );\n    }\n\n    debug!(2, \"Precomputing in precompute\");\n    for state_id\
      \ in tqdm!(0..tokenizer.max_state()) {\n        // debug!(1, \"Precomputing\
      \ state {}\", state_id);\n        let mut state_map_root_arc: Arc<Mutex<TrieNode<GroupID,\
      \ (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>>\
      \ = Arc::new(Mutex::new(TrieNode::new((BTreeMap::new(), BTreeMap::new(), None))));\n\
      \n        for (i, (llm_token, llm_token_id)) in llm_token_map.iter().enumerate()\
      \ {\n            debug!(1, \"Precomputing for token {:?} ({:?}) ({})\", llm_token_id,\
      \ llm_token, i);\n            // todo: REMOVE THIS\n            // if i < 121\
      \ {\n            //     continue;\n            // }\n            // dump_structure(state_map_root_arc.clone());\n\
      \            tokenizer.execute_all_from_state(\n                llm_token,\n\
      \                state_id,\n                state_map_root_arc.clone(),\n  \
      \              *llm_token_id,\n                max_llm_token_id,\n         \
      \   );\n        }\n\n        // debug!(2, \"Done precomputing state {}\", state_id);\n\
      \        let state_map_root = state_map_root_arc.try_lock().unwrap().clone();\n\
      \        result.insert(glr::table::StateID(state_id), state_map_root);\n   \
      \ }\n\n    result\n}\n\n/// Adds EOF token to the precomputed map\npub(crate)\
      \ fn precompute_add_eof(\n    precomputed: &mut BTreeMap<StateID, TrieNode<TokenID,\
      \ (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>,\n\
      \    eof_llm_token_id: LLMTokenID,\n    eof_grammar_token_id: TokenID,\n   \
      \ max_llm_token_id: usize,\n) {\n    let mut bitset = bitvec![0; max_llm_token_id\
      \ + 1];\n    bitset.set(eof_llm_token_id.0, true);\n    let node = precomputed.get_mut(&StateID(0)).expect(\"\
      State 0 should exist\");\n    node.value.1.insert(eof_grammar_token_id, bitset);\n\
      }\n\nimpl Tokenizer for Regex {\n    fn initial_state_id(&self) -> usize {\n\
      \        0\n    }\n\n    fn execute_from_state(&self, text: &[u8], state: usize)\
      \ -> ExecuteResult {\n        let mut regex_state = self.init_to_state(state);\n\
      \        regex_state.execute(text);\n\n        let matches: Vec<_> = regex_state.matches.iter().map(|(&id,\
      \ &width)| Token { id, width })\n            // Filter out zero-width tokens\n\
      \            .filter(|token| token.width != 0).collect();\n\n        ExecuteResult\
      \ {\n            matches,\n            new_state: if regex_state.done { None\
      \ } else { Some(regex_state.current_state) },\n        }\n    }\n\n    fn tokens_accessible_from_state(&self,\
      \ state: usize) -> Vec<TokenID> {\n        let regex_state = self.init_to_state(state);\n\
      \        regex_state.possible_group_ids().iter().cloned().collect()\n    }\n\
      \n    fn max_state(&self) -> usize {\n        self.dfa.states.len()\n    }\n\
      }\n\npub fn print_precomputed(precomputed: &BTreeMap<StateID, TrieNode<TokenID,\
      \ (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>)\
      \ {\n    println!(\"Precomputed:\");\n    for (tokenizer_state, root) in precomputed\
      \ {\n        println!(\"  Tokenizer state: {}\", tokenizer_state.0);\n     \
      \   for node in TrieNode::all_nodes(Arc::new(Mutex::new(root.clone()))) {\n\
      \            println!(\"    Node address: {:p}, value: {:?}\", Arc::as_ptr(&node),\
      \ node.try_lock().unwrap().value);\n            // print edge values and destination\
      \ addresses\n            for (edge, dest) in node.try_lock().unwrap().children()\
      \ {\n                println!(\"      Edge value: {:?}, destination address:\
      \ {:p}\", edge, Arc::as_ptr(&dest));\n            }\n        }\n    }\n}\n\n\
      #[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::charmap::TrieMap;\n\
      \    use crate::finite_automata::{eat_u8, DFAState, Regex, DFA};\n    use crate::u8set::U8Set;\n\
      \    use crate::{groups, seq};\n    use std::collections::{BTreeMap, BTreeSet};\n\
      \    use bimap::BiBTreeMap;\n\n    #[test]\n    fn test_precompute() {\n   \
      \     let _tokenizer = groups![\n            eat_u8(b'a'), // Token 0: 'a'\n\
      \            eat_u8(b'b'), // Token 1: 'b'\n            seq![eat_u8(b'a'), eat_u8(b'b')],\
      \ // Token 2: 'ab'\n            seq![eat_u8(b'a'), eat_u8(b'b'), eat_u8(b'c')],\
      \ // Token 3: 'abc'\n        ].build();\n\n        let tokenizer = Regex {\n\
      \            dfa: DFA {\n                states: vec![\n                   \
      \ DFAState {\n                        transitions: TrieMap::from_iter(vec![(b'a',\
      \ 1), (b'b', 2)]),\n                        finalizers: BTreeSet::new(),\n \
      \                       possible_group_ids: BTreeSet::from([0, 1, 2, 3]),\n\
      \                        group_id_to_u8set: BTreeMap::from([\n             \
      \               (0, U8Set::from_bytes(b\"a\")),\n                          \
      \  (1, U8Set::from_bytes(b\"b\")),\n                            (2, U8Set::from_bytes(b\"\
      a\")),\n                            (3, U8Set::from_bytes(b\"a\")),\n      \
      \                  ]),\n                    },\n                    DFAState\
      \ {\n                        transitions: TrieMap::from_iter(vec![(b'b', 3)]),\n\
      \                        finalizers: BTreeSet::from([0]),\n                \
      \        possible_group_ids: BTreeSet::from([0, 2, 3]),\n                  \
      \      group_id_to_u8set: BTreeMap::from([\n                            (2,\
      \ U8Set::from_bytes(b\"b\")),\n                            (3, U8Set::from_bytes(b\"\
      b\")),\n                        ]),\n                    },\n              \
      \      DFAState {\n                        transitions: TrieMap::new(),\n  \
      \                      finalizers: BTreeSet::from([1]),\n                  \
      \      possible_group_ids: BTreeSet::from([1]),\n                        group_id_to_u8set:\
      \ BTreeMap::new(),\n                    },\n                    DFAState {\n\
      \                        transitions: TrieMap::from_iter(vec![(b'c', 4)]),\n\
      \                        finalizers: BTreeSet::from([2]),\n                \
      \        possible_group_ids: BTreeSet::from([2, 3]),\n                     \
      \   group_id_to_u8set: BTreeMap::from([(3, U8Set::from_bytes(b\"c\"))]),\n \
      \                   },\n                    DFAState {\n                   \
      \     transitions: TrieMap::new(),\n                        finalizers: BTreeSet::from([3]),\n\
      \                        possible_group_ids: BTreeSet::from([3]),\n        \
      \                group_id_to_u8set: BTreeMap::new(),\n                    },\n\
      \                ],\n                start_state: 0,\n                non_greedy_finalizers:\
      \ BTreeSet::new(),\n            },\n        };\n        assert_eq!(_tokenizer,\
      \ tokenizer);\n\n        // Define the LLM tokens\n        let llm_tokens: &[&[u8]]\
      \ = &[b\"a\", b\"b\", b\"c\", b\"ab\", b\"bc\", b\"abc\"];\n        let llm_token_map:\
      \ BiBTreeMap<Vec<u8>, LLMTokenID> = llm_tokens.iter().enumerate().map(|(i, token)|\
      \ (token.to_vec(), LLMTokenID(i))).collect();\n\n        // Run precompute\n\
      \        let max_llm_token_id = llm_tokens.len() + 1;\n        let result =\
      \ precompute(&tokenizer, &llm_token_map, LLMTokenID(max_llm_token_id), max_llm_token_id);\n\
      \n        // todo: update this for TrieNode\n        // // Build the expected\
      \ output\n        // let mut state_0: BTreeMap<Vec<GroupID>, BTreeMap<&[u8],\
      \ StateID>> = BTreeMap::new();\n        // state_0.insert(vec![], BTreeMap::from([(b\"\
      a\".as_slice(), StateID(1)), (b\"ab\", StateID(3))]));\n        // state_0.insert(vec![0],\
      \ BTreeMap::from([(b\"a\".as_slice(), StateID(0))]));\n        // state_0.insert(vec![0,\
      \ 1], BTreeMap::from([(b\"ab\".as_slice(), StateID(0))]));\n        // state_0.insert(vec![1],\
      \ BTreeMap::from([(b\"b\".as_slice(), StateID(0))]));\n        // state_0.insert(vec![2],\
      \ BTreeMap::from([(b\"ab\".as_slice(), StateID(0))]));\n        // state_0.insert(vec![3],\
      \ BTreeMap::from([(b\"abc\".as_slice(), StateID(0))]));\n        // assert_eq!(Some(&state_0),\
      \ result.get(&StateID(0)));\n        //\n        // let mut state_1: BTreeMap<Vec<GroupID>,\
      \ BTreeMap<&[u8], StateID>> = BTreeMap::new();\n        // state_1.insert(vec![],\
      \ BTreeMap::from([(b\"b\".as_slice(), StateID(3))]));\n        // state_1.insert(vec![2],\
      \ BTreeMap::from([(b\"b\".as_slice(), StateID(0))]));\n        // state_1.insert(vec![3],\
      \ BTreeMap::from([(b\"bc\".as_slice(), StateID(0))]));\n        // assert_eq!(Some(&state_1),\
      \ result.get(&StateID(1)));\n        //\n        // assert_eq!(None, result.get(&StateID(2)));\n\
      \        //\n        // let mut state_3: BTreeMap<Vec<GroupID>, BTreeMap<&[u8],\
      \ StateID>> = BTreeMap::new();\n        // state_3.insert(vec![3], BTreeMap::from([(b\"\
      c\".as_slice(), StateID(0))]));\n        // assert_eq!(Some(&state_3), result.get(&StateID(3)));\n\
      \        //\n        // assert_eq!(None, result.get(&StateID(4)));\n       \
      \ //\n        // let mut expected: BTreeMap<StateID, BTreeMap<Vec<GroupID>,\
      \ BTreeMap<&[u8], StateID>>> = BTreeMap::new();\n        // expected.insert(StateID(0),\
      \ state_0);\n        // expected.insert(StateID(1), state_1);\n        // expected.insert(StateID(3),\
      \ state_3);\n        //\n        // assert_eq!(&expected, &result);\n    }\n\
      }\n\n```\n\n```rust\nuse std::collections::BTreeMap;\n\n/// Represents a generic\
      \ tree-like node structure for precomputation\n#[derive(Debug, Clone)]\npub(crate)\
      \ enum PrecomputeGSSNode<GrammarToken, Leaf> {\n    /// An internal node with\
      \ child nodes mapped by tokens\n    Internal(BTreeMap<GrammarToken, PrecomputeGSSNode<GrammarToken,\
      \ Leaf>>),\n    /// A leaf node containing a value\n    Leaf(Leaf),\n}\n\nimpl<GrammarToken,\
      \ Leaf> PrecomputeGSSNode<GrammarToken, Leaf>\nwhere\n    GrammarToken: Clone\
      \ + Ord,\n    Leaf: Clone,\n{\n    /// Flattens the tree structure into a map\
      \ of token sequences to leaf values\n    pub(crate) fn flatten(&self) -> BTreeMap<Vec<GrammarToken>,\
      \ Leaf> {\n        let mut result = BTreeMap::new();\n        self.flatten_recursive(&mut\
      \ result, Vec::new());\n        result\n    }\n\n    /// Recursive helper method\
      \ for flattening the tree\n    fn flatten_recursive(\n        &self,\n     \
      \   result: &mut BTreeMap<Vec<GrammarToken>, Leaf>,\n        path: Vec<GrammarToken>,\n\
      \    ) {\n        match self {\n            PrecomputeGSSNode::Internal(children)\
      \ => {\n                for (token, child) in children {\n                 \
      \   let mut new_path = path.clone();\n                    new_path.push(token.clone());\n\
      \                    child.flatten_recursive(result, new_path);\n          \
      \      }\n            }\n            PrecomputeGSSNode::Leaf(leaf) => {\n  \
      \              result.insert(path, leaf.clone());\n            }\n        }\n\
      \    }\n    \n    /// Maps leaf values using a provided transformation function\n\
      \    pub(crate) fn map<F, U>(&self, f: F) -> PrecomputeGSSNode<GrammarToken,\
      \ U>\n    where\n        F: Copy + Fn(&Leaf) -> U,\n    {\n        match self\
      \ {\n            PrecomputeGSSNode::Internal(children) => {\n              \
      \  let mapped_children = children\n                    .iter()\n           \
      \         .map(|(token, child)| (token.clone(), child.map(f)))\n           \
      \         .collect();\n                PrecomputeGSSNode::Internal(mapped_children)\n\
      \            }\n            PrecomputeGSSNode::Leaf(leaf) => {\n           \
      \     PrecomputeGSSNode::Leaf(f(leaf))\n            }\n        }\n    }\n}\n\
      \n```\n\n```rust\nuse super::items::{compute_closure, compute_goto, split_on_dot,\
      \ Item};\nuse crate::glr::grammar::{compute_first_sets, compute_follow_sets,\
      \ NonTerminal, Production, Symbol, Terminal};\nuse crate::glr::parser::GLRParser;\n\
      use bimap::BiBTreeMap;\nuse std::collections::{HashMap, VecDeque};\nuse std::collections::{BTreeMap,\
      \ BTreeSet};\nuse std::fmt::Display;\nuse crate::analyze_grammar::{drop_dead,\
      \ validate};\n\ntype Stage1Table = BTreeMap<BTreeSet<Item>, Stage1Row>;\ntype\
      \ Stage2Table = BTreeMap<BTreeSet<Item>, Stage2Row>;\ntype Stage3Table = BTreeMap<BTreeSet<Item>,\
      \ Stage3Row>;\ntype Stage4Table = BTreeMap<BTreeSet<Item>, Stage4Row>;\ntype\
      \ Stage5Table = BTreeMap<BTreeSet<Item>, Stage5Row>;\ntype Stage6Table = BTreeMap<BTreeSet<Item>,\
      \ Stage6Row>;\npub type Stage7Table = BTreeMap<StateID, Stage7Row>;\n\n\ntype\
      \ Stage1Row = BTreeMap<Option<Symbol>, BTreeSet<Item>>;\n#[derive(Debug)]\n\
      struct Stage2Row {\n    shifts: BTreeMap<Terminal, BTreeSet<Item>>,\n    gotos:\
      \ BTreeMap<NonTerminal, BTreeSet<Item>>,\n    reduces: BTreeSet<Item>,\n}\n\
      #[derive(Debug)]\nstruct Stage3Row {\n    shifts: BTreeMap<Terminal, BTreeSet<Item>>,\n\
      \    gotos: BTreeMap<NonTerminal, BTreeSet<Item>>,\n    /// Split the reduce\
      \ items by lookahead.\n    /// For LR(0), all possible terminals map to the\
      \ entire reduce item set.\n    /// But there are various cleverer and more selective\
      \ ways to compute lookaheads.\n    /// For simplicity, use LALR.\n    reduces:\
      \ BTreeMap<Terminal, BTreeSet<Item>>,\n}\n#[derive(Debug)]\nstruct Stage4Row\
      \ {\n    shifts: BTreeMap<Terminal, BTreeSet<Item>>,\n    gotos: BTreeMap<NonTerminal,\
      \ BTreeSet<Item>>,\n    reduces: BTreeMap<Terminal, BTreeSet<ProductionID>>,\n\
      }\ntype Stage5Row = Stage4Row;\n#[derive(Debug)]\nstruct Stage6Row {\n    shifts_and_reduces:\
      \ BTreeMap<Terminal, Stage6ShiftsAndReduces>,\n    gotos: BTreeMap<NonTerminal,\
      \ BTreeSet<Item>>,\n}\n\n#[derive(Debug)]\nenum Stage6ShiftsAndReduces {\n \
      \   Shift(BTreeSet<Item>),\n    Reduce(ProductionID),\n    Split {\n       \
      \ shift: Option<BTreeSet<Item>>,\n        reduces: BTreeSet<ProductionID>,\n\
      \    },\n}\n\n#[derive(Debug, Clone)]\npub enum Stage7ShiftsAndReduces {\n \
      \   /// Map each item set to a unique ID, and do the same for terminals and\
      \ nonterminals.\n    Shift(StateID),\n    Reduce { production_id: ProductionID,\
      \ nonterminal_id: NonTerminalID, len: usize },\n    Split {\n        shift:\
      \ Option<StateID>,\n        reduces: BTreeMap<usize, BTreeMap<NonTerminalID,\
      \ BTreeSet<ProductionID>>>,\n    },\n}\n\n#[derive(Debug, Clone)]\npub struct\
      \ Stage7Row {\n    /// Map each item set to a unique ID, and do the same for\
      \ terminals and nonterminals.\n    pub shifts_and_reduces: BTreeMap<TerminalID,\
      \ Stage7ShiftsAndReduces>,\n    pub gotos: BTreeMap<NonTerminalID, StateID>,\n\
      }\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub struct\
      \ StateID(pub usize);\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd,\
      \ Ord)]\npub struct ProductionID(pub usize);\n#[derive(Debug, Clone, Copy, PartialEq,\
      \ Eq, PartialOrd, Ord)]\npub struct NonTerminalID(pub usize);\n#[derive(Debug,\
      \ Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub struct TerminalID(pub usize);\n\
      \n\n\ntype Stage1Result = Stage1Table;\ntype Stage2Result = Stage2Table;\ntype\
      \ Stage3Result = Stage3Table;\ntype Stage4Result = Stage4Table;\ntype Stage5Result\
      \ = Stage5Table;\ntype Stage6Result = Stage6Table;\ntype Stage7Result = (\n\
      \    Stage7Table,\n    BiBTreeMap<BTreeSet<Item>, StateID>,\n    StateID,\n\
      \    TerminalID,\n);\n\nfn stage_1(productions: &[Production], start_production_id:\
      \ usize) -> Stage1Result {\n    let initial_item = Item {\n        production:\
      \ productions[start_production_id].clone(),\n        dot_position: 0,\n    };\n\
      \    let initial_closure = BTreeSet::from([initial_item]);\n    let mut worklist\
      \ = VecDeque::from([initial_closure.clone()]);\n\n    let mut transitions: BTreeMap<BTreeSet<Item>,\
      \ BTreeMap<Option<Symbol>, BTreeSet<Item>>> = BTreeMap::new();\n\n    while\
      \ let Some(items) = worklist.pop_front() {\n        if transitions.contains_key(&items)\
      \ {\n            continue;\n        }\n\n        let closure = compute_closure(&items,\
      \ productions);\n        let splits = split_on_dot(&closure);\n        let mut\
      \ row = BTreeMap::new();\n\n        for (symbol, items) in splits {\n      \
      \      if symbol.is_none() {\n                continue;\n            }\n   \
      \         let goto_set = compute_goto(&items);\n            row.insert(symbol.clone(),\
      \ goto_set.clone());\n            worklist.push_back(goto_set);\n        }\n\
      \n        transitions.insert(items.clone(), row);\n    }\n\n    transitions\n\
      }\n\nfn stage_2(stage_1_table: Stage1Table, productions: &[Production]) -> Stage2Result\
      \ {\n    let mut stage_2_table = BTreeMap::new();\n    for (item_set, transitions)\
      \ in stage_1_table {\n        let mut shifts = BTreeMap::new();\n        let\
      \ mut gotos = BTreeMap::new();\n        let mut reduces = BTreeSet::new();\n\
      \n        for item in &item_set {\n            if item.dot_position >= item.production.rhs.len()\
      \ {\n                // Reduce item\n                reduces.insert(item.clone());\n\
      \            }\n        }\n\n        for (symbol_opt, next_item_set) in &transitions\
      \ {\n            if let Some(symbol) = symbol_opt {\n                match symbol\
      \ {\n                    Symbol::Terminal(t) => {\n                        shifts.insert(t.clone(),\
      \ next_item_set.clone());\n                    }\n                    Symbol::NonTerminal(nt)\
      \ => {\n                        gotos.insert(nt.clone(), next_item_set.clone());\n\
      \                    }\n                }\n            }\n        }\n\n    \
      \    stage_2_table.insert(\n            item_set,\n            Stage2Row {\n\
      \                shifts,\n                gotos,\n                reduces,\n\
      \            },\n        );\n    }\n    stage_2_table\n}\n\n\nfn stage_3(stage_2_table:\
      \ Stage2Table, productions: &[Production]) -> Stage3Result {\n    let follow_sets\
      \ = compute_follow_sets(productions);\n\n    let mut stage_3_table = BTreeMap::new();\n\
      \n    for (item_set, row) in stage_2_table {\n        let mut reduces: BTreeMap<Terminal,\
      \ BTreeSet<Item>> = BTreeMap::new();\n\n        for item in &row.reduces {\n\
      \            let lhs = &item.production.lhs;\n            let lookaheads = follow_sets.get(lhs).unwrap();\n\
      \n            for terminal in lookaheads {\n                reduces\n      \
      \              .entry(terminal.clone())\n                    .or_default()\n\
      \                    .insert(item.clone());\n            }\n        }\n\n  \
      \      stage_3_table.insert(\n            item_set,\n            Stage3Row {\n\
      \                shifts: row.shifts,\n                gotos: row.gotos,\n  \
      \              reduces,\n            },\n        );\n    }\n\n    stage_3_table\n\
      }\n\nfn stage_4(stage_3_table: Stage3Table, productions: &[Production]) -> Stage4Result\
      \ {\n    let production_ids: BTreeMap<Production, ProductionID> = productions\n\
      \        .iter()\n        .enumerate()\n        .map(|(i, p)| (p.clone(), ProductionID(i)))\n\
      \        .collect();\n\n    let mut stage_4_table = BTreeMap::new();\n\n   \
      \ for (item_set, row) in stage_3_table {\n        let mut reduces = BTreeMap::new();\n\
      \n        for (terminal, items) in row.reduces {\n            let mut prod_ids\
      \ = BTreeSet::new();\n            for item in items {\n                let prod_id\
      \ = production_ids.get(&item.production).unwrap();\n                prod_ids.insert(*prod_id);\n\
      \            }\n            reduces.insert(terminal.clone(), prod_ids);\n  \
      \      }\n\n        stage_4_table.insert(\n            item_set,\n         \
      \   Stage4Row {\n                shifts: row.shifts,\n                gotos:\
      \ row.gotos,\n                reduces,\n            },\n        );\n    }\n\n\
      \    stage_4_table\n}\n\nfn stage_5(stage_4_table: Stage4Table, productions:\
      \ &[Production]) -> Stage5Result {\n    // todo: remove this\n    stage_4_table\n\
      }\n\nfn stage_6(stage_5_table: Stage5Table) -> Stage6Result {\n    let mut stage_6_table\
      \ = BTreeMap::new();\n\n    for (item_set, row) in stage_5_table {\n       \
      \ let mut shifts_and_reduces = BTreeMap::new();\n\n        for (terminal, next_item_set)\
      \ in row.shifts {\n            shifts_and_reduces.insert(terminal, Stage6ShiftsAndReduces::Shift(next_item_set));\n\
      \        }\n\n        for (terminal, mut production_ids) in row.reduces {\n\
      \            if let Some(mut existing) = shifts_and_reduces.remove(&terminal)\
      \ {\n                match existing {\n                    Stage6ShiftsAndReduces::Shift(shift_set)\
      \ => {\n                        shifts_and_reduces.insert(terminal, Stage6ShiftsAndReduces::Split\
      \ {\n                            shift: Some(shift_set.clone()),\n         \
      \                   reduces: production_ids.clone(),\n                     \
      \   });\n                    }\n                    Stage6ShiftsAndReduces::Reduce(existing_production_id)\
      \ => {\n                        production_ids.insert(existing_production_id);\n\
      \                        shifts_and_reduces.insert(terminal, Stage6ShiftsAndReduces::Split\
      \ {\n                            shift: None,\n                            reduces:\
      \ production_ids,\n                        });\n                    }\n    \
      \                Stage6ShiftsAndReduces::Split { shift, mut reduces } => {\n\
      \                        reduces.extend(production_ids.into_iter());\n     \
      \                   shifts_and_reduces.insert(terminal, Stage6ShiftsAndReduces::Split\
      \ { shift, reduces });\n                    }\n                }\n         \
      \   } else {\n                // If there's only one production ID, we can optimize\
      \ by storing it directly\n                if production_ids.len() == 1 {\n \
      \                   shifts_and_reduces.insert(terminal, Stage6ShiftsAndReduces::Reduce(production_ids.iter().next().unwrap().clone()));\n\
      \                } else {\n                    shifts_and_reduces.insert(terminal,\
      \ Stage6ShiftsAndReduces::Split { shift: None, reduces: production_ids });\n\
      \                }\n            }\n        }\n\n        stage_6_table.insert(\n\
      \            item_set,\n            Stage6Row {\n                shifts_and_reduces,\n\
      \                gotos: row.gotos,\n            },\n        );\n    }\n\n  \
      \  stage_6_table\n}\n\nfn stage_7(stage_6_table: Stage6Table, productions: &[Production],\
      \ start_production_id: usize, terminal_map: &BiBTreeMap<Terminal, TerminalID>,\
      \ non_terminal_map: &BiBTreeMap<NonTerminal, NonTerminalID>) -> Stage7Result\
      \ {\n    let mut item_set_map = BiBTreeMap::new();\n    let mut next_terminal_id\
      \ = 0;\n    let mut next_non_terminal_id = 0;\n    let mut next_state_id = 0;\n\
      \n    // Collect all terminals, non-terminals, and states\n    let mut terminals\
      \ = BTreeSet::new();\n    let mut non_terminals = BTreeSet::new();\n\n    for\
      \ (item_set, row) in &stage_6_table {\n        item_set_map.insert(item_set.clone(),\
      \ StateID(next_state_id));\n        next_state_id += 1;\n\n        for t in\
      \ row.shifts_and_reduces.keys() {\n            terminals.insert(t.clone());\n\
      \        }\n\n        for nt in row.gotos.keys() {\n            non_terminals.insert(nt.clone());\n\
      \        }\n    }\n\n    let mut stage_7_table = BTreeMap::new();\n\n    for\
      \ (item_set, row) in stage_6_table {\n        let state_id = *item_set_map.get_by_left(&item_set).unwrap();\n\
      \        let mut shifts_and_reduces = BTreeMap::new();\n        let mut gotos\
      \ = BTreeMap::new();\n\n        for (terminal, action) in row.shifts_and_reduces\
      \ {\n            let terminal_id = *terminal_map.get_by_left(&terminal).expect(format!(\"\
      {:?} not found in terminal map {:?}\", terminal, terminal_map.left_values().map(|t|\
      \ t.0.clone()).collect::<Vec<String>>()).as_str());\n            let converted_action\
      \ = match action {\n                Stage6ShiftsAndReduces::Shift(next_item_set)\
      \ => {\n                    let next_state_id = *item_set_map.get_by_left(&next_item_set).unwrap();\n\
      \                    Stage7ShiftsAndReduces::Shift(next_state_id)\n        \
      \        }\n                Stage6ShiftsAndReduces::Reduce(production_id) =>\
      \ {\n                    let production = productions.get(production_id.0).unwrap();\n\
      \                    let nonterminal_id = *non_terminal_map.get_by_left(&production.lhs).unwrap();\n\
      \                    let len = production.rhs.len();\n                    Stage7ShiftsAndReduces::Reduce\
      \ { production_id, nonterminal_id, len }\n                }\n              \
      \  Stage6ShiftsAndReduces::Split { shift, reduces } => {\n                 \
      \   let shift_state_id = shift.as_ref().map(|set| *item_set_map.get_by_left(set).unwrap());\n\
      \                    let mut len_to_nt_to_production_id: BTreeMap<usize, BTreeMap<NonTerminalID,\
      \ BTreeSet<ProductionID>>> = BTreeMap::new();\n                    for production_id\
      \ in reduces {\n                        let production = productions.get(production_id.0).unwrap();\n\
      \                        let nonterminal_id = *non_terminal_map.get_by_left(&production.lhs).unwrap();\n\
      \                        let len = production.rhs.len();\n                 \
      \       len_to_nt_to_production_id.entry(len).or_default().entry(nonterminal_id).or_default().insert(production_id);\n\
      \                    }\n                    Stage7ShiftsAndReduces::Split {\
      \ shift: shift_state_id, reduces: len_to_nt_to_production_id }\n           \
      \     }\n            };\n            shifts_and_reduces.insert(terminal_id,\
      \ converted_action);\n        }\n\n        for (nonterminal, next_item_set)\
      \ in row.gotos {\n            let non_terminal_id = *non_terminal_map.get_by_left(&nonterminal).unwrap();\n\
      \            let next_state_id = *item_set_map.get_by_left(&next_item_set).unwrap();\n\
      \            gotos.insert(non_terminal_id, next_state_id);\n        }\n\n  \
      \      stage_7_table.insert(state_id, Stage7Row { shifts_and_reduces, gotos\
      \ });\n    }\n\n    let start_item = Item {\n        production: productions[start_production_id].clone(),\n\
      \        dot_position: 0,\n    };\n    let start_state_id = *item_set_map.get_by_left(&BTreeSet::from([start_item])).unwrap();\n\
      \    let eof_terminal_id = *terminal_map.get_by_left(&Terminal(\"$\".to_string())).unwrap();\n\
      \n    (stage_7_table, item_set_map, start_state_id, eof_terminal_id)\n}\n\n\
      pub fn generate_glr_parser_with_maps(productions: &[Production], start_production_id:\
      \ usize, mut terminal_map: BiBTreeMap<Terminal, TerminalID>, non_terminal_map:\
      \ BiBTreeMap<NonTerminal, NonTerminalID>) -> GLRParser {\n    crate::debug!(2,\
      \ \"Validating\");\n    validate(productions).expect(\"Validation error\");\n\
      \n    // todo: this is messy\n    assign_eof_terminal_id(&mut terminal_map);\n\
      \n    crate::debug!(2, \"Stage 1\");\n    let stage_1_table = stage_1(productions,\
      \ start_production_id);\n    crate::debug!(2, \"Stage 2\");\n    let stage_2_table\
      \ = stage_2(stage_1_table, productions);\n    crate::debug!(2, \"Stage 3\");\n\
      \    let stage_3_table = stage_3(stage_2_table, productions);\n    crate::debug!(2,\
      \ \"Stage 4\");\n    let stage_4_table = stage_4(stage_3_table, productions);\n\
      \    crate::debug!(2, \"Stage 5\");\n    let stage_5_table = stage_5(stage_4_table,\
      \ productions);\n    crate::debug!(2, \"Stage 6\");\n    let stage_6_table =\
      \ stage_6(stage_5_table);\n    crate::debug!(2, \"Stage 7\");\n    let (stage_7_table,\
      \ item_set_map, start_state_id, eof_terminal_id) = stage_7(stage_6_table, productions,\
      \ start_production_id, &terminal_map, &non_terminal_map);\n    crate::debug!(2,\
      \ \"Stage 8\");\n\n    GLRParser::new(stage_7_table, productions.to_vec(), terminal_map,\
      \ non_terminal_map, item_set_map, start_state_id, eof_terminal_id)\n}\n\npub\
      \ fn generate_glr_parser(productions: &[Production], start_production_id: usize)\
      \ -> GLRParser {\n    let terminal_map = assign_terminal_ids(productions);\n\
      \    let non_terminal_map = assign_non_terminal_ids(productions);\n    generate_glr_parser_with_maps(productions,\
      \ start_production_id, terminal_map, non_terminal_map)\n}\n\npub fn assign_terminal_ids(productions:\
      \ &[Production]) -> BiBTreeMap<Terminal, TerminalID> {\n    let mut terminal_map\
      \ = BiBTreeMap::new();\n    let mut next_terminal_id = 0;\n\n    for p in productions\
      \ {\n        for symbol in &p.rhs {\n            if let Symbol::Terminal(t)\
      \ = symbol {\n                if !terminal_map.contains_left(t) {\n        \
      \            terminal_map.insert(t.clone(), TerminalID(next_terminal_id));\n\
      \                    next_terminal_id += 1;\n                }\n           \
      \ }\n        }\n    }\n\n    terminal_map\n}\n\npub fn assign_eof_terminal_id(terminal_map:\
      \ &mut BiBTreeMap<Terminal, TerminalID>) {\n    if !terminal_map.contains_left(&Terminal(\"\
      $\".to_string())) {\n        let max_terminal_id = terminal_map.right_values().max().unwrap().0;\n\
      \        terminal_map.insert(Terminal(\"$\".to_string()), TerminalID(max_terminal_id\
      \ + 1));\n    }\n}\n\npub fn assign_non_terminal_ids(productions: &[Production])\
      \ -> BiBTreeMap<NonTerminal, NonTerminalID> {\n    let mut non_terminal_map\
      \ = BiBTreeMap::new();\n    let mut next_non_terminal_id = 0;\n\n    for p in\
      \ productions {\n        if !non_terminal_map.contains_left(&p.lhs) {\n    \
      \        non_terminal_map.insert(p.lhs.clone(), NonTerminalID(next_non_terminal_id));\n\
      \            next_non_terminal_id += 1;\n        }\n    }\n    non_terminal_map\n\
      }\n\n```\n\n```rust\nuse crate::glr::grammar::{nt, prod, t, Terminal};\nuse\
      \ crate::glr::parser::GLRParser;\nuse crate::glr::table::{generate_glr_parser,\
      \ TerminalID};\n\n/// Creates a simple parser for testing basic parsing scenarios\n\
      fn create_simple_parser() -> GLRParser {\n    let productions = vec![\n    \
      \    // S -> A\n        prod(\"S\", vec![nt(\"A\")]),\n        // A -> A a |\
      \ b\n        prod(\"A\", vec![nt(\"A\"), t(\"a\")]),\n        prod(\"A\", vec![t(\"\
      b\")]),\n    ];\n\n    generate_glr_parser(&productions, 0)\n}\n\n/// Creates\
      \ a parser for parsing simple arithmetic expressions\nfn create_expression_parser()\
      \ -> GLRParser {\n    let productions = vec![\n        // S -> E\n        prod(\"\
      S\", vec![nt(\"E\")]),\n        // E -> E + T\n        prod(\"E\", vec![nt(\"\
      E\"), t(\"+\"), nt(\"T\")]),\n        // E -> T\n        prod(\"E\", vec![nt(\"\
      T\")]),\n        // T -> T * F\n        prod(\"T\", vec![nt(\"T\"), t(\"*\"\
      ), nt(\"F\")]),\n        // T -> F\n        prod(\"T\", vec![nt(\"F\")]),\n\
      \        // F -> ( E )\n        prod(\"F\", vec![t(\"(\"), nt(\"E\"), t(\")\"\
      )]),\n        // F -> i\n        prod(\"F\", vec![t(\"i\")]),\n    ];\n\n  \
      \  generate_glr_parser(&productions, 0)\n}\n\n/// Tokenizes an input string\
      \ into terminal IDs for a given parser\nfn tokenize(parser: &GLRParser, input:\
      \ &str) -> Vec<TerminalID> {\n    input.chars()\n        .filter_map(|c| parser.terminal_map.get_by_left(&Terminal(c.to_string())).copied())\n\
      \        .collect()\n}\n\n#[test]\nfn test_simple_parse_table() {\n    let parser\
      \ = create_simple_parser();\n    \n    let test_cases = [\n        (\"b\", true),\n\
      \        (\"ba\", true),\n        (\"baa\", true),\n        (\"a\", false),\n\
      \        (\"bb\", false),\n    ];\n\n    for (input, expected_match) in test_cases\
      \ {\n        let result = parser.parse(&tokenize(&parser, input));\n       \
      \ assert_eq!(\n            result.fully_matches(), \n            expected_match,\
      \ \n            \"Failed for input: {}\", input\n        );\n    }\n}\n\n#[test]\n\
      fn test_parse_simple_expression() {\n    let parser = create_expression_parser();\n\
      \    \n    let test_cases = [\n        (\"i\", true),\n        (\"i+i*i\", true),\n\
      \        (\"i+i\", true),\n        (\"i*i\", true),\n        (\"(i+i)*i\", true),\n\
      \        (\"i+\", false),\n        (\"i++i\", false),\n        (\"\", false),\n\
      \        (\")\", false),\n    ];\n\n    for (input, expected_match) in test_cases\
      \ {\n        let result = parser.parse(&tokenize(&parser, input));\n       \
      \ assert_eq!(\n            result.fully_matches(), \n            expected_match,\
      \ \n            \"Failed for input: {}\", input\n        );\n    }\n}\n\n```\n\
      \n```rust\nuse crate::finite_automata::{Expr, QuantifierType};\nuse crate::u8set::U8Set;\n\
      \n/// Creates a sequence of parsers\npub fn seq_fast(parsers: Vec<Expr>) ->\
      \ Expr {\n    Expr::Seq(parsers)\n}\n\n/// Creates a choice of parsers\npub\
      \ fn choice_fast(parsers: Vec<Expr>) -> Expr {\n    Expr::Choice(parsers)\n\
      }\n\n/// Makes a parser optional\npub fn opt_fast(parser: Expr) -> Expr {\n\
      \    Expr::Choice(vec![parser, Expr::Seq(vec![])])\n}\n\n/// Requires one or\
      \ more occurrences of a parser\npub fn repeat1_fast(parser: Expr) -> Expr {\n\
      \    Expr::Quantifier(Box::new(parser), QuantifierType::OneOrMore)\n}\n\n///\
      \ Matches a specific byte\npub fn eat_u8_fast(byte: u8) -> Expr {\n    Expr::U8Seq(vec![byte])\n\
      }\n\n/// Matches any byte except the specified one\npub fn eat_u8_negation_fast(byte:\
      \ u8) -> Expr {\n    Expr::U8Class(U8Set::from_byte(byte).complement())\n}\n\
      \n/// Matches any of the specified bytes\npub fn eat_u8_choice_fast(bytes: &[u8])\
      \ -> Expr {\n    Expr::U8Class(U8Set::from_bytes(bytes))\n}\n\n/// Matches any\
      \ byte not in the specified set\npub fn eat_u8_negation_choice_fast(bytes: &[u8])\
      \ -> Expr {\n    Expr::U8Class(U8Set::from_bytes(bytes).complement())\n}\n\n\
      /// Matches a byte within a specified range\npub fn eat_u8_range_fast(start:\
      \ u8, end: u8) -> Expr {\n    Expr::U8Class(U8Set::from_byte_range(start..=end))\n\
      }\n\n/// Matches a specific character\npub fn eat_char_fast(c: char) -> Expr\
      \ {\n    Expr::U8Seq(vec![c as u8])\n}\n\n/// Matches any character except the\
      \ specified one\npub fn eat_char_negation_fast(c: char) -> Expr {\n    Expr::U8Class(U8Set::from_char(c).complement())\n\
      }\n\n/// Matches any of the specified characters\npub fn eat_char_choice_fast(s:\
      \ &str) -> Expr {\n    Expr::U8Class(U8Set::from_chars(s))\n}\n\n/// Matches\
      \ any character not in the specified set\npub fn eat_char_negation_choice_fast(s:\
      \ &str) -> Expr {\n    Expr::U8Class(U8Set::from_chars(s).complement())\n}\n\
      \n/// Matches a specific string\npub fn eat_string_fast(s: &str) -> Expr {\n\
      \    Expr::U8Seq(s.bytes().collect())\n}\n\n/// Matches a byte within a specified\
      \ range\npub fn eat_byte_range_fast(start: u8, end: u8) -> Expr {\n    Expr::U8Class(U8Set::from_byte_range(start..=end))\n\
      }\n\n/// Creates a choice of byte strings\npub fn eat_bytestring_choice_fast(bytestrings:\
      \ Vec<Vec<u8>>) -> Expr {\n    let children: Vec<Expr> = bytestrings\n     \
      \   .into_iter()\n        .map(eat_bytestring_fast)\n        .collect();\n \
      \   choice_fast(children)\n}\n\n/// Matches a specific byte string\npub fn eat_bytestring_fast(bytes:\
      \ Vec<u8>) -> Expr {\n    Expr::U8Seq(bytes)\n}\n\n/// Creates a choice of strings\n\
      pub fn eat_string_choice_fast(strings: &[&str]) -> Expr {\n    choice_fast(strings.iter().map(|s|\
      \ eat_string_fast(s)).collect())\n}\n\n/// Allows zero or more occurrences of\
      \ a parser\npub fn repeat0_fast(parser: Expr) -> Expr {\n    opt_fast(repeat1_fast(parser))\n\
      }\n\n/// Matches a separator-delimited sequence of elements\npub fn seprep1_fast(a:\
      \ Expr, b: Expr) -> Expr {\n    seq_fast(vec![a.clone(), repeat0_fast(seq_fast(vec![b,\
      \ a]))])\n}\n\n/// Optionally matches a separator-delimited sequence of elements\n\
      pub fn seprep0_fast(a: Expr, b: Expr) -> Expr {\n    opt_fast(seprep1_fast(a,\
      \ b))\n}\n\n/// Matches exactly n occurrences of a parser\npub fn repeatn_fast(n:\
      \ usize, parser: Expr) -> Expr {\n    if n == 0 {\n        return seq_fast(vec![]);\n\
      \    }\n    let parsers = std::iter::repeat(parser).take(n).collect();\n   \
      \ seq_fast(parsers)\n}\n\n/// Macro for creating a sequence of parsers\n#[macro_export]\n\
      macro_rules! seq_fast {\n    ($($x:expr),* $(,)?) => {\n        $crate::tokenizer_combinators::seq_fast(vec![$($x),*])\n\
      \    };\n}\n\n/// Macro for creating a choice of parsers\n#[macro_export]\n\
      macro_rules! choice_fast {\n    ($($x:expr),* $(,)?) => {\n        $crate::tokenizer_combinators::choice_fast(vec![$($x),*])\n\
      \    };\n}\n\n```\n\n```rust\nuse std::collections::{HashMap, HashSet, VecDeque};\n\
      use std::collections::BTreeMap;\nuse std::fmt::Debug;\nuse std::rc::Rc;\nuse\
      \ std::sync::{Arc, Mutex};\nuse kdam::term::init;\n\n#[derive(Debug, Clone)]\n\
      pub struct TrieNode<E, T> {\n    pub value: T,\n    children: BTreeMap<E, Arc<Mutex<TrieNode<E,\
      \ T>>>>,\n    num_parents: usize,\n}\n\nimpl<T, E: Ord> TrieNode<E, T> {\n \
      \   pub fn new(value: T) -> TrieNode<E, T> {\n        TrieNode {\n         \
      \   value,\n            children: BTreeMap::new(),\n            num_parents:\
      \ 0,\n        }\n    }\n\n    pub fn insert(&mut self, edge: E, child: Arc<Mutex<TrieNode<E,\
      \ T>>>) -> Option<Arc<Mutex<TrieNode<E, T>>>> {\n        // crate::dbgprintln2!(\"\
      TrieNode::insert: begin\");\n        // Get the raw pointer to the current TrieNode\n\
      \        // assert!(!child.try_lock().unwrap().can_reach(self), \"TrieNode::insert:\
      \ cycle detected\");\n        child.try_lock().unwrap().num_parents += 1;\n\
      \        if let Some(existing_child) = self.children.insert(edge, child) {\n\
      \            // println!(\"warning: replacing existing node\");\n          \
      \  existing_child.try_lock().unwrap().num_parents -= 1;\n            // crate::dbgprintln2!(\"\
      TrieNode::insert: replacing existing node\");\n            Some(existing_child)\n\
      \        } else {\n            // crate::dbgprintln2!(\"TrieNode::insert: new\
      \ node\");\n            None\n        }\n    }\n\n    pub fn insert_with(&mut\
      \ self, edge: E, child: Arc<Mutex<TrieNode<E, T>>>, combine: impl FnOnce(&mut\
      \ T, T)) {\n        todo!()\n    }\n\n    pub fn get(&self, edge: &E) -> Option<Arc<Mutex<TrieNode<E,\
      \ T>>>> {\n        self.children.get(edge).cloned()\n    }\n\n    pub fn children(&self)\
      \ -> &BTreeMap<E, Arc<Mutex<TrieNode<E, T>>>> {\n        &self.children\n  \
      \  }\n\n    pub fn is_empty(&self) -> bool {\n        self.children.is_empty()\n\
      \    }\n\n    pub fn deep_clone(&self) -> Arc<Mutex<TrieNode<E, T>>> where T:\
      \ Clone, E: Clone {\n        let mut new_children = BTreeMap::new();\n     \
      \   for (edge, child) in &self.children {\n            let new_child = child.try_lock().unwrap().deep_clone();\n\
      \            new_children.insert(edge.clone(), new_child);\n        }\n    \
      \    Arc::new(Mutex::new(TrieNode {\n            value: self.value.clone(),\n\
      \            children: new_children,\n            num_parents: 0,\n        }))\n\
      \    }\n\n    pub fn shallow_clone(&self) -> Arc<Mutex<TrieNode<E, T>>> where\
      \ T: Clone, E: Clone {\n        let mut new_children = BTreeMap::new();\n  \
      \      for (edge, child) in self.children.clone() {\n            new_children.insert(edge,\
      \ child);\n        }\n        Arc::new(Mutex::new(TrieNode {\n            value:\
      \ self.value.clone(),\n            children: new_children,\n            num_parents:\
      \ 0,\n        }))\n    }\n\n    pub fn replace_child_with_clone(&mut self, edge:\
      \ &E) -> Arc<Mutex<TrieNode<E, T>>> where T: Clone, E: Clone {\n        let\
      \ child = self.children.get(edge).unwrap();\n        let new_child = child.try_lock().unwrap().shallow_clone();\n\
      \        self.insert(edge.clone(), new_child.clone());\n        new_child\n\
      \    }\n\n    pub fn all_nodes(root: Arc<Mutex<TrieNode<E, T>>>) -> Vec<Arc<Mutex<TrieNode<E,\
      \ T>>>> {\n        let mut node_ptrs_in_order: Vec<*const TrieNode<E, T>> =\
      \ Vec::new();\n        let mut nodes: BTreeMap<*const TrieNode<E, T>, Arc<Mutex<TrieNode<E,\
      \ T>>>> = BTreeMap::new();\n        let mut queue: VecDeque<Arc<Mutex<TrieNode<E,\
      \ T>>>> = VecDeque::new();\n        queue.push_back(root);\n        while let\
      \ Some(node) = queue.pop_front() {\n            if node_ptrs_in_order.contains(&(&*node.try_lock().unwrap()\
      \ as *const TrieNode<E, T>)) {\n                continue;\n            }\n \
      \           node_ptrs_in_order.push(&*node.try_lock().unwrap() as *const TrieNode<E,\
      \ T>);\n            nodes.insert(&*node.try_lock().unwrap() as *const TrieNode<E,\
      \ T>, node.clone());\n            let node = node.try_lock().unwrap();\n   \
      \         for (_, child) in &node.children {\n                queue.push_back(child.clone());\n\
      \            }\n        }\n        node_ptrs_in_order.into_iter().map(|ptr|\
      \ nodes.get(&ptr).unwrap().clone()).collect()\n    }\n\n    pub fn map_t<F,\
      \ U>(self, f: F) -> Arc<Mutex<TrieNode<E, U>>>\n    where\n        T: Clone,\n\
      \        E: Clone,\n        // todo: is it 'proper' to use `Copy` here?\n  \
      \      F: Copy + Fn(T) -> U,\n    {\n        let mut active_states: Vec<(Arc<Mutex<TrieNode<E,\
      \ T>>>, Arc<Mutex<TrieNode<E, U>>>)> = Vec::new();\n        let mut dormant_states:\
      \ HashMap<*const TrieNode<E, T>, (usize, Arc<Mutex<TrieNode<E, U>>>)> = HashMap::new();\n\
      \        let root = Arc::new(Mutex::new(TrieNode::new(f(self.value.clone()))));\n\
      \        active_states.push((Arc::new(Mutex::new(self)), root.clone()));\n\n\
      \        while let Some((node, new_node)) = active_states.pop() {\n        \
      \    let node = node.try_lock().unwrap();\n            for (edge, child_arc)\
      \ in &node.children {\n                let child = child_arc.try_lock().unwrap();\n\
      \                if let Some((num_parents_seen, new_child)) = dormant_states.get_mut(&(&*child\
      \ as *const TrieNode<E, T>)) {\n                    new_node.try_lock().unwrap().insert(edge.clone(),\
      \ new_child.clone());\n                    *num_parents_seen += 1;\n       \
      \             if *num_parents_seen == child.num_parents {\n                \
      \        active_states.push((child_arc.clone(), new_child.clone()));\n     \
      \               }\n                } else {\n                    let new_child\
      \ = Arc::new(Mutex::new(TrieNode::new(f(child.value.clone()))));\n         \
      \           new_node.try_lock().unwrap().insert(edge.clone(), new_child.clone());\n\
      \                    if child.num_parents == 1 {\n                        active_states.push((child_arc.clone(),\
      \ new_child.clone()));\n                    } else {\n                     \
      \   dormant_states.insert(&*child as *const TrieNode<E, T>, (1, new_child.clone()));\n\
      \                    }\n                }\n            }\n        }\n\n    \
      \    root\n    }\n\n    pub fn flatten<F>(&self, is_terminal: F) -> BTreeMap<Vec<E>,\
      \ T>\n    where\n        E: Clone,\n        T: Clone,\n        F: Copy + Fn(&T)\
      \ -> bool,\n    {\n        let mut result = BTreeMap::new();\n        self.flatten_recursive(&mut\
      \ result, Vec::new(), is_terminal);\n        result\n    }\n\n    fn flatten_recursive<F>(\n\
      \        &self,\n        result: &mut BTreeMap<Vec<E>, T>,\n        path: Vec<E>,\n\
      \        is_terminal: F,\n    )\n    where\n        E: Clone,\n        T: Clone,\n\
      \        F: Copy + Fn(&T) -> bool,\n    {\n        if is_terminal(&self.value)\
      \ {\n            result.insert(path.clone(), self.value.clone());\n        }\n\
      \        for (edge, child) in &self.children {\n            let mut new_path\
      \ = path.clone();\n            new_path.push(edge.clone());\n            child.try_lock().unwrap().flatten_recursive(result,\
      \ new_path, is_terminal);\n        }\n    }\n\n    fn can_reach(&self, target:\
      \ *const TrieNode<E, T>) -> bool {\n        let mut visited = HashSet::new();\n\
      \        let mut queue = VecDeque::new();\n\n        queue.push_back(self as\
      \ *const TrieNode<E, T>);\n        visited.insert(self as *const TrieNode<E,\
      \ T>);\n\n        while let Some(current_ptr) = queue.pop_front() {\n      \
      \      if current_ptr == target {\n                return true;\n          \
      \  }\n\n            // Safety: We need to ensure that the pointer is valid.\n\
      \            // In this context, it's assumed that the TrieNode instances\n\
      \            // are managed such that pointers to them remain valid as long\n\
      \            // as they are reachable from the root.\n            unsafe {\n\
      \                if let Some(current) = current_ptr.as_ref() {\n           \
      \         for child in current.children.values() {\n                       \
      \ let child_ptr = &*child.try_lock().unwrap() as *const TrieNode<E, T>;\n  \
      \                      if visited.insert(child_ptr) {\n                    \
      \        queue.push_back(child_ptr);\n                        }\n          \
      \          }\n                }\n            }\n        }\n\n        false\n\
      \    }\n}\n\nimpl<T: Clone, E: Ord + Clone> TrieNode<E, T> {\npub fn special_map<V>(\n\
      \        initial_node: Arc<Mutex<TrieNode<E, T>>>,\n        initial_value: V,\n\
      \        mut step: impl FnMut(&V, &E, &TrieNode<E, T>) -> V,\n        mut merge:\
      \ impl FnMut(Vec<V>) -> V,\n        mut process: impl FnMut(&T, &V),\n    )\
      \ where\n        V: Clone,\n        E: Ord,\n    {\n        // A queue of active\
      \ states (node and its associated value of type V)\n        let mut active_states:\
      \ VecDeque<(Arc<Mutex<TrieNode<E, T>>>, V)> = VecDeque::new();\n\n        //\
      \ A map of dormant states (node ID to a vector of values of type V)\n      \
      \  let mut dormant_states: HashMap<*const TrieNode<E, T>, Vec<V>> = HashMap::new();\n\
      \n        // Initialize the queue with the root node and the default initial\
      \ value\n        active_states.push_back((initial_node.clone(), initial_value));\n\
      \n        while let Some((node_arc, value)) = active_states.pop_front() {\n\
      \            let node = node_arc.try_lock().unwrap();\n\n            // Process\n\
      \            process(&node.value, &value);\n\n            // Traverse each child\
      \ of the current node\n            for (edge, child_arc) in &node.children {\n\
      \                let child = child_arc.try_lock().unwrap();\n\n            \
      \    // Apply the step function to compute the new value\n                let\
      \ new_value = step(&value, edge, &child);\n\n                // Get the raw\
      \ pointer to the child node for identification\n                let child_ptr\
      \ = &*child as *const TrieNode<E, T>;\n\n                // Update the dormant\
      \ state map\n                let entry = dormant_states.entry(child_ptr).or_default();\n\
      \                entry.push(new_value.clone());\n\n                // Check\
      \ if we've visited all parents of this child\n                if entry.len()\
      \ == child.num_parents {\n                    // Merge the values and push the\
      \ result to the active states queue\n                    let merged_value =\
      \ merge(entry.clone());\n                    dormant_states.remove(&child_ptr);\
      \ // Remove the entry from dormant states\n                    active_states.push_back((child_arc.clone(),\
      \ merged_value));\n                }\n            }\n        }\n\n        //\
      \ At the end, if there are any dormant states left, something went wrong\n \
      \       if !dormant_states.is_empty() {\n            // dump_structure(initial_node);\n\
      \            for (node_ptr, values) in &dormant_states {\n                println!(\"\
      dormant state: {:?}\", node_ptr)\n            }\n            panic!(\"Leftover\
      \ dormant states\");\n            // println!(\"Leftover dormant states\");\n\
      \        }\n    }\n\n    pub fn merge<T2>(\n        node: Arc<Mutex<TrieNode<E,\
      \ T>>>,\n        other: Arc<Mutex<TrieNode<E, T2>>>,\n        t_merge: impl\
      \ Fn(T, T2) -> T,\n        t_init: impl Fn() -> T,\n    )\n    where\n     \
      \   T2: Clone,\n    {\n        // println!(\"node structure (before)\");\n \
      \       // dump_structure(node.clone());\n        // println!(\"other structure\
      \ (before)\");\n        // dump_structure(other.clone());\n\n        // A map\
      \ to track the mapping of nodes from `other` to `self`\n        let mut node_map:\
      \ HashMap<*const TrieNode<E, T2>, Arc<Mutex<TrieNode<E, T>>>> = HashMap::new();\n\
      \n        let mut already_merged_values: HashSet<*const TrieNode<E, T>> = HashSet::new();\n\
      \n        // Special case: merge T for the root node\n        let existing_value\
      \ = node.try_lock().unwrap().value.clone();\n        let new_value = t_merge(existing_value,\
      \ other.try_lock().unwrap().value.clone());\n        node.try_lock().unwrap().value\
      \ = new_value;\n\n        TrieNode::special_map(\n            other.clone(),\n\
      \            vec![node.clone()],\n            // Step function\n           \
      \ |current_nodes: &Vec<Arc<Mutex<TrieNode<E, T>>>>, edge: &E, dest_other_node:\
      \ &TrieNode<E, T2>| {\n                let mut new_nodes = Vec::new();\n\n \
      \               for current_self_node in current_nodes {\n                 \
      \   let mut current_self_node_guard = current_self_node.try_lock().unwrap();\n\
      \n                    // Check if the current node has an equivalent edge\n\
      \                    if let Some(child) = current_self_node_guard.get(edge)\
      \ {\n                        if !already_merged_values.contains(&(&*child.try_lock().unwrap()\
      \ as *const TrieNode<E, T>)) {\n                            // Merge the values\n\
      \                            let child_value = child.try_lock().unwrap().value.clone();\n\
      \                            let merged_value = t_merge(child_value, dest_other_node.value.clone());\n\
      \                            child.try_lock().unwrap().value = merged_value;\n\
      \                        }\n                        new_nodes.push(child);\n\
      \                    } else {\n                        // Check if the `other`\
      \ node is already mapped\n                        let other_node_ptr = dest_other_node\
      \ as *const TrieNode<E, T2>;\n                        if let Some(mapped_node)\
      \ = node_map.get(&other_node_ptr) {\n                            // Add the\
      \ mapped node as a child\n                            current_self_node_guard.insert(edge.clone(),\
      \ mapped_node.clone());\n                            new_nodes.push(mapped_node.clone());\n\
      \                        } else {\n                            // Create a new\
      \ node and map it\n                            let new_node = Arc::new(Mutex::new(TrieNode::new(t_merge(t_init(),\
      \ dest_other_node.value.clone()))));\n                            current_self_node_guard.insert(edge.clone(),\
      \ new_node.clone());\n                            node_map.insert(other_node_ptr,\
      \ new_node.clone());\n                            new_nodes.push(new_node);\n\
      \                        }\n                    }\n                }\n\n   \
      \             new_nodes\n            },\n            // Merge function\n   \
      \         |values: Vec<Vec<Arc<Mutex<TrieNode<E, T>>>>>| {\n               \
      \ // Flatten the vectors and remove duplicates\n                let mut merged_nodes\
      \ = Vec::new();\n                let mut seen = HashSet::new();\n\n        \
      \        for value in values {\n                    for node in value {\n  \
      \                      let node_ptr = Arc::as_ptr(&node);\n                \
      \        if seen.insert(node_ptr) {\n                            merged_nodes.push(node);\n\
      \                        }\n                    }\n                }\n\n   \
      \             merged_nodes\n            },\n            // Process function\n\
      \            |_, _| {}\n        );\n    }\n}\n\n// pub trait TrieNodeTrait<E,\
      \ T> {\n//     fn insert(self, edge: E, value: T) -> Option<Arc<Mutex<TrieNode<E,\
      \ T>>>> {\n//         let new_node = TrieNode::new(value);\n//         self.insert(edge,\
      \ Arc::new(Mutex::new(new_node)));\n//     }\n// }\n\npub(crate) fn dump_structure<E,\
      \ T>(root: Arc<Mutex<TrieNode<E, T>>>) where E: Debug, T: Debug {\n    // TODO:\
      \ modify this to use letter names \"a\" \"b\" \"c\"... for nodes rather than\
      \ raw pointers.\n    // TODO: make it possible to print edge value and node's\
      \ internal value if they implement Debug\n    let mut queue: VecDeque<Arc<Mutex<TrieNode<E,\
      \ T>>>> = VecDeque::new();\n    let mut seen: HashSet<*const TrieNode<E, T>>\
      \ = HashSet::new();\n\n    queue.push_back(root);\n\n    while let Some(node)\
      \ = queue.pop_front() {\n        let node = node.try_lock().unwrap();\n    \
      \    let node_ptr = &*node as *const TrieNode<E, T>;\n        println!(\"{:?}:\
      \ num_parents: {}\", node_ptr, node.num_parents);\n        for (edge, child)\
      \ in &node.children {\n            let child_ptr = &*child.try_lock().unwrap()\
      \ as *const TrieNode<E, T>;\n            println!(\"  - {:?} -> {:?}\", edge,\
      \ child_ptr);\n            if !seen.contains(&child_ptr) {\n               \
      \ seen.insert(child_ptr);\n                queue.push_back(child.clone());\n\
      \            }\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use std::sync::{Arc,\
      \ Mutex};\n    use crate::trie::{dump_structure, TrieNode};\n\n    #[test]\n\
      \    fn test_trie() {\n        let mut a = TrieNode::new(\"a\");\n        let\
      \ mut b = TrieNode::new(\"b\");\n        let c = TrieNode::new(\"c\");\n\n \
      \       b.insert(\"b->c\", Arc::new(Mutex::new(c)));\n        a.insert(\"a->b\"\
      , Arc::new(Mutex::new(b)));\n\n        let mut a2 = TrieNode::new(\"a\");\n\
      \        let mut b2 = TrieNode::new(\"b\");\n        let d = TrieNode::new(\"\
      d\");\n\n        b2.insert(\"b->d\", Arc::new(Mutex::new(d)));\n        a2.insert(\"\
      a->b\", Arc::new(Mutex::new(b2)));\n\n        let a = Arc::new(Mutex::new(a));\n\
      \        let a2 = Arc::new(Mutex::new(a2));\n\n        println!(\"a structure\
      \ (before)\");\n        dump_structure(a.clone());\n        println!(\"a2 structure\
      \ (before)\");\n        dump_structure(a2.clone());\n\n        let merged =\
      \ TrieNode::merge(\n            a.clone(),\n            a2.clone(),\n      \
      \      |x, y| { if x.is_empty() { y } else { assert_eq!(x, y); x } },\n    \
      \        || { \"\" }\n        );\n\n        println!(\"a structure (after)\"\
      );\n        dump_structure(a.clone());\n        println!(\"a2 structure (after)e\"\
      );\n        dump_structure(a2.clone());\n    }\n}\n```\n\n```rust\n\n\n#[macro_export]\n\
      macro_rules! debug {\n    ($level:expr, $($arg:tt)*) => {{\n        pub const\
      \ DEBUG_LEVEL: usize = 1;\n        if $level <= DEBUG_LEVEL {\n            #[cfg(feature\
      \ = \"debug\")]\n            println!(\"[DEBUG {}] {}\", $level, format!($($arg)*));\n\
      \        }\n    }};\n}\n\n```"
  role: tool
- user: I want to clean up this code. Make it more concise and readable where possible.
    There's a lot of junk in it. Retain todos as-is as comments; don't try and do
    them unless they're easy.
  metadata:
    cache_control:
      type: ephemeral
- assistant: "Okay, I will refactor the code to improve its conciseness and readability.\
    \ I'll focus on simplifying logic, removing unnecessary verbosity, and using more\
    \ idiomatic Rust where appropriate. I will also retain the todos as comments.\n\
    \nHere's the cleaned-up code:\n\n```rust id=\"d7817759\"\nuse std::collections::{BTreeMap,\
    \ BTreeSet};\nuse crate::glr::grammar::{NonTerminal, Production, Symbol};\n\n\
    pub fn validate(productions: &[Production]) -> Result<(), String> {\n    let lhs_nonterms:\
    \ BTreeSet<NonTerminal> = productions.iter().map(|prod| prod.lhs.clone()).collect();\n\
    \    let rhs_nonterms: BTreeSet<NonTerminal> = productions.iter().flat_map(|prod|\
    \ \n        prod.rhs.iter().filter_map(|symbol| \n            if let Symbol::NonTerminal(nt)\
    \ = symbol { Some(nt.clone()) } else { None }\n        )\n    ).collect();\n\n\
    \    let missing_nonterms: BTreeSet<_> = rhs_nonterms.difference(&lhs_nonterms).collect();\n\
    \    if !missing_nonterms.is_empty() {\n        let missing_nonterm_strings: BTreeSet<_>\
    \ = missing_nonterms.into_iter().map(|nt| nt.0.clone()).collect();\n        return\
    \ Err(format!(\"Nonterminals missing a production: {:?}. All RHS nonterminals:\
    \ {:?}. All LHS nonterminals: {:?}\", missing_nonterm_strings, rhs_nonterms, lhs_nonterms));\n\
    \    }\n\n    Ok(())\n}\n\npub fn drop_dead(productions: &[Production]) -> Vec<Production>\
    \ {\n    // todo: this function is broken\n    let mut nt_reachables: BTreeMap<&NonTerminal,\
    \ BTreeSet<&NonTerminal>> = productions.iter()\n        .map(|prod| {\n      \
    \      let rhs_nonterms = prod.rhs.iter()\n                .filter_map(|symbol|\
    \ if let Symbol::NonTerminal(nt) = symbol { Some(nt) } else { None })\n      \
    \          .collect();\n            (&prod.lhs, rhs_nonterms)\n        })\n  \
    \      .collect();\n\n    loop {\n        let mut changed = false;\n        for\
    \ (nt, reachables) in nt_reachables.clone() {\n            let old_len = nt_reachables[nt].len();\n\
    \            for reachable in reachables {\n                if let Some(reachable_reachables)\
    \ = nt_reachables.get(reachable).cloned() {\n                    nt_reachables.get_mut(nt).unwrap().extend(reachable_reachables);\n\
    \                }\n            }\n            if nt_reachables[nt].len() != old_len\
    \ {\n                changed = true;\n            }\n        }\n        if !changed\
    \ {\n            break;\n        }\n    }\n\n    let start_prod = &productions[0];\n\
    \    let mut reachable_from_start = start_prod.rhs.iter()\n        .filter_map(|symbol|\
    \ if let Symbol::NonTerminal(nt) = symbol { \n            let mut reachable =\
    \ BTreeSet::from([nt]);\n             if let Some(nt_reachables) = nt_reachables.get(nt).cloned()\
    \ {\n                reachable.extend(nt_reachables);\n            }\n       \
    \     Some(reachable)\n        } else { None })\n        .flatten()\n        .collect();\n\
    \    \n\n    let new_productions: Vec<_> = std::iter::once(start_prod.clone()).chain(\n\
    \        productions.iter().skip(1)\n             .filter(|prod| reachable_from_start.contains(&prod.lhs))\n\
    \             .cloned()\n    ).collect();\n\n    crate::debug!(2, \"Dropped {}\
    \ productions\", productions.len() - new_productions.len());\n\n    new_productions\n\
    }\n```\n\n```rust id=\"83d03a2f\"\nuse crate::glr::parser::{GLRParser, GLRParserState,\
    \ InsertWith, ParseState, ParseStateKey};\nuse crate::glr::table::{StateID, TerminalID};\n\
    use crate::{precompute, debug};\nuse crate::precompute::{LLMTokenID, TokenID,\
    \ Tokenizer};\nuse bitvec::prelude::*;\nuse bimap::BiBTreeMap;\nuse std::collections::{BTreeMap,\
    \ BTreeSet};\nuse std::sync::{Arc, Mutex};\nuse crate::trie::TrieNode;\n\ntype\
    \ LLMToken = Vec<u8>;\ntype LLMTokenMap = BiBTreeMap<Vec<u8>, LLMTokenID>;\n\n\
    #[derive(Debug, Clone)]\npub struct GrammarConstraint<T: Tokenizer> {\n    pub(crate)\
    \ tokenizer: T,\n    pub(crate) parser: GLRParser,\n    pub precomputed: BTreeMap<StateID,\
    \ TrieNode<TokenID, (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID,\
    \ BitVec>, Option<BitVec>)>>,\n    pub(crate) max_llm_token_id: usize,\n}\n\n\
    #[derive(Debug, Clone)]\npub struct GrammarConstraintState<T: Tokenizer> {\n \
    \   parent: GrammarConstraint<T>,\n    states: Vec<(ParseState, BTreeSet<StateID>)>,\n\
    }\n\nimpl<T: Tokenizer> GrammarConstraint<T> {\n    pub fn new(\n        tokenizer:\
    \ T,\n        parser: GLRParser,\n        llm_tokens: LLMTokenMap,\n        eof_llm_token_id:\
    \ usize,\n        max_llm_token_id: usize,\n    ) -> Self {\n        let mut precomputed\
    \ = precompute::precompute(&tokenizer, &llm_tokens, LLMTokenID(eof_llm_token_id),\
    \ max_llm_token_id);\n        precompute::precompute_add_eof(&mut precomputed,\
    \ LLMTokenID(eof_llm_token_id), parser.eof_terminal_id.0, max_llm_token_id);\n\
    \n        Self {\n            tokenizer,\n            parser,\n            precomputed,\n\
    \            max_llm_token_id,\n        }\n    }\n\n    pub fn init(self) -> GrammarConstraintState<T>\
    \ {\n        let parser_initial_state = self.parser.init_parse_state();\n    \
    \    let tokenizer_initial_state_id = StateID(self.tokenizer.initial_state_id());\n\
    \n        GrammarConstraintState {\n            parent: self,\n            states:\
    \ vec![(parser_initial_state, BTreeSet::from([tokenizer_initial_state_id]))],\n\
    \        }\n    }\n}\n\nimpl<'a, T: Tokenizer> GrammarConstraintState<T> {\n \
    \   pub fn get_mask(&self) -> BitVec {\n        let mut result = bitvec![0; self.parent.max_llm_token_id\
    \ + 1];\n\n        for (parse_state, tokenizer_state_ids) in &self.states {\n\
    \            for tokenizer_state in tokenizer_state_ids {\n                let\
    \ token_sequence_map = &self.parent.precomputed[tokenizer_state];\n          \
    \       TrieNode::special_map(\n                    Arc::new(Mutex::new(token_sequence_map.clone())),\n\
    \                    vec![parse_state.clone()],\n                    |current_parse_states,\
    \ token_id, _| {\n                        let mut glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(current_parse_states.clone());\n\
    \                        glr_parse_state.step(TerminalID(*token_id));\n      \
    \                  glr_parse_state.active_states\n                    },\n   \
    \                 |parse_states: Vec<Vec<ParseState>>| {\n                   \
    \     let all_parse_states: Vec<ParseState> = parse_states.into_iter().flatten().collect();\n\
    \                        let mut new_glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(all_parse_states);\n\
    \                        new_glr_parse_state.merge_active_states();\n        \
    \                new_glr_parse_state.active_states\n                    },\n \
    \                   |(_, bitsets, maybe_clean_end_bitset), current_parse_states|\
    \ {\n                        let mut glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(current_parse_states.clone());\n\
    \                        if glr_parse_state.is_ok() {\n                      \
    \      for (possible_next_grammar_token, bitset) in bitsets {\n              \
    \                  let mut new_glr_parse_state = glr_parse_state.clone();\n  \
    \                              let possible_next_grammar_token_id = TerminalID(*possible_next_grammar_token);\n\
    \                                new_glr_parse_state.step(possible_next_grammar_token_id);\n\
    \n                                if new_glr_parse_state.is_ok() {\n         \
    \                           result |= bitset;\n                              \
    \  }\n                            }\n                            if let Some(bitset)\
    \ = maybe_clean_end_bitset {\n                                result |= bitset;\n\
    \                            }\n                        }\n                  \
    \  },\n                );\n            }\n        }\n        result\n    }\n\n\
    \    pub fn commit(&mut self, llm_token_id: LLMTokenID) {\n        let mut new_states:\
    \ BTreeMap<(ParseStateKey, BTreeSet<StateID>), ParseState> = BTreeMap::new();\n\
    \        \n         for (parse_state, tokenizer_state_ids) in &self.states {\n\
    \            for tokenizer_state_id in tokenizer_state_ids {\n               \
    \ TrieNode::special_map(\n                    Arc::new(Mutex::new(self.parent.precomputed[tokenizer_state_id].clone())),\n\
    \                    vec![parse_state.clone()],\n                    |current_parse_states,\
    \ token_id, _| {\n                         let mut glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(current_parse_states.clone());\n\
    \                        glr_parse_state.step(TerminalID(*token_id));\n      \
    \                  glr_parse_state.active_states\n                    },\n   \
    \                 |parse_states: Vec<Vec<ParseState>>| {\n                   \
    \     let all_parse_states: Vec<ParseState> = parse_states.into_iter().flatten().collect();\n\
    \                        let mut new_glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(all_parse_states);\n\
    \                        new_glr_parse_state.merge_active_states();\n        \
    \                new_glr_parse_state.active_states\n                    },\n \
    \                   |(llm_token_id_to_state_id, _, _), current_parse_states| {\n\
    \                        let mut new_glr_parse_state = self.parent.parser.init_glr_parser_from_parse_states(current_parse_states.clone());\n\
    \                        if let Some(info) = llm_token_id_to_state_id.get(&llm_token_id)\
    \ {\n                            for active_parse_state in new_glr_parse_state.active_states\
    \ {\n                                new_states.insert_with(\n               \
    \                     (active_parse_state.key(), BTreeSet::from([info.unwrap_or(StateID(0))])),\n\
    \                                    active_parse_state,\n                   \
    \                 |old, new| {\n                                        old.merge(new);\n\
    \                                    },\n                                );\n\
    \                            }\n                        }\n                  \
    \  },\n                )\n            }\n        }\n        \n        self.states\
    \ = new_states.into_iter().map(|((_key, tokenizer_state_ids), parse_state)| {\n\
    \            (parse_state, tokenizer_state_ids)\n        }).collect();\n    }\n\
    \n    pub fn commit_many(&mut self, llm_token_ids: &[LLMTokenID]) {\n        for\
    \ &llm_token_id in llm_token_ids {\n            self.commit(llm_token_id);\n \
    \       }\n    }\n\n    pub fn get_precomputed(&self) -> &BTreeMap<StateID, TrieNode<TokenID,\
    \ (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>\
    \ {\n        &self.parent.precomputed\n    }\n}\n```\n\n```rust id=\"eb63087d\"\
    \nuse std::collections::{BTreeMap, BTreeSet};\n\n#[derive(Debug, Clone, PartialEq,\
    \ Eq, PartialOrd, Ord)]\npub struct NonTerminal(pub String);\n\n#[derive(Debug,\
    \ Clone, PartialEq, Eq, PartialOrd, Ord)]\npub struct Terminal(pub String);\n\n\
    #[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]\npub enum Symbol {\n \
    \   Terminal(Terminal),\n    NonTerminal(NonTerminal),\n}\n\n#[derive(Debug, Clone,\
    \ PartialEq, Eq, PartialOrd, Ord)]\npub struct Production {\n    pub lhs: NonTerminal,\n\
    \    pub rhs: Vec<Symbol>,\n}\n\npub fn nt(name: &str) -> Symbol {\n    Symbol::NonTerminal(NonTerminal(name.to_string()))\n\
    }\n\npub fn t(name: &str) -> Symbol {\n    Symbol::Terminal(Terminal(name.to_string()))\n\
    }\n\npub fn prod(name: &str, rhs: Vec<Symbol>) -> Production {\n    Production\
    \ {\n        lhs: NonTerminal(name.to_string()),\n        rhs,\n    }\n}\n\npub\
    \ fn compute_epsilon_nonterminals(productions: &[Production]) -> BTreeSet<NonTerminal>\
    \ {\n    let mut epsilon_nonterminals = BTreeSet::new();\n    let mut changed\
    \ = true;\n\n    while changed {\n        changed = false;\n        for production\
    \ in productions {\n            if production.rhs.is_empty() && !epsilon_nonterminals.contains(&production.lhs)\
    \ {\n                epsilon_nonterminals.insert(production.lhs.clone());\n  \
    \              changed = true;\n            } else if production.rhs.iter().all(|symbol|\
    \ {\n                matches!(symbol, Symbol::NonTerminal(nt) if epsilon_nonterminals.contains(nt))\n\
    \            }) && !epsilon_nonterminals.contains(&production.lhs)\n         \
    \   {\n                epsilon_nonterminals.insert(production.lhs.clone());\n\
    \                changed = true;\n            }\n        }\n    }\n\n    epsilon_nonterminals\n\
    }\n\npub fn compute_first_sets(productions: &[Production]) -> BTreeMap<NonTerminal,\
    \ BTreeSet<Terminal>> {\n    let epsilon_nonterminals = compute_epsilon_nonterminals(productions);\n\
    \    let mut first_sets: BTreeMap<NonTerminal, BTreeSet<Terminal>> = BTreeMap::new();\n\
    \n    for production in productions {\n        let lhs = &production.lhs;\n  \
    \      first_sets.entry(lhs.clone()).or_default();\n        \n        for symbol\
    \ in &production.rhs {\n            match symbol {\n                Symbol::Terminal(t)\
    \ => {\n                    first_sets.get_mut(lhs).unwrap().insert(t.clone());\n\
    \                    break;\n                }\n                Symbol::NonTerminal(nt)\
    \ => {\n                    if !epsilon_nonterminals.contains(nt) {\n        \
    \                break;\n                    }\n                }\n          \
    \  }\n        }\n    }\n\n    let mut changed = true;\n    while changed {\n \
    \       changed = false;\n\n        for production in productions {\n        \
    \    let lhs = &production.lhs;\n            let rhs = &production.rhs;\n\n  \
    \          let old_size = first_sets.get_mut(lhs).unwrap().len();\n\n        \
    \    for symbol in rhs {\n                if let Symbol::NonTerminal(nt) = symbol\
    \ {\n                    let first_nt = first_sets[nt].clone();\n            \
    \        first_sets.get_mut(lhs).unwrap().extend(first_nt);\n\n              \
    \      if !epsilon_nonterminals.contains(nt) {\n                        break;\n\
    \                    }\n                }\n            }\n\n            if first_sets.get_mut(lhs).unwrap().len()\
    \ != old_size {\n                changed = true;\n            }\n        }\n \
    \   }\n\n    first_sets\n}\n\npub fn compute_follow_sets(productions: &[Production])\
    \ -> BTreeMap<NonTerminal, BTreeSet<Terminal>> {\n    let first_sets = compute_first_sets(productions);\n\
    \    let epsilon_nonterminals = compute_epsilon_nonterminals(productions);\n \
    \   let mut follow_sets: BTreeMap<NonTerminal, BTreeSet<Terminal>> = BTreeMap::new();\n\
    \n    for production in productions {\n        follow_sets.entry(production.lhs.clone()).or_default();\n\
    \    }\n\n    if let Some(start_symbol) = productions.first() {\n        follow_sets\n\
    \            .get_mut(&start_symbol.lhs)\n            .unwrap()\n            .insert(Terminal(\"\
    $\".to_string()));\n    }\n\n    let mut changed = true;\n    while changed {\n\
    \        changed = false;\n\n        for production in productions {\n       \
    \     let lhs = &production.lhs;\n            let rhs = &production.rhs;\n\n \
    \           for (i, symbol) in rhs.iter().enumerate() {\n                if let\
    \ Symbol::NonTerminal(nt) = symbol {\n                    let old_size = follow_sets.get_mut(nt).unwrap().len();\n\
    \n                    let mut nullable = true;\n                    for next_symbol\
    \ in &rhs[i + 1..] {\n                        match next_symbol {\n          \
    \                  Symbol::Terminal(t_next) => {\n                           \
    \     follow_sets.get_mut(nt).unwrap().insert(t_next.clone());\n             \
    \                   nullable = false;\n                                break;\n\
    \                            }\n                            Symbol::NonTerminal(nt_next)\
    \ => {\n                                let first_next = &first_sets[nt_next];\n\
    \                                follow_sets.get_mut(nt).unwrap().extend(first_next.iter().cloned());\n\
    \                                \n                                if !epsilon_nonterminals.contains(nt_next)\
    \ {\n                                    nullable = false;\n                 \
    \                   break;\n                                }\n              \
    \              }\n                        }\n                    }\n\n       \
    \             if nullable {\n                        let follow_lhs = follow_sets.get(lhs).unwrap().clone();\n\
    \                        follow_sets.get_mut(nt).unwrap().extend(follow_lhs);\n\
    \                    }\n\n                    if follow_sets.get_mut(nt).unwrap().len()\
    \ != old_size {\n                        changed = true;\n                   \
    \ }\n                }\n            }\n        }\n    }\n\n    follow_sets\n}\n\
    ```\n\n```rust id=\"343707c5\"\nuse std::sync::Arc;\nuse std::collections::{BTreeMap,\
    \ HashMap};\nuse crate::gss::BulkMerge;\n\n\n#[derive(Debug, Clone, PartialEq,\
    \ Eq, PartialOrd, Ord)]\npub struct GSSNode<T> {\n    value: T,\n    predecessors:\
    \ Vec<Arc<GSSNode<T>>>,\n}\n\nimpl<T> GSSNode<T> {\n    pub fn new(value: T) ->\
    \ Self {\n        Self {\n            value,\n            predecessors: Vec::new(),\n\
    \        }\n    }\n\n    pub fn from_iter<I>(iter: I) -> Self\n    where\n   \
    \     I: IntoIterator<Item = T>,\n    {\n        let mut iter = iter.into_iter();\n\
    \        let mut root = Self::new(iter.next().unwrap());\n        for value in\
    \ iter {\n            root = root.push(value);\n        }\n        root\n    }\n\
    \n    pub fn push(self, value: T) -> Self {\n        let mut new_node = Self::new(value);\n\
    \        new_node.predecessors.push(Arc::new(self));\n        new_node\n    }\n\
    \n    pub fn pop(&self) -> Vec<Arc<Self>> {\n        self.predecessors.clone()\n\
    \    }\n\n    pub fn popn(&self, n: usize) -> Vec<Arc<Self>>\n    where\n    \
    \    T: Clone,\n    {\n        if n == 0 {\n            return vec![Arc::new(self.clone())];\n\
    \        }\n        let mut nodes = Vec::new();\n        for popped in self.pop()\
    \ {\n            nodes.extend(popped.popn(n - 1));\n        }\n        nodes\n\
    \    }\n\n    pub fn peek(&self) -> &T {\n        &self.value\n    }\n\n    pub\
    \ fn flatten(&self) -> Vec<Vec<T>>\n    where\n        T: Clone,\n    {\n    \
    \    let mut result = Vec::new();\n        let mut stack = Vec::new();\n     \
    \   stack.push((self, Vec::new()));\n        while let Some((node, mut path))\
    \ = stack.pop() {\n            path.push(node.value.clone());\n            if\
    \ node.predecessors.is_empty() {\n                result.push(path);\n       \
    \     } else {\n                for predecessor in &node.predecessors {\n    \
    \                stack.push((predecessor, path.clone()));\n                }\n\
    \            }\n        }\n        result\n    }\n\n    pub fn flatten_bulk(nodes:\
    \ &[Self]) -> Vec<Vec<T>>\n    where\n        T: Clone,\n    {\n        nodes.iter().flat_map(|node|\
    \ node.flatten()).collect()\n    }\n\n    pub fn merge(&mut self, mut other: Self)\n\
    \    where\n        T: PartialEq,\n    {\n        assert!(self.value == other.value);\n\
    \        self.predecessors.append(&mut other.predecessors);\n    }\n\n    pub\
    \ fn map<F, U>(&self, f: F) -> GSSNode<U>\n    where\n        F: Copy + Fn(&T)\
    \ -> U,\n    {\n        GSSNode {\n            value: f(&self.value),\n      \
    \      predecessors: self.predecessors.clone().into_iter()\n                .map(|node|\
    \ Arc::new(node.map(f)))\n                .collect(),\n        }\n    }\n}\n\n\
    \nimpl<T> Drop for GSSNode<T> {\n    fn drop(&mut self) {\n         let mut cur_nodes\
    \ = std::mem::take(&mut self.predecessors);\n        while let Some(node) = cur_nodes.pop()\
    \ {\n            if let Ok(mut inner_node) = Arc::try_unwrap(node) {\n       \
    \         cur_nodes.append(&mut inner_node.predecessors);\n            }\n   \
    \     }\n    }\n}\n\n\npub trait GSSTrait<T: Clone> {\n    type Peek<'a> where\
    \ T: 'a, Self: 'a;\n    fn peek(&self) -> Self::Peek<'_>;\n    fn push(&self,\
    \ value: T) -> GSSNode<T>;\n    fn pop(&self) -> Vec<Arc<GSSNode<T>>>;\n    fn\
    \ popn(&self, n: usize) -> Vec<Arc<GSSNode<T>>>;\n}\n\nimpl<T: Clone> GSSTrait<T>\
    \ for GSSNode<T> {\n    type Peek<'a> = &'a T where T: 'a;\n\n    fn peek(&self)\
    \ -> Self::Peek<'_> {\n        &self.value\n    }\n\n    fn push(&self, value:\
    \ T) -> GSSNode<T> {\n        let mut new_node = GSSNode::new(value);\n      \
    \  new_node.predecessors.push(Arc::new(self.clone()));\n        new_node\n   \
    \ }\n\n    fn pop(&self) -> Vec<Arc<GSSNode<T>>> {\n        self.predecessors.clone()\n\
    \    }\n\n    fn popn(&self, n: usize) -> Vec<Arc<GSSNode<T>>> {\n        if n\
    \ == 0 {\n            return vec![Arc::new(self.clone())];\n        }\n      \
    \  let mut nodes = Vec::new();\n        for popped in self.pop() {\n         \
    \   nodes.extend(popped.popn(n - 1));\n        }\n        nodes\n    }\n}\n\n\
    impl<T: Clone> GSSTrait<T> for Arc<GSSNode<T>> {\n    type Peek<'a> = &'a T where\
    \ T: 'a;\n\n    fn peek(&self) -> Self::Peek<'_> {\n        &self.value\n    }\n\
    \n    fn push(&self, value: T) -> GSSNode<T> {\n         let mut new_node = GSSNode::new(value);\n\
    \        new_node.predecessors.push(self.clone());\n        new_node\n    }\n\n\
    \    fn pop(&self) -> Vec<Arc<GSSNode<T>>> {\n         self.predecessors.clone()\n\
    \    }\n\n    fn popn(&self, n: usize) -> Vec<Arc<GSSNode<T>>> {\n         if\
    \ n == 0 {\n            return vec![self.clone()];\n        }\n        let mut\
    \ nodes = Vec::new();\n        for popped in self.pop() {\n            nodes.extend(popped.popn(n\
    \ - 1));\n        }\n        nodes\n    }\n}\n\n\nimpl<T: Clone> GSSTrait<T> for\
    \ Option<Arc<GSSNode<T>>> {\n    type Peek<'a> = Option<&'a T> where T: 'a;\n\n\
    \    fn peek(&self) -> Self::Peek<'_> {\n        self.as_ref().map(|node| node.peek())\n\
    \    }\n\n    fn push(&self, value: T) -> GSSNode<T> {\n        self.clone().map(|node|\
    \ node.push(value.clone())).unwrap_or_else(|| GSSNode::new(value))\n    }\n\n\
    \    fn pop(&self) -> Vec<Arc<GSSNode<T>>> {\n        self.as_ref().map(|node|\
    \ node.pop()).unwrap_or_default()\n    }\n\n    fn popn(&self, n: usize) -> Vec<Arc<GSSNode<T>>>\
    \ {\n        self.as_ref().map(|node| node.popn(n)).unwrap_or_default()\n    }\n\
    }\n\nimpl<T: Clone> GSSTrait<T> for Option<GSSNode<T>> {\n    type Peek<'a> =\
    \ Option<&'a T> where T: 'a;\n\n    fn peek(&self) -> Self::Peek<'_> {\n     \
    \   self.as_ref().map(|node| node.peek())\n    }\n\n    fn push(&self, value:\
    \ T) -> GSSNode<T> {\n         self.clone().map(|node| node.push(value.clone())).unwrap_or_else(||\
    \ GSSNode::new(value))\n    }\n\n    fn pop(&self) -> Vec<Arc<GSSNode<T>>> {\n\
    \        self.as_ref().map(|node| node.pop()).unwrap_or_default()\n    }\n\n \
    \   fn popn(&self, n: usize) -> Vec<Arc<GSSNode<T>>> {\n        self.as_ref().map(|node|\
    \ node.popn(n)).unwrap_or_default()\n    }\n}\n\nimpl<T: Clone + Ord> BulkMerge<T>\
    \ for Vec<Arc<GSSNode<T>>> {\n    fn bulk_merge(&mut self) {\n        let mut\
    \ groups: BTreeMap<T, HashMap<_, Arc<GSSNode<T>>>> = BTreeMap::new();\n      \
    \  for node in self.drain(..) {\n            groups.entry(node.value.clone()).or_default().entry(Arc::as_ptr(&node)).or_insert(node);\n\
    \        }\n        for mut group in groups.into_values() {\n            let mut\
    \ group = group.into_values().collect::<Vec<_>>();\n            let mut first\
    \ = group.pop().unwrap();\n            if group.is_empty() {\n               \
    \ self.push(first);\n            } else {\n                let mut predecessors_set:\
    \ BTreeMap<_, _> = BTreeMap::new();\n                for sibling in group {\n\
    \                    for predecessor in &sibling.predecessors {\n            \
    \            predecessors_set.insert(Arc::as_ptr(predecessor), predecessor.clone());\n\
    \                    }\n                }\n                let first_mut_ref =\
    \ Arc::make_mut(&mut first);\n                first_mut_ref.predecessors = predecessors_set.into_values().collect();\n\
    \                self.push(first);\n            }\n        }\n    }\n}\n```\n\n\
    ```rust id=\"b5877a43\"\nuse crate::finite_automata::{greedy_group, groups, non_greedy_group,\
    \ ExprGroup, ExprGroups};\nuse crate::finite_automata::{Expr, Regex};\nuse crate::glr::grammar::{NonTerminal,\
    \ Production, Symbol, Terminal};\nuse crate::glr::parser::{GLRParser};\nuse crate::glr::table::{assign_non_terminal_ids,\
    \ generate_glr_parser, generate_glr_parser_with_maps, NonTerminalID, TerminalID};\n\
    use crate::precompute::{precompute, LLMTokenID, Token, Tokenizer};\nuse bimap::BiBTreeMap;\n\
    use std::collections::{BTreeMap, BTreeSet, HashSet};\nuse std::fmt::{Debug, Formatter};\n\
    use kdam::tqdm;\nuse crate::analyze_grammar::drop_dead;\nuse crate::constraint::{GrammarConstraint};\n\
    use crate::debug;\n\ntype LLMToken<'a> = &'a [u8];\ntype LLMTokenMap = BiBTreeMap<Vec<u8>,\
    \ LLMTokenID>;\n\n#[derive(Clone)]\npub struct Grammar<T> {\n    pub productions:\
    \ Vec<Production>,\n    pub start_production_id: usize,\n    pub literal_map:\
    \ BTreeMap<String, String>,\n    pub terminal_name_to_group_id: BiBTreeMap<String,\
    \ usize>,\n    pub terminal_expr_to_group_id: BiBTreeMap<Expr, usize>,\n    pub\
    \ tokenizer: T,\n}\n\nimpl<T> Debug for Grammar<T> where T: Debug {\n    fn fmt(&self,\
    \ f: &mut Formatter<'_>) -> std::fmt::Result {\n        writeln!(f, \"Grammar:\"\
    )?;\n        writeln!(f, \"  Start Production ID: {}\", self.start_production_id)?;\n\
    \        writeln!(f, \"  Productions:\")?;\n\n        for production in &self.productions\
    \ {\n            write!(f, \"    {} -> \", production.lhs.0)?;\n            for\
    \ (i, symbol) in production.rhs.iter().enumerate() {\n                match symbol\
    \ {\n                    Symbol::Terminal(terminal) => write!(f, \"{}\", terminal.0)?,\n\
    \                    Symbol::NonTerminal(non_terminal) => write!(f, \"{}\", non_terminal.0)?,\n\
    \                }\n                if i < production.rhs.len() - 1 {\n      \
    \              write!(f, \" \")?;\n                }\n            }\n        \
    \    writeln!(f)?;\n        }\n\n        writeln!(f, \"  Literal Map:\")?;\n \
    \       for (literal, mangled_name) in &self.literal_map {\n            writeln!(f,\
    \ \"    {:?}: {}\", literal, mangled_name)?;\n        }\n\n        writeln!(f,\
    \ \"  Terminal Map (name to group ID):\")?;\n        for (name, group_id) in &self.terminal_name_to_group_id\
    \ {\n            writeln!(f, \"    {:?}: {}\", name, group_id)?;\n        }\n\n\
    \        writeln!(f, \"Tokenizer:\");\n        writeln!(f, \"{:?}\", &self.tokenizer);\n\
    \n        Ok(())\n    }\n}\n\nimpl<T> Grammar<T> {\n     fn mangle_literal(literal:\
    \ &str, tokens: &BTreeMap<String, Expr>) -> String {\n        let mut mangled_name\
    \ = literal.to_string();\n        let mut i = 0;\n        while tokens.contains_key(&mangled_name)\
    \ {\n            mangled_name = format!(\"{}__literal_{}\", literal, i);\n   \
    \         i += 1;\n        }\n        mangled_name\n    }\n}\n\n#[derive(Debug,\
    \ Clone, PartialEq, Eq)]\npub enum GrammarExpr {\n    RegexExpr(Expr),\n    Ref(String),\n\
    \    Sequence(Vec<GrammarExpr>),\n    Choice(Vec<GrammarExpr>),\n    Optional(Box<GrammarExpr>),\n\
    \    Repeat(Box<GrammarExpr>),\n}\n\npub fn regex(expr: Expr) -> GrammarExpr {\n\
    \    GrammarExpr::RegexExpr(expr)\n}\n\npub fn r#ref(name: &str) -> GrammarExpr\
    \ {\n    GrammarExpr::Ref(name.to_string())\n}\n\npub fn sequence(exprs: Vec<GrammarExpr>)\
    \ -> GrammarExpr {\n    GrammarExpr::Sequence(exprs)\n}\n\npub fn choice(exprs:\
    \ Vec<GrammarExpr>) -> GrammarExpr {\n    GrammarExpr::Choice(exprs)\n}\n\npub\
    \ fn optional(expr: GrammarExpr) -> GrammarExpr {\n    GrammarExpr::Optional(Box::new(expr))\n\
    }\n\npub fn repeat(expr: GrammarExpr) -> GrammarExpr {\n    GrammarExpr::Repeat(Box::new(expr))\n\
    }\n\nimpl<T> Grammar<T> {\n    pub fn glr_parser(&self) -> GLRParser {\n     \
    \   generate_glr_parser(&self.productions, self.start_production_id)\n    }\n\
    }\n\nimpl Grammar<Regex> {\n    pub fn from_exprs(exprs: Vec<(String, GrammarExpr)>)\
    \ -> Self {\n         let mut productions = Vec::new();\n        let mut literal_map\
    \ = BTreeMap::new();\n        let mut terminal_name_to_group_id = BiBTreeMap::new();\n\
    \        let mut terminal_expr_to_group_id = BiBTreeMap::new();\n        let mut\
    \ next_terminal_id = 0;\n\n        let mut start_production_name = \"start'\"\
    .to_string();\n        let nonterminals: HashSet<&str> = exprs.iter().map(|(name,\
    \ _)| name.as_str()).collect();\n        while nonterminals.contains(&start_production_name.as_str())\
    \ {\n            start_production_name.push('\\'');\n        }\n        debug!(2,\
    \ \"start_production_name: {:?}\", start_production_name);\n        productions.push(Production\
    \ {\n            lhs: NonTerminal(start_production_name),\n            rhs: vec![Symbol::NonTerminal(NonTerminal(exprs[0].0.clone()))],\n\
    \        });\n\n        fn convert_expr(\n            expr: &GrammarExpr,\n  \
    \          productions: &mut Vec<Production>,\n            non_terminal_map: &mut\
    \ BiBTreeMap<NonTerminal, NonTerminalID>,\n            next_non_terminal_id: &mut\
    \ usize,\n            literal_map: &mut BTreeMap<String, String>,\n          \
    \  tokens: &mut BTreeMap<String, Expr>,\n            terminal_name_to_group_id:\
    \ &mut BiBTreeMap<String, usize>,\n            terminal_expr_to_group_id: &mut\
    \ BiBTreeMap<Expr, usize>,\n            next_terminal_id: &mut usize,\n      \
    \  ) -> Vec<Symbol> {\n            match expr {\n                 GrammarExpr::RegexExpr(regex_expr)\
    \ => {\n                    if let Some(terminal_id) = terminal_expr_to_group_id.get_by_left(&regex_expr)\
    \ {\n                        vec![Symbol::Terminal(Terminal(format!(\"__regex_{}\"\
    , terminal_id)))]\n                    } else {\n                        let terminal_id\
    \ = *next_terminal_id;\n                        let terminal_name = format!(\"\
    __regex_{}\", terminal_id);\n                        terminal_name_to_group_id.insert(terminal_name.clone(),\
    \ terminal_id);\n                        terminal_expr_to_group_id.insert(regex_expr.clone(),\
    \ terminal_id);\n                        tokens.insert(terminal_name.clone(),\
    \ regex_expr.clone());\n                        *next_terminal_id += 1;\n    \
    \                    vec![Symbol::Terminal(Terminal(terminal_name))]\n       \
    \             }\n                }\n                GrammarExpr::Ref(name) =>\
    \ vec![Symbol::NonTerminal(NonTerminal(name.clone()))],\n                GrammarExpr::Sequence(exprs)\
    \ => exprs.iter()\n                    .flat_map(|e| convert_expr(\n         \
    \               e,\n                        productions,\n                   \
    \     non_terminal_map,\n                        next_non_terminal_id,\n     \
    \                   literal_map,\n                        tokens,\n          \
    \              terminal_name_to_group_id,\n                        terminal_expr_to_group_id,\n\
    \                        next_terminal_id,\n                    )).collect(),\n\
    \                GrammarExpr::Choice(exprs) => {\n                     let new_nonterminal\
    \ = format!(\"Choice{}\", *next_non_terminal_id);\n                    let nt\
    \ = NonTerminal(new_nonterminal.clone());\n\n                    if !non_terminal_map.contains_left(&nt)\
    \ {\n                        non_terminal_map.insert(nt.clone(), NonTerminalID(*next_non_terminal_id));\n\
    \                        *next_non_terminal_id += 1;\n                    }\n\n\
    \                    for expr in exprs {\n                        let rhs = convert_expr(\n\
    \                            expr,\n                            productions,\n\
    \                            non_terminal_map,\n                            next_non_terminal_id,\n\
    \                            literal_map,\n                            tokens,\n\
    \                            terminal_name_to_group_id,\n                    \
    \        terminal_expr_to_group_id,\n                            next_terminal_id,\n\
    \                        );\n                        productions.push(Production\
    \ {\n                            lhs: nt.clone(),\n                          \
    \  rhs,\n                        });\n                    }\n\n              \
    \      vec![Symbol::NonTerminal(nt)]\n                }\n                 GrammarExpr::Optional(expr)\
    \ => {\n                    convert_expr(\n                        &GrammarExpr::Choice(vec![*expr.clone(),\
    \ GrammarExpr::Sequence(vec![])]),\n                        productions,\n   \
    \                     non_terminal_map,\n                        next_non_terminal_id,\n\
    \                        literal_map,\n                        tokens,\n     \
    \                   terminal_name_to_group_id,\n                        terminal_expr_to_group_id,\n\
    \                        next_terminal_id,\n                    )\n          \
    \      }\n                GrammarExpr::Repeat(expr) => {\n                   \
    \ let nonterminal_id = *next_non_terminal_id;\n                    let nonterminal_name\
    \ = format!(\"Repeat{}\", nonterminal_id);\n                    non_terminal_map.insert(NonTerminal(nonterminal_name.clone()),\
    \ NonTerminalID(nonterminal_id));\n                    *next_non_terminal_id +=\
    \ 1;\n                    let rhs = convert_expr(\n                        expr,\n\
    \                        productions,\n                        non_terminal_map,\n\
    \                        next_non_terminal_id,\n                        literal_map,\n\
    \                        tokens,\n                        terminal_name_to_group_id,\n\
    \                        terminal_expr_to_group_id,\n                        next_terminal_id,\n\
    \                    );\n                    productions.push(Production {\n \
    \                       lhs: NonTerminal(nonterminal_name.clone()),\n        \
    \                rhs,\n                    });\n                    vec![Symbol::NonTerminal(NonTerminal(nonterminal_name))]\n\
    \                }\n            }\n        }\n\n        let mut non_terminal_map\
    \ = BiBTreeMap::new();\n        let mut next_non_terminal_id = 0;\n        let\
    \ mut tokens = BTreeMap::new();\n\n        for (name, expr) in tqdm!(exprs.iter())\
    \ {\n            let rhs = convert_expr(\n                expr,\n            \
    \    &mut productions,\n                &mut non_terminal_map,\n             \
    \   &mut next_non_terminal_id,\n                &mut literal_map,\n          \
    \      &mut tokens,\n                &mut terminal_name_to_group_id,\n       \
    \         &mut terminal_expr_to_group_id,\n                &mut next_terminal_id,\n\
    \            );\n            productions.push(Production {\n                lhs:\
    \ NonTerminal(name.clone()),\n                rhs,\n            });\n        }\n\
    \n\n        let tokenizer_exprs_vec: Vec<ExprGroup> = tokens\n            .into_iter()\n\
    \            .map(|(_, expr)| greedy_group(expr))\n            .collect();\n \
    \       let tokenizer_expr_groups = groups(tokenizer_exprs_vec);\n        debug!(2,\
    \ \"Building tokenizer\");\n        let tokenizer = tokenizer_expr_groups.clone().build();\n\
    \n        debug!(2, \"Done defining grammar\");\n        Self {\n            productions,\n\
    \            start_production_id: 0,\n            literal_map,\n            terminal_name_to_group_id,\n\
    \            terminal_expr_to_group_id,\n            tokenizer,\n        }\n \
    \   }\n}\n\nimpl<T: Tokenizer> GrammarConstraint<T> {\n    pub fn from_grammar(grammar:\
    \ Grammar<T>, llm_tokens: LLMTokenMap, eof_llm_token_id: usize, max_llm_token_id:\
    \ usize) -> Self {\n        debug!(2, \"GrammarConstraint::from_grammar\");\n\
    \        let terminal_map = grammar.terminal_name_to_group_id.iter().map(|(name,\
    \ group_id)| { (Terminal(name.clone()), TerminalID(*group_id)) }).collect();\n\
    \        let non_terminal_map = assign_non_terminal_ids(&grammar.productions);\n\
    \        debug!(2, \"Generating GLR parser\");\n        let parser = generate_glr_parser_with_maps(&grammar.productions,\
    \ grammar.start_production_id, terminal_map, non_terminal_map);\n\n        debug!(2,\
    \ \"Precomputing\");\n        let mut precomputed = precompute(&grammar.tokenizer,\
    \ &llm_tokens, LLMTokenID(eof_llm_token_id), max_llm_token_id);\n        debug!(2,\
    \ \"precomputed.len(): {}\", precomputed.len());\n        crate::precompute::precompute_add_eof(&mut\
    \ precomputed, LLMTokenID(eof_llm_token_id), parser.eof_terminal_id.0, max_llm_token_id);\n\
    \        debug!(2, \"precomputed.len(): {}\", precomputed.len());\n        debug!(2,\
    \ \"Done precomputing\");\n\n        Self {\n            tokenizer: grammar.tokenizer,\n\
    \            parser,\n            precomputed,\n            max_llm_token_id,\n\
    \        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use std::sync::{Arc, Mutex};\n\
    \    use bitvec::prelude::*;\n    use super::*;\n    use crate::finite_automata::eat_u8;\n\
    \    use crate::glr::table::generate_glr_parser;\n    use crate::precompute::{print_precomputed,\
    \ LLMTokenID};\n    use crate::{choice_fast, groups, seq_fast};\n    use crate::tokenizer_combinators::{eat_u8_fast,\
    \ eat_u8_negation_fast, eat_u8_range_fast, repeat0_fast};\n    use crate::trie::TrieNode;\n\
    \n\n    fn bitvec_with_capacity_and_values(capacity: usize, values: Vec<usize>)\
    \ -> BitVec {\n        let mut bitvec = BitVec::new();\n        bitvec.resize(capacity,\
    \ false);\n        for value in values {\n            bitvec.set(value, true);\n\
    \        }\n        bitvec\n    }\n\n     #[test]\n    fn test_grammar_from_exprs()\
    \ {\n        let exprs = vec![\n            (\n                \"E\".to_string(),\n\
    \                choice(vec![\n                    sequence(vec![\n          \
    \              r#ref(\"E\"),\n                        regex(eat_u8(b'+')),\n \
    \                       r#ref(\"T\"),\n                    ]),\n             \
    \       r#ref(\"T\"),\n                ]),\n            ),\n            (\n  \
    \              \"T\".to_string(),\n                choice(vec![\n            \
    \        sequence(vec![\n                        r#ref(\"T\"),\n             \
    \           regex(eat_u8(b'*')),\n                        r#ref(\"F\"),\n    \
    \                ]),\n                    r#ref(\"F\"),\n                ]),\n\
    \            ),\n            (\n                \"F\".to_string(),\n         \
    \       choice(vec![\n                    sequence(vec![\n                   \
    \     regex(eat_u8(b'(')),\n                        r#ref(\"E\"),\n          \
    \              regex(eat_u8(b')')),\n                    ]),\n               \
    \     regex(eat_u8(b'i')),\n                ]),\n            ),\n        ];\n\n\
    \        let grammar = Grammar::from_exprs(exprs.clone());\n        debug!(2,\
    \ \"{:?}\", &grammar);\n\n        let parser = grammar.glr_parser();\n       \
    \ debug!(2, \"{:?}\", &parser);\n\n        let llm_tokens: Vec<Vec<u8>> = vec![b\"\
    i\".to_vec(), b\"+\".to_vec(), b\"*\".to_vec(), b\"(\".to_vec(), b\")\".to_vec(),\
    \ b\"(i\".to_vec(), b\"+i\".to_vec()];\n        let llm_token_map: LLMTokenMap\
    \ = llm_tokens.iter().enumerate().map(|(i, token)| (token.clone(), LLMTokenID(i))).collect();\n\
    \        let eof_llm_token_id = llm_tokens.len();\n        let max_llm_token_id\
    \ = llm_tokens.len();\n        let grammar_constraint = GrammarConstraint::from_grammar(grammar,\
    \ llm_token_map.clone(), eof_llm_token_id, max_llm_token_id);\n        let mut\
    \ grammar_constraint_state = grammar_constraint.init();\n\n        macro_rules!\
    \ llm_token_vec {\n            ($($token:expr),* $(,)?) => {\n               \
    \ vec![\n                    $(\n                        llm_token_map.get_by_left(&$token.to_vec()).unwrap().0,\n\
    \                    )*\n                ]\n            }\n        }\n\n     \
    \    for (tokenizer_state, root) in grammar_constraint_state.get_precomputed()\
    \ {\n            debug!(3, \"Tokenizer state: {}\", tokenizer_state.0);\n    \
    \        for node in TrieNode::all_nodes(Arc::new(Mutex::new(root.clone()))) {\n\
    \                debug!(3, \"Node address: {:p}, value: {:?}\", Arc::as_ptr(&node),\
    \ node.try_lock().unwrap().value);\n                for (edge, dest) in node.try_lock().unwrap().children()\
    \ {\n                    debug!(3, \"    Edge value: {:?}, destination address:\
    \ {:p}\", edge, Arc::as_ptr(&dest));\n                }\n            }\n     \
    \   }\n\n        let mask = grammar_constraint_state.get_mask();\n        let\
    \ expected_mask = bitvec_with_capacity_and_values(llm_tokens.len() + 1, llm_token_vec!(b\"\
    i\", b\"(\", b\"(i\"));\n        assert_eq!(mask, expected_mask);\n\n        let\
    \ prefill: Vec<_> = llm_token_vec!(b\"(i\", b\"+i\", b\"*\", b\"i\").into_iter().map(|token_id|\
    \ LLMTokenID(token_id)).collect();\n        grammar_constraint_state.commit_many(&prefill);\n\
    \n        let mask = grammar_constraint_state.get_mask();\n        let expected_mask\
    \ = bitvec_with_capacity_and_values(llm_tokens.len() + 1, llm_token_vec!(b\"+\"\
    , b\"*\", b\")\", b\"+i\"));\n        assert_eq!(mask, expected_mask);\n\n   \
    \     let terminals: Vec<_> = llm_token_vec!(b\")\").into_iter().map(|token_id|\
    \ LLMTokenID(token_id)).collect();\n        grammar_constraint_state.commit_many(&terminals);\n\
    \        let mask = grammar_constraint_state.get_mask();\n        let mut expected_mask\
    \ = bitvec_with_capacity_and_values(llm_tokens.len() + 1, llm_token_vec!(b\"+\"\
    , b\"*\", b\"+i\"));\n        expected_mask.set(llm_tokens.len(), true);\n   \
    \     assert_eq!(mask, expected_mask);\n    }\n\n    #[test]\n    fn test_grammar_from_exprs_simple()\
    \ {\n        let exprs = vec![\n            (\n                \"E\".to_string(),\n\
    \                sequence(vec![\n                    regex(eat_u8(b'a')),\n  \
    \                  regex(eat_u8(b'b')),\n                ]),\n            ),\n\
    \        ];\n\n        let grammar = Grammar::from_exprs(exprs.clone());\n   \
    \     dbg!(&grammar);\n\n        let parser = grammar.glr_parser();\n        dbg!(&parser);\n\
    \n        let llm_tokens: Vec<Vec<u8>> = vec![b\"a\".to_vec(), b\"b\".to_vec()];\n\
    \        let llm_token_map: LLMTokenMap = llm_tokens.iter().enumerate().map(|(i,\
    \ token)| (token.clone(), LLMTokenID(i))).collect();\n        let eof_llm_token_id\
    \ = llm_tokens.len();\n        let max_llm_token_id = llm_tokens.len();\n    \
    \    let grammar_constraint = GrammarConstraint::from_grammar(grammar, llm_token_map.clone(),\
    \ eof_llm_token_id, max_llm_token_id);\n        let mut grammar_constraint_state\
    \ = grammar_constraint.init();\n\n        for (tokenizer_state, root) in grammar_constraint_state.get_precomputed()\
    \ {\n            debug!(1, \"Tokenizer state: {}\", tokenizer_state.0);\n    \
    \        for node in TrieNode::all_nodes(Arc::new(Mutex::new(root.clone()))) {\n\
    \                debug!(1, \"Node address: {:p}, value: {:?}\", Arc::as_ptr(&node),\
    \ node.try_lock().unwrap().value);\n                for (edge, dest) in node.try_lock().unwrap().children()\
    \ {\n                    debug!(1, \"    Edge value: {:?}, destination address:\
    \ {:p}\", edge, Arc::as_ptr(&dest));\n                }\n            }\n     \
    \   }\n\n        macro_rules! llm_token_vec {\n            ($($token:expr),* $(,)?)\
    \ => {\n                vec![\n                    $(\n                      \
    \  llm_token_map.get_by_left(&$token.to_vec()).unwrap().0,\n                 \
    \   )*\n                ]\n            }\n        }\n\n        let mask = grammar_constraint_state.get_mask();\n\
    \        let expected_mask = bitvec_with_capacity_and_values(llm_tokens.len()\
    \ + 1, llm_token_vec!(b\"a\"));\n        assert_eq!(mask, expected_mask);\n\n\
    \        let terminals: Vec<_> = llm_token_vec!(b\"a\").into_iter().map(|token_id|\
    \ LLMTokenID(token_id)).collect();\n        grammar_constraint_state.commit_many(&terminals);\n\
    \n        let mask = grammar_constraint_state.get_mask();\n        let expected_mask\
    \ = bitvec_with_capacity_and_values(llm_tokens.len() + 1, llm_token_vec!(b\"b\"\
    ));\n        assert_eq!(mask, expected_mask);\n    }\n\n    #[test]\n    fn test_grammar_from_exprs_very_simple()\
    \ {\n        let exprs = vec![\n            (\n                \"E\".to_string(),\n\
    \                regex(eat_u8(b'a')),\n            ),\n        ];\n\n        let\
    \ grammar = Grammar::from_exprs(exprs.clone());\n        dbg!(&grammar);\n\n \
    \       let parser = grammar.glr_parser();\n        dbg!(&parser);\n\n       \
    \ let llm_tokens: Vec<Vec<u8>> = vec![b\"a\".to_vec()];\n        let llm_token_map:\
    \ LLMTokenMap = llm_tokens.iter().enumerate().map(|(i, token)| (token.clone(),\
    \ LLMTokenID(i))).collect();\n        let eof_llm_token_id = llm_tokens.len();\n\
    \        let max_llm_token_id = llm_tokens.len();\n        let grammar_constraint\
    \ = GrammarConstraint::from_grammar(grammar, llm_token_map.clone(), eof_llm_token_id,\
    \ max_llm_token_id);\n        let mut grammar_constraint_state = grammar_constraint.init();\n\
    \n        print_precomputed(&grammar_constraint_state.get_precomputed());\n\n\
    \        for (tokenizer_state, root) in grammar_constraint_state.get_precomputed()\
    \ {\n            debug!(1, \"Tokenizer state: {}\", tokenizer_state.0);\n    \
    \        for node in TrieNode::all_nodes(Arc::new(Mutex::new(root.clone()))) {\n\
    \                debug!(1, \"Node address: {:p}, value: {:?}\", Arc::as_ptr(&node),\
    \ node.try_lock().unwrap().value);\n                 for (edge, dest) in node.try_lock().unwrap().children()\
    \ {\n                    debug!(1, \"    Edge value: {:?}, destination address:\
    \ {:p}\", edge, Arc::as_ptr(&dest));\n                }\n            }\n     \
    \   }\n\n        macro_rules! llm_token_vec {\n            ($($token:expr),* $(,)?)\
    \ => {\n                vec![\n                    $(\n                      \
    \  llm_token_map.get_by_left(&$token.to_vec()).unwrap().0,\n                 \
    \   )*\n                ]\n            }\n        }\n\n        let mask = grammar_constraint_state.get_mask();\n\
    \        let expected_mask = bitvec_with_capacity_and_values(llm_tokens.len()\
    \ + 1, llm_token_vec!(b\"a\"));\n        assert_eq!(mask, expected_mask);\n\n\
    \        let terminals: Vec<_> = llm_token_vec!(b\"a\").into_iter().map(|token_id|\
    \ LLMTokenID(token_id)).collect();\n        grammar_constraint_state.commit_many(&terminals);\n\
    \n        let mask = grammar_constraint_state.get_mask();\n        let mut expected_mask\
    \ = bitvec_with_capacity_and_values(llm_tokens.len() + 1, llm_token_vec!());\n\
    \         expected_mask.set(llm_tokens.len(), true);\n        assert_eq!(mask,\
    \ expected_mask);\n    }\n\n    #[test]\n    fn test_precompute_for_python_name_token()\
    \ {\n        let ignore = repeat0_fast(choice_fast!(eat_u8_fast(b' '), seq_fast!(eat_u8_fast(b'#'),\
    \ repeat0_fast(eat_u8_negation_fast(b'\\n')), eat_u8_fast(b'\\n'))));\n      \
    \  let digit = eat_u8_range_fast(b'0', b'9');\n        let alph_lower = eat_u8_range_fast(b'a',\
    \ b'z');\n        let alph_upper = eat_u8_range_fast(b'A', b'Z');\n\n        let\
    \ name_start = choice_fast!(alph_lower, alph_upper, eat_u8_fast(b'_'));\n    \
    \    let name_middle = choice_fast!(name_start.clone(), digit);\n        let name\
    \ = seq_fast!(ignore, name_start, repeat0_fast(seq_fast!(name_middle)));\n\n \
    \       let tokenizer = name.build();\n        dbg!(&tokenizer);\n\n         let\
    \ llm_tokens: Vec<Vec<u8>> = (0..2).map(|i| format!(\"abcdefghijk{}\", i).as_bytes().to_vec()).collect();\n\
    \         let llm_token_map: LLMTokenMap = llm_tokens.iter().enumerate().map(|(i,\
    \ token)| (token.clone(), LLMTokenID(i))).collect();\n        let eof_llm_token_id\
    \ = llm_tokens.len();\n        let max_llm_token_id = llm_tokens.len();\n    \
    \    let precomputed = precompute(&tokenizer, &llm_token_map, LLMTokenID(eof_llm_token_id),\
    \ max_llm_token_id);\n        print_precomputed(&precomputed);\n        println!(\"\
    Done precomputing\");\n    }\n\n    #[test]\n    fn test_precompute_explosion()\
    \ {\n        let tokenizer = groups![\n            eat_u8(b'a'),\n           \
    \ eat_u8(b'a'),\n        ].build();\n\n         let llm_tokens: Vec<Vec<u8>> =\
    \ vec![b\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\".to_vec()];\n        let llm_token_map:\
    \ LLMTokenMap = llm_tokens.iter().enumerate().map(|(i, token)| (token.clone(),\
    \ LLMTokenID(i))).collect();\n        let eof_llm_token_id = llm_tokens.len();\n\
    \        let max_llm_token_id = llm_tokens.len();\n        let precomputed = precompute(&tokenizer,\
    \ &llm_token_map, LLMTokenID(eof_llm_token_id), max_llm_token_id);\n        print_precomputed(&precomputed);\n\
    \        println!(\"Done precomputing\");\n    }\n}\n```\n\n```rust id=\"17db0384\"\
    \nuse sep1::finite_automata::{Expr as RegexExpr, ExprGroups as RegexGroups, greedy_group,\
    \ non_greedy_group, groups as regex_groups, _choice as regex_choice, eat_u8, eat_u8_negation,\
    \ eat_u8_set, eps, opt, prec, rep, rep1, _seq as regex_seq};\nuse sep1::finite_automata::Regex;\n\
    use pyo3::prelude::*;\nuse pyo3::types::{PyDict};\nuse sep1::glr::grammar::{NonTerminal,\
    \ Production, Symbol, Terminal};\nuse sep1::glr::parser::GLRParser;\nuse sep1::glr::table::{generate_glr_parser,\
    \ StateID};\nuse sep1::interface::{Grammar, GrammarExpr, choice as grammar_choice,\
    \ optional as grammar_optional, regex as grammar_regex, repeat as grammar_repeat,\
    \ r#ref as grammar_ref, sequence as grammar_sequence};\nuse sep1::constraint::{GrammarConstraint,\
    \ GrammarConstraintState};\nuse sep1::precompute::{print_precomputed, LLMTokenID,\
    \ Tokenizer};\nuse std::collections::{BTreeMap, BTreeSet};\nuse bimap::BiBTreeMap;\n\
    use numpy::{IntoPyArray, PyArray1, ToPyArray};\nuse sep1::u8set::U8Set;\n\n#[pyclass]\n\
    #[derive(Clone)]\nstruct PyGrammarExpr {\n    inner: GrammarExpr,\n}\n\n#[pymethods]\n\
    impl PyGrammarExpr {\n    #[staticmethod]\n    fn r#ref(name: &str) -> PyResult<Self>\
    \ {\n        Ok(Self {\n            inner: grammar_ref(name),\n        })\n  \
    \  }\n\n    #[staticmethod]\n    fn sequence(exprs: Vec<PyGrammarExpr>) -> Self\
    \ {\n        Self {\n            inner: grammar_sequence(exprs.into_iter().map(|e|\
    \ e.inner).collect()),\n        }\n    }\n\n    #[staticmethod]\n    fn choice(exprs:\
    \ Vec<PyGrammarExpr>) -> Self {\n        Self {\n            inner: grammar_choice(exprs.into_iter().map(|e|\
    \ e.inner).collect()),\n        }\n    }\n\n    #[staticmethod]\n    fn optional(expr:\
    \ PyGrammarExpr) -> Self {\n        Self {\n            inner: grammar_optional(expr.inner),\n\
    \        }\n    }\n\n    #[staticmethod]\n    fn repeat(expr: PyGrammarExpr) ->\
    \ Self {\n        Self {\n            inner: grammar_repeat(expr.inner),\n   \
    \     }\n    }\n\n    #[staticmethod]\n    fn regex(regex: PyRegexExpr) -> Self\
    \ {\n        Self {\n            inner: grammar_regex(regex.inner)\n        }\n\
    \    }\n}\n\n#[pyclass]\n#[derive(Clone)]\nstruct PyRegexExpr {\n    inner: RegexExpr,\n\
    }\n\n#[pymethods]\nimpl PyRegexExpr {\n    #[staticmethod]\n    fn eat_u8(c: u8)\
    \ -> Self {\n        Self { inner: eat_u8(c) }\n    }\n\n    #[staticmethod]\n\
    \    fn eat_u8_negation(c: u8) -> Self {\n        Self { inner: eat_u8_negation(c)\
    \ }\n    }\n\n    #[staticmethod]\n    fn rep(expr: PyRegexExpr) -> Self {\n \
    \       Self { inner: rep(expr.inner) }\n    }\n\n    #[staticmethod]\n    fn\
    \ rep1(expr: PyRegexExpr) -> Self {\n        Self { inner: rep1(expr.inner) }\n\
    \    }\n\n    #[staticmethod]\n    fn opt(expr: PyRegexExpr) -> Self {\n     \
    \   Self { inner: opt(expr.inner) }\n    }\n\n    #[staticmethod]\n    fn prec(precedence:\
    \ isize, expr: PyRegexExpr) -> PyRegexGroup {\n        PyRegexGroup { inner: prec(precedence,\
    \ expr.inner) }\n    }\n\n    #[staticmethod]\n    fn eps() -> Self {\n      \
    \  Self { inner: eps() }\n    }\n\n    #[staticmethod]\n    fn seq(exprs: Vec<PyRegexExpr>)\
    \ -> Self {\n        Self { inner: regex_seq(exprs.into_iter().map(|e| e.inner).collect())\
    \ }\n    }\n\n    #[staticmethod]\n    fn choice(exprs: Vec<PyRegexExpr>) -> Self\
    \ {\n        Self { inner: regex_choice(exprs.into_iter().map(|e| e.inner).collect())\
    \ }\n    }\n\n    fn build(&self) -> PyResult<PyRegex> {\n        Ok(PyRegex {\
    \ inner: self.inner.clone().build() })\n    }\n}\n\n#[pyclass]\n#[derive(Clone)]\n\
    struct PyRegexGroup {\n    inner: sep1::finite_automata::ExprGroup,\n}\n\n#[pymethods]\n\
    impl PyRegexGroup {\n    #[staticmethod]\n    fn greedy_group(expr: PyRegexExpr)\
    \ -> Self {\n        Self { inner: greedy_group(expr.inner) }\n    }\n\n    #[staticmethod]\n\
    \    fn non_greedy_group(expr: PyRegexExpr) -> Self {\n        Self { inner: non_greedy_group(expr.inner)\
    \ }\n    }\n}\n\n#[pyclass]\n#[derive(Clone)]\nstruct PyRegexGroups {\n    inner:\
    \ RegexGroups,\n}\n\n#[pymethods]\nimpl PyRegexGroups {\n    #[staticmethod]\n\
    \    fn groups(groups: Vec<PyRegexGroup>) -> Self {\n        Self {\n        \
    \    inner: regex_groups(groups.into_iter().map(|g| g.inner).collect()),\n   \
    \     }\n    }\n\n    fn build(&self) -> PyResult<PyRegex> {\n        Ok(PyRegex\
    \ { inner: self.inner.clone().build() })\n    }\n}\n\n#[pyclass]\n#[derive(Clone)]\n\
    pub struct PyRegex {\n    inner: Regex,\n}\n\n#[pymethods]\nimpl PyRegex {\n}\n\
    \n#[pyclass]\n#[derive(Clone)]\npub struct PyGrammar {\n    inner: Grammar<Regex>,\n\
    }\n\n#[pymethods]\nimpl PyGrammar {\n    #[new]\n    fn new(exprs: Vec<(String,\
    \ PyGrammarExpr)>) -> Self {\n         let inner = Grammar::from_exprs(exprs.into_iter().map(|(s,\
    \ e)| (s, e.inner)).collect());\n        Self { inner }\n    }\n\n    fn glr_parser(&self)\
    \ -> PyGLRParser {\n        PyGLRParser { inner: self.inner.glr_parser() }\n \
    \   }\n\n    fn print(&self) {\n        println!(\"{:?}\", self.inner)\n    }\n\
    }\n\n#[pyclass]\n#[derive(Clone)]\npub struct PyGLRParser {\n    inner: GLRParser,\n\
    }\n\n#[pyclass]\n#[derive(Clone)]\npub struct PyGrammarConstraint {\n    inner:\
    \ GrammarConstraint<Regex>,\n}\n\n#[pymethods]\nimpl PyGrammarConstraint {\n \
    \   #[new]\n    fn new(py: Python, grammar: PyGrammar, token_to_id: &PyDict, eof_llm_token_id:\
    \ usize, max_llm_token_id: usize) -> PyResult<Self> {\n        let mut llm_token_map:\
    \ BiBTreeMap<Vec<u8>, LLMTokenID> = BiBTreeMap::new();\n        for (key, value)\
    \ in token_to_id.iter() {\n            let token = key.extract::<&[u8]>()?;\n\
    \            let id = value.extract::<usize>()?;\n            llm_token_map.insert(token.to_vec(),\
    \ LLMTokenID(id));\n        }\n\n        let inner = GrammarConstraint::from_grammar(grammar.inner,\
    \ llm_token_map, eof_llm_token_id, max_llm_token_id);\n        Ok(Self { inner\
    \ })\n    }\n\n    fn print(&self) {\n        print_precomputed(&self.inner.precomputed);\n\
    \    }\n}\n\n#[pyclass]\npub struct PyGrammarConstraintState {\n    inner: GrammarConstraintState<Regex>,\n\
    }\n\n#[pymethods]\nimpl PyGrammarConstraintState {\n    #[new]\n    fn new(grammar_constraint:\
    \ PyGrammarConstraint) -> Self {\n        Self { inner: grammar_constraint.inner.init()\
    \ }\n    }\n\n    fn get_mask<'py>(&self, py: Python<'py>) -> PyResult<Bound<'py,\
    \ PyArray1<bool>>> {\n        let bitset = self.inner.get_mask();\n        let\
    \ bools: Vec<bool> = bitset.iter().map(|bit_ref| *bit_ref).collect();\n      \
    \  let array = bools.into_pyarray_bound(py);\n        Ok(array)\n    }\n\n   \
    \ fn commit(&mut self, llm_token_id: usize) {\n        self.inner.commit(LLMTokenID(llm_token_id));\n\
    \    }\n}\n\n\n#[pymodule]\nfn _sep1(m: &Bound<'_, PyModule>) -> PyResult<()>\
    \ {\n    m.add_class::<PyGrammarExpr>()?;\n    m.add_class::<PyRegexExpr>()?;\n\
    \    m.add_class::<PyRegexGroup>()?;\n    m.add_class::<PyRegexGroups>()?;\n \
    \   m.add_class::<PyGrammar>()?;\n    m.add_class::<PyGrammarConstraint>()?;\n\
    \    m.add_class::<PyGrammarConstraintState>()?;\n    Ok(())\n}\n```\n\n```rust\
    \ id=\"eb0ae09e\"\nextern crate core;\n\npub mod frozenset;\npub mod charmap;\n\
    pub mod precompute;\npub mod tokenizer_combinators;\npub mod u8set;\npub mod finite_automata;\n\
    mod gss;\npub mod glr;\npub mod constraint;\npub mod interface;\nmod precompute_gss;\n\
    mod trie;\nmod utils;\nmod analyze_grammar;\n\n```\n\n```rust id=\"3b3b24bf\"\n\
    pub mod table;\npub mod grammar;\npub mod items;\npub mod parser;\nmod tests;\n\
    ```\n\n```rust id=\"1cef9270\"\nuse crate::gss::BulkMerge;\nuse crate::glr::grammar::{NonTerminal,\
    \ Production, Symbol, Terminal};\nuse crate::glr::items::Item;\nuse crate::glr::table::{NonTerminalID,\
    \ ProductionID, Stage7ShiftsAndReduces, Stage7Table, StateID, TerminalID};\nuse\
    \ crate::gss::{GSSNode, GSSTrait};\nuse bimap::BiBTreeMap;\nuse std::collections::{BTreeMap,\
    \ BTreeSet};\nuse std::fmt::{Debug, Display, Formatter};\nuse std::sync::Arc;\n\
    use crate::debug;\n\n#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]\n\
    pub struct ParseState {\n    pub stack: Arc<GSSNode<StateID>>,\n    pub action_stack:\
    \ Option<Arc<GSSNode<Action>>>,\n    pub status: ParseStatus,\n}\n\n#[derive(Debug,\
    \ Clone, PartialEq, Eq, PartialOrd, Ord)]\npub enum Action {\n    Shift(TerminalID),\n\
    \    Reduce { production_id: ProductionID, len: usize, nonterminal_id: NonTerminalID\
    \ },\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub\
    \ enum ParseStatus {\n    Active,\n    Inactive(StopReason),\n}\n\n#[derive(Debug,\
    \ Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub enum StopReason {\n    ActionNotFound,\n\
    \    GotoNotFound,\n}\n\n#[derive(Clone)]\npub struct GLRParser {\n    pub stage_7_table:\
    \ Stage7Table,\n    pub productions: Vec<Production>,\n    pub terminal_map: BiBTreeMap<Terminal,\
    \ TerminalID>,\n    pub non_terminal_map: BiBTreeMap<NonTerminal, NonTerminalID>,\n\
    \    pub item_set_map: BiBTreeMap<BTreeSet<Item>, StateID>,\n    pub start_state_id:\
    \ StateID,\n    pub eof_terminal_id: TerminalID,\n}\n\nimpl GLRParser {\n    pub\
    \ fn new(\n        stage_7_table: Stage7Table,\n        productions: Vec<Production>,\n\
    \        terminal_map: BiBTreeMap<Terminal, TerminalID>,\n        non_terminal_map:\
    \ BiBTreeMap<NonTerminal, NonTerminalID>,\n        item_set_map: BiBTreeMap<BTreeSet<Item>,\
    \ StateID>,\n        start_state_id: StateID,\n        eof_terminal_id: TerminalID,\n\
    \    ) -> Self {\n        Self {\n            stage_7_table,\n            productions,\n\
    \            terminal_map,\n            non_terminal_map,\n            item_set_map,\n\
    \            start_state_id,\n            eof_terminal_id,\n        }\n    }\n\
    \n    pub fn init_glr_parser(&self) -> GLRParserState {\n        GLRParserState\
    \ {\n            parser: self,\n            active_states: vec![self.init_parse_state()],\n\
    \            inactive_states: BTreeMap::new(),\n            input_pos: 0,\n  \
    \      }\n    }\n    \n    pub fn init_glr_parser_from_parse_state(&self, parse_state:\
    \ ParseState) -> GLRParserState {\n        GLRParserState {\n            parser:\
    \ self,\n            active_states: vec![parse_state],\n            inactive_states:\
    \ BTreeMap::new(),\n            input_pos: 0,\n        }\n    }\n\n    pub fn\
    \ init_glr_parser_from_parse_states(&self, parse_states: Vec<ParseState>) -> GLRParserState\
    \ {\n        GLRParserState {\n            parser: self,\n            active_states:\
    \ parse_states,\n            inactive_states: BTreeMap::new(),\n            input_pos:\
    \ 0,\n        }\n    }\n\n    pub fn init_parse_state(&self) -> ParseState {\n\
    \        ParseState {\n            stack: Arc::new(GSSNode::new(self.start_state_id)),\n\
    \            action_stack: None,\n            status: ParseStatus::Active,\n \
    \       }\n    }\n\n    pub fn parse(&self, input: &[TerminalID]) -> GLRParserState\
    \ {\n        let mut state = self.init_glr_parser();\n        state.parse(input);\n\
    \        state\n    }\n}\n\n\nimpl Debug for GLRParser {\n    fn fmt(&self, f:\
    \ &mut Formatter<'_>) -> std::fmt::Result {\n        write!(f, \"{}\", self)\n\
    \    }\n}\n\nimpl Display for GLRParser {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>)\
    \ -> std::fmt::Result {\n        let stage_7_table = &self.stage_7_table;\n  \
    \      let terminal_map = &self.terminal_map;\n        let non_terminal_map =\
    \ &self.non_terminal_map;\n        let item_set_map = &self.item_set_map;\n\n\
    \        writeln!(f, \"Parse Table:\")?;\n        writeln!(f, \"  Start State:\
    \ {}\", self.start_state_id.0)?;\n        for (&state_id, row) in stage_7_table.iter().collect::<BTreeMap<_,\
    \ _>>() {\n            writeln!(f, \"  State {}:\", state_id.0)?;\n\n        \
    \    writeln!(f, \"    Items:\")?;\n            let item_set = item_set_map.get_by_right(&state_id).unwrap();\n\
    \            for item in item_set {\n                write!(f, \"      - {} ->\"\
    , item.production.lhs.0)?;\n                for (i, symbol) in item.production.rhs.iter().enumerate()\
    \ {\n                    if i == item.dot_position {\n                       \
    \ write!(f, \" •\")?;\n                    }\n                    match symbol\
    \ {\n                        Symbol::Terminal(terminal) => write!(f, \" {:?}\"\
    , terminal.0)?,\n                        Symbol::NonTerminal(non_terminal) =>\
    \ write!(f, \" {}\", non_terminal.0)?,\n                    }\n              \
    \  }\n                if item.dot_position == item.production.rhs.len() {\n  \
    \                  write!(f, \" •\")?;\n                }\n                writeln!(f)?;\n\
    \            }\n\n            writeln!(f, \"    Actions:\")?;\n             for\
    \ (&terminal_id, action) in &row.shifts_and_reduces {\n                let terminal\
    \ = terminal_map.get_by_right(&terminal_id).unwrap();\n                match action\
    \ {\n                    Stage7ShiftsAndReduces::Shift(next_state_id) => {\n \
    \                       writeln!(f, \"      - {:?} -> Shift {}\", terminal.0,\
    \ next_state_id.0)?;\n                    }\n                    Stage7ShiftsAndReduces::Reduce\
    \ { production_id, nonterminal_id: nonterminal, len } => {\n                 \
    \       let nt = non_terminal_map.get_by_right(nonterminal).unwrap();\n      \
    \                  writeln!(f, \"      - {:?} -> Reduce {} (len {})\", terminal.0,\
    \ nt.0, len)?;\n                    }\n                     Stage7ShiftsAndReduces::Split\
    \ { shift, reduces } => {\n                         writeln!(f, \"      - {:?}\
    \ -> Conflict:\", terminal.0)?;\n                         if let Some(shift_state)\
    \ = shift {\n                            writeln!(f, \"        - Shift {}\", shift_state.0)?;\n\
    \                        }\n                        for (len, nt_id_to_prod_ids)\
    \ in reduces {\n                            writeln!(f, \"        - Reduce (len\
    \ {}):\", len)?;\n                            for (nt_id, prod_ids) in nt_id_to_prod_ids\
    \ {\n                                let nt = non_terminal_map.get_by_right(nt_id).unwrap();\n\
    \                                for prod_id in prod_ids {\n                 \
    \                   let prod = self.productions.get(prod_id.0).unwrap();\n   \
    \                                 writeln!(f, \"          - {} -> {}\", nt.0,\
    \ prod.lhs.0)?;\n                                }\n                         \
    \   }\n\n                        }\n                     }\n                }\n\
    \            }\n\n            writeln!(f, \"    Gotos:\")?;\n            for (&non_terminal_id,\
    \ &next_state_id) in &row.gotos {\n                let non_terminal = non_terminal_map.get_by_right(&non_terminal_id).unwrap();\n\
    \                writeln!(f, \"      - {} -> {}\", non_terminal.0, next_state_id.0)?;\n\
    \            }\n        }\n\n        writeln!(f, \"\\nTerminal Map (name to terminal\
    \ ID):\")?;\n        for (terminal, terminal_id) in terminal_map {\n         \
    \   writeln!(f, \"  {} -> {}\", terminal.0, terminal_id.0)?;\n        }\n\n  \
    \      writeln!(f, \"\\nNon-Terminal Map:\")?;\n        for (non_terminal, non_terminal_id)\
    \ in non_terminal_map {\n            writeln!(f, \"  {} -> {}\", non_terminal.0,\
    \ non_terminal_id.0)?;\n        }\n\n        Ok(())\n    }\n}\n\n\n#[derive(Clone)]\n\
    pub struct GLRParserState<'a> {\n    pub parser: &'a GLRParser,\n    pub active_states:\
    \ Vec<ParseState>,\n    pub inactive_states: BTreeMap<usize, Vec<ParseState>>,\n\
    \    pub input_pos: usize,\n}\n\n\nimpl<'a> GLRParserState<'a> {\n    pub fn parse(&mut\
    \ self, input: &[TerminalID]) {\n        self.parse_part(input);\n        self.parse_eof();\n\
    \    }\n\n    pub fn parse_part(&mut self, input: &[TerminalID]) {\n        for\
    \ &token_id in input {\n            self.step(token_id);\n        }\n    }\n\n\
    \    pub fn parse_eof(&mut self) {\n         self.step(self.parser.eof_terminal_id);\n\
    \    }\n\n    pub fn step(&mut self, token_id: TerminalID) {\n        let mut\
    \ next_active_states = Vec::new();\n        let mut inactive_states = Vec::new();\n\
    \n        while let Some(state) = self.active_states.pop() {\n            let\
    \ stack = state.stack;\n            let action_stack = state.action_stack;\n \
    \           let state_id = *stack.peek();\n\n            let row = self.parser.stage_7_table.get(&state_id).unwrap();\n\
    \n            if let Some(action) = row.shifts_and_reduces.get(&token_id) {\n\
    \                match action {\n                    Stage7ShiftsAndReduces::Shift(next_state_id)\
    \ => {\n                        debug!(3, \"Shifting\");\n                   \
    \     let new_stack = stack.push(*next_state_id);\n                        let\
    \ new_actions = action_stack.push(Action::Shift(token_id));\n                \
    \        next_active_states.push(ParseState {\n                            stack:\
    \ Arc::new(new_stack),\n                            action_stack: Some(Arc::new(new_actions)),\n\
    \                            status: ParseStatus::Active,\n                  \
    \      });\n                    }\n                    Stage7ShiftsAndReduces::Reduce\
    \ { production_id, nonterminal_id: nonterminal, len } => {\n                 \
    \       debug!(3, \"Reducing by production {:?} with len {}\", production_id,\
    \ len);\n                        let mut popped_stack_nodes = stack.popn(*len);\n\
    \                        popped_stack_nodes.bulk_merge();\n                  \
    \      for stack_node in popped_stack_nodes {\n                             let\
    \ revealed_state = *stack_node.peek();\n                            let goto_row\
    \ = self.parser.stage_7_table.get(&revealed_state).unwrap();\n\n             \
    \               if let Some(&goto_state) = goto_row.gotos.get(nonterminal) {\n\
    \                                debug!(3, \"Going to state {:?}\", goto_state);\n\
    \                                let new_stack = stack_node.push(goto_state);\n\
    \                                let new_actions = action_stack.clone().push(Action::Reduce\
    \ { production_id: *production_id, len: *len, nonterminal_id: *nonterminal });\n\
    \                                self.active_states.push(ParseState {\n      \
    \                              stack: Arc::new(new_stack),\n                 \
    \                   action_stack: Some(Arc::new(new_actions)),\n             \
    \                       status: ParseStatus::Active,\n                       \
    \         });\n                            } else {\n                        \
    \         inactive_states.push(ParseState {\n                                \
    \    stack: stack_node,\n                                    action_stack: None,\n\
    \                                    status: ParseStatus::Inactive(StopReason::GotoNotFound),\n\
    \                                });\n                            }\n        \
    \                }\n                     }\n                     Stage7ShiftsAndReduces::Split\
    \ { shift, reduces } => {\n                         debug!(3, \"Split\");\n  \
    \                       if let Some(shift_state) = shift {\n                 \
    \           let new_stack = stack.push(*shift_state);\n                      \
    \       let new_actions = action_stack.clone().push(Action::Shift(token_id));\n\
    \                            next_active_states.push(ParseState {\n          \
    \                      stack: Arc::new(new_stack),\n                         \
    \       action_stack: Some(Arc::new(new_actions)),\n                         \
    \       status: ParseStatus::Active,\n                            });\n      \
    \                   }\n\n                         for (len, nt_ids) in reduces\
    \ {\n                             let mut popped_stack_nodes = stack.popn(*len);\n\
    \                             popped_stack_nodes.bulk_merge();\n             \
    \                for (nt_id, prod_ids) in nt_ids {\n                         \
    \       for stack_node in &popped_stack_nodes {\n                            \
    \        let revealed_state = *stack_node.peek();\n                          \
    \          let goto_row = self.parser.stage_7_table.get(&revealed_state).unwrap();\n\
    \                                    if let Some(&goto_state) = goto_row.gotos.get(nt_id)\
    \ {\n                                        let new_stack = Arc::new(stack_node.push(goto_state));\n\
    \                                        for prod_id in prod_ids {\n         \
    \                                   let new_actions = action_stack.clone().push(Action::Reduce\
    \ { production_id: *prod_id, len: *len, nonterminal_id: *nt_id });\n         \
    \                                   self.active_states.push(ParseState {\n   \
    \                                             stack: new_stack.clone(),\n    \
    \                                            action_stack: Some(Arc::new(new_actions)),\n\
    \                                                status: ParseStatus::Active,\n\
    \                                            });\n                           \
    \             }\n                                    } else {\n              \
    \                           inactive_states.push(ParseState {\n              \
    \                              stack: stack_node.clone(),\n                  \
    \                          action_stack: action_stack.clone(),\n             \
    \                               status: ParseStatus::Inactive(StopReason::GotoNotFound),\n\
    \                                        });\n                               \
    \     }\n                                }\n                             }\n \
    \                        }\n                     }\n                }\n      \
    \      } else {\n                inactive_states.push(ParseState {\n         \
    \           stack,\n                    action_stack,\n                    status:\
    \ ParseStatus::Inactive(StopReason::ActionNotFound),\n                });\n  \
    \          }\n        }\n        self.active_states = next_active_states;\n  \
    \      self.inactive_states.insert(self.input_pos, inactive_states);\n       \
    \ if token_id != self.parser.eof_terminal_id {\n            self.input_pos +=\
    \ 1;\n        }\n    }\n\n    pub fn merge_active_states(&mut self) {\n      \
    \   let mut active_state_map: BTreeMap<ParseStateKey, ParseState> = BTreeMap::new();\n\
    \n         let mut new_active_states = Vec::new();\n\n        for mut state in\
    \ std::mem::take(&mut self.active_states) {\n            let key = state.key();\n\
    \            if let Some(existing) = active_state_map.get_mut(&key) {\n      \
    \          Arc::make_mut(&mut existing.stack).merge(state.stack.as_ref().clone());\n\
    \                if let Some(existing_action_stack) = existing.action_stack.as_mut()\
    \ {\n                    Arc::make_mut(existing_action_stack).merge(state.action_stack.unwrap().as_ref().clone());\n\
    \                }\n            } else {\n                active_state_map.insert(key,\
    \ state.clone());\n                 new_active_states.push(state);\n         \
    \   }\n        }\n        self.active_states = new_active_states;\n    }\n\n \
    \   pub fn fully_matches(&self) -> bool {\n         !self.fully_matching_states().is_empty()\n\
    \    }\n\n    pub fn fully_matching_states(&self) -> Vec<&ParseState> {\n    \
    \    self.inactive_states.get(&self.input_pos).map_or(vec![], |states| {\n   \
    \         states.iter().filter(|state| state.status == ParseStatus::Inactive(StopReason::GotoNotFound)).collect()\n\
    \        })\n    }\n\n    pub fn is_ok(&self) -> bool {\n        !self.active_states.is_empty()\
    \ || self.fully_matches()\n    }\n}\n\n\n#[derive(Debug, Clone, PartialEq, Eq,\
    \ PartialOrd, Ord)]\npub struct ParseStateKey {\n    stack: StateID,\n    action_stack:\
    \ Option<Action>,\n}\n\nimpl ParseState {\n    pub fn key(&self) -> ParseStateKey\
    \ {\n         ParseStateKey {\n            stack: *self.stack.peek(),\n      \
    \      action_stack: self.action_stack.peek().cloned(),\n        }\n    }\n\n\
    \    pub fn merge(&mut self, other: ParseState) {\n         assert_eq!(self.key(),\
    \ other.key());\n        Arc::make_mut(&mut self.stack).merge(Arc::unwrap_or_clone(other.stack));\n\
    \        match (&mut self.action_stack, other.action_stack) {\n            (Some(a),\
    \ Some(b)) => {\n                Arc::make_mut(a).merge(Arc::unwrap_or_clone(b));\n\
    \            }\n            (None, None) => {}\n            _ => unreachable!(),\n\
    \        }\n    }\n}\n\npub trait InsertWith<K, V> {\n    fn insert_with<F: FnOnce(&mut\
    \ V, V)>(&mut self, k: K, v: V, combine: F);\n}\n\nimpl<K, V> InsertWith<K, V>\
    \ for BTreeMap<K, V> where K: Eq + Ord {\n    fn insert_with<F: FnOnce(&mut V,\
    \ V)>(&mut self, k: K, v: V, combine: F) {\n         match self.entry(k) {\n \
    \           std::collections::btree_map::Entry::Occupied(mut occupied) => {\n\
    \                let value = occupied.get_mut();\n                combine(value,\
    \ v);\n            }\n            std::collections::btree_map::Entry::Vacant(vacant)\
    \ => {\n                vacant.insert(v);\n            }\n        }\n    }\n}\n\
    \n```\n\n```rust id=\"c9b36d82\"\nuse crate::finite_automata::{GroupID, Regex};\n\
    use crate::glr;\nuse crate::glr::table::StateID;\nuse std::collections::{BTreeMap,\
    \ BTreeSet};\nuse std::sync::{Arc, Mutex};\nuse bitvec::prelude::BitVec;\nuse\
    \ kdam::tqdm;\nuse crate::trie::{TrieNode};\nuse bimap::BiBTreeMap;\nuse bitvec::bitvec;\n\
    use crate::debug;\n\npub type TokenID = usize;\n\n#[derive(Debug, Clone, Copy,\
    \ PartialEq, Eq, PartialOrd, Ord, Hash)]\npub struct LLMTokenID(pub usize);\n\n\
    #[derive(Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]\npub struct Token {\n\
    \    pub id: GroupID,\n    pub width: usize,\n}\n\npub struct ExecuteResult {\n\
    \    pub matches: Vec<Token>,\n    pub new_state: Option<usize>,\n}\n\npub trait\
    \ Tokenizer: Sized {\n    fn initial_state_id(&self) -> usize;\n    fn execute_from_state(&self,\
    \ text: &[u8], state: usize) -> ExecuteResult;\n    fn tokens_accessible_from_state(&self,\
    \ state: usize) -> Vec<TokenID>;\n    fn max_state(&self) -> usize;\n\n    fn\
    \ execute_all_from_state(\n        &self,\n        text: &[u8],\n        state:\
    \ usize,\n        state_map_root_arc: Arc<Mutex<TrieNode<GroupID, (BTreeMap<LLMTokenID,\
    \ Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>>,\n        llm_token_id:\
    \ LLMTokenID,\n        max_llm_token_id: usize,\n    ) {\n        let mut queue:\
    \ BTreeMap<(usize, Option<usize>), BTreeMap<_, _>> = BTreeMap::new();\n      \
    \  let mut queue_positions: BTreeMap<*const Mutex<TrieNode<GroupID, (BTreeMap<LLMTokenID,\
    \ Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>, (usize, Option<usize>)>\
    \ = BTreeMap::new();\n        let mut new_nodes_for_positions: BTreeMap<(usize,\
    \ Option<usize>), Arc<Mutex<TrieNode<GroupID, (BTreeMap<LLMTokenID, Option<StateID>>,\
    \ BTreeMap<TokenID, BitVec>, Option<BitVec>)>>>> = BTreeMap::new();\n\n      \
    \  let root = state_map_root_arc.clone();\n\n        queue.insert((0, Some(state)),\
    \ BTreeMap::from([(Arc::as_ptr(&root), root.clone())]));\n        queue_positions.insert(Arc::as_ptr(&root),\
    \ (0, Some(state)));\n\n        while let Some(((position, maybe_state), nodes))\
    \ = queue.pop_first() {\n            for (_, node) in nodes {\n              \
    \   if position == text.len() {\n                    let mut node_guard = node.try_lock().unwrap();\n\
    \                    assert!(!node_guard.value.0.contains_key(&llm_token_id));\n\
    \                    node_guard.value.0.insert(llm_token_id, maybe_state.map(StateID));\n\
    \                    if let Some(state) = maybe_state {\n                    \
    \    for possible_grammar_token_id in &self.tokens_accessible_from_state(state)\
    \ {\n                            node_guard.value.1.entry(*possible_grammar_token_id).or_insert_with(||\
    \ {\n                                let mut bitset = BitVec::new();\n       \
    \                         bitset.resize(max_llm_token_id + 1, false);\n      \
    \                          bitset\n                            }).set(llm_token_id.0,\
    \ true);\n                        }\n                    } else {\n          \
    \               node_guard.value.2.get_or_insert_with(|| {\n                 \
    \           let mut bitset = BitVec::new();\n                            bitset.resize(max_llm_token_id\
    \ + 1, false);\n                            bitset\n                        }).set(llm_token_id.0,\
    \ true);\n                    }\n                    continue;\n             \
    \   }\n\n                 let remaining_text = &text[position..];\n          \
    \      let execute_result = self.execute_from_state(remaining_text, maybe_state.unwrap_or(0));\n\
    \                for token in &execute_result.matches {\n                    let\
    \ new_position = position + token.width;\n                    assert_ne!(token.width,\
    \ 0);\n                    assert!(new_position <= text.len());\n            \
    \        let new_state: Option<usize> = None;\n                     let mut node_guard\
    \ = node.try_lock().unwrap();\n                    if let Some(child) = node_guard.get(&token.id)\
    \ {\n                         let child_ptr = Arc::as_ptr(&child);\n         \
    \                if let Some(&(child_position, child_state)) = queue_positions.get(&child_ptr)\
    \ {\n                            if (child_position, child_state) != (new_position,\
    \ new_state) {\n                                let new_child = node_guard.replace_child_with_clone(&token.id);\n\
    \                                queue_positions.insert(Arc::as_ptr(&new_child),\
    \ (new_position, new_state));\n                                queue.entry((new_position,\
    \ new_state)).or_default().insert(Arc::as_ptr(&new_child), new_child.clone());\n\
    \                            }\n                         } else {\n          \
    \                   queue_positions.insert(child_ptr, (new_position, new_state));\n\
    \                            queue.entry((new_position, new_state)).or_default().insert(child_ptr,\
    \ child.clone());\n                         }\n                    } else {\n\
    \                        if let Some(new_node) = new_nodes_for_positions.get(&(new_position,\
    \ new_state)) {\n                           node_guard.insert(token.id, new_node.clone());\n\
    \                        } else {\n                             let new_node =\
    \ Arc::new(Mutex::new(TrieNode::new((BTreeMap::new(), BTreeMap::new(), None))));\n\
    \                             new_nodes_for_positions.insert((new_position, new_state),\
    \ new_node.clone());\n                            node_guard.insert(token.id,\
    \ new_node.clone());\n                            queue_positions.insert(Arc::as_ptr(&new_node),\
    \ (new_position, new_state));\n                             queue.entry((new_position,\
    \ new_state)).or_default().insert(Arc::as_ptr(&new_node), new_node.clone());\n\
    \                        }\n                    }\n                }\n       \
    \     }\n        }\n    }\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd,\
    \ Ord)]\npub struct TokenizerStateInfoForLLMToken {\n    pub tokenizer_state_id:\
    \ usize,\n    pub position_in_llm_token: usize,\n    pub dirty_end_state: Option<StateID>,\n\
    \    pub clean_end: bool,\n}\n\npub fn precompute<'a>(\n    tokenizer: &impl Tokenizer,\n\
    \    llm_token_map: &BiBTreeMap<Vec<u8>, LLMTokenID>,\n    eof_llm_token_id: LLMTokenID,\n\
    \    max_llm_token_id: usize,\n) -> BTreeMap<StateID, TrieNode<TokenID, (BTreeMap<LLMTokenID,\
    \ Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>> {\n    let mut\
    \ result: BTreeMap<StateID, TrieNode<GroupID, _>> = BTreeMap::new();\n\n     debug!(2,\
    \ \"Ensuring tokenizer doesn't match on empty strings\");\n    let execute_result\
    \ = tokenizer.execute_from_state(&[], 0);\n    if !execute_result.matches.is_empty()\
    \ {\n        panic!(\"Tokenizer should not match on empty string. If it did, there\
    \ would be infinitely many possible token sequences for any LLM token.\");\n \
    \   }\n\n     debug!(2, \"Precomputing in precompute\");\n    for state_id in\
    \ tqdm!(0..tokenizer.max_state()) {\n        let mut state_map_root_arc: Arc<Mutex<TrieNode<GroupID,\
    \ (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>>\
    \ = Arc::new(Mutex::new(TrieNode::new((BTreeMap::new(), BTreeMap::new(), None))));\n\
    \n        for (i, (llm_token, llm_token_id)) in llm_token_map.iter().enumerate()\
    \ {\n             debug!(1, \"Precomputing for token {:?} ({:?}) ({})\", llm_token_id,\
    \ llm_token, i);\n            tokenizer.execute_all_from_state(\n            \
    \    llm_token,\n                state_id,\n                state_map_root_arc.clone(),\n\
    \                *llm_token_id,\n                max_llm_token_id,\n         \
    \   );\n        }\n\n         let state_map_root = state_map_root_arc.try_lock().unwrap().clone();\n\
    \        result.insert(glr::table::StateID(state_id), state_map_root);\n    }\n\
    \n    result\n}\n\npub(crate) fn precompute_add_eof(\n    precomputed: &mut BTreeMap<StateID,\
    \ TrieNode<TokenID, (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID,\
    \ BitVec>, Option<BitVec>)>>,\n    eof_llm_token_id: LLMTokenID,\n    eof_grammar_token_id:\
    \ TokenID,\n    max_llm_token_id: usize,\n) {\n     let mut bitset = bitvec![0;\
    \ max_llm_token_id + 1];\n    bitset.set(eof_llm_token_id.0, true);\n    let node\
    \ = precomputed.get_mut(&StateID(0)).expect(\"State 0 should exist\");\n    node.value.1.insert(eof_grammar_token_id,\
    \ bitset);\n}\n\n\nimpl Tokenizer for Regex {\n    fn initial_state_id(&self)\
    \ -> usize {\n        0\n    }\n\n    fn execute_from_state(&self, text: &[u8],\
    \ state: usize) -> ExecuteResult {\n        let mut regex_state = self.init_to_state(state);\n\
    \        regex_state.execute(text);\n         let matches: Vec<_> = regex_state.matches.iter().map(|(&id,\
    \ &width)| Token { id, width })\n            .filter(|token| token.width != 0).collect();\n\
    \n        ExecuteResult {\n            matches,\n            new_state: if regex_state.done\
    \ { None } else { Some(regex_state.current_state) },\n        }\n    }\n\n   \
    \  fn tokens_accessible_from_state(&self, state: usize) -> Vec<TokenID> {\n  \
    \      let regex_state = self.init_to_state(state);\n        regex_state.possible_group_ids().iter().cloned().collect()\n\
    \    }\n\n\n    fn max_state(&self) -> usize {\n        self.dfa.states.len()\n\
    \    }\n}\n\npub fn print_precomputed(precomputed: &BTreeMap<StateID, TrieNode<TokenID,\
    \ (BTreeMap<LLMTokenID, Option<StateID>>, BTreeMap<TokenID, BitVec>, Option<BitVec>)>>)\
    \ {\n    println!(\"Precomputed:\");\n    for (tokenizer_state, root) in precomputed\
    \ {\n        println!(\"  Tokenizer state: {}\", tokenizer_state.0);\n       \
    \ for node in TrieNode::all_nodes(Arc::new(Mutex::new(root.clone()))) {\n    \
    \        println!(\"    Node address: {:p}, value: {:?}\", Arc::as_ptr(&node),\
    \ node.try_lock().unwrap().value);\n           for (edge, dest) in node.try_lock().unwrap().children()\
    \ {\n                println!(\"      Edge value: {:?}, destination address: {:p}\"\
    , edge, Arc::as_ptr(&dest));\n            }\n        }\n    }\n}\n\n#[cfg(test)]\n\
    mod tests {\n    use super::*;\n    use crate::charmap::TrieMap;\n    use crate::finite_automata::{eat_u8,\
    \ DFAState, Regex, DFA};\n    use crate::u8set::U8Set;\n    use crate::{groups,\
    \ seq};\n    use std::collections::{BTreeMap, BTreeSet};\n     use bimap::BiBTreeMap;\n\
    \n     #[test]\n    fn test_precompute() {\n        let _tokenizer = groups![\n\
    \            eat_u8(b'a'),\n            eat_u8(b'b'),\n             seq![eat_u8(b'a'),\
    \ eat_u8(b'b')],\n            seq![eat_u8(b'a'), eat_u8(b'b'), eat_u8(b'c')],\n\
    \        ].build();\n\n         let tokenizer = Regex {\n            dfa: DFA\
    \ {\n                states: vec![\n                    DFAState {\n         \
    \               transitions: TrieMap::from_iter(vec![(b'a', 1), (b'b', 2)]),\n\
    \                        finalizers: BTreeSet::new(),\n                      \
    \  possible_group_ids: BTreeSet::from([0, 1, 2, 3]),\n                       \
    \ group_id_to_u8set: BTreeMap::from([\n                            (0, U8Set::from_bytes(b\"\
    a\")),\n                            (1, U8Set::from_bytes(b\"b\")),\n        \
    \                    (2, U8Set::from_bytes(b\"a\")),\n                       \
    \     (3, U8Set::from_bytes(b\"a\")),\n                        ]),\n         \
    \           },\n                    DFAState {\n                        transitions:\
    \ TrieMap::from_iter(vec![(b'b', 3)]),\n                        finalizers: BTreeSet::from([0]),\n\
    \                        possible_group_ids: BTreeSet::from([0, 2, 3]),\n    \
    \                    group_id_to_u8set: BTreeMap::from([\n                   \
    \         (2, U8Set::from_bytes(b\"b\")),\n                             (3, U8Set::from_bytes(b\"\
    b\")),\n                        ]),\n                    },\n                \
    \    DFAState {\n                        transitions: TrieMap::new(),\n      \
    \                  finalizers: BTreeSet::from([1]),\n                        possible_group_ids:\
    \ BTreeSet::from([1]),\n                         group_id_to_u8set: BTreeMap::new(),\n\
    \                    },\n                    DFAState {\n                    \
    \     transitions: TrieMap::from_iter(vec![(b'c', 4)]),\n                    \
    \    finalizers: BTreeSet::from([2]),\n                        possible_group_ids:\
    \ BTreeSet::from([2, 3]),\n                         group_id_to_u8set: BTreeMap::from([(3,\
    \ U8Set::from_bytes(b\"c\"))]),\n                    },\n                    DFAState\
    \ {\n                        transitions: TrieMap::new(),\n                  \
    \      finalizers: BTreeSet::from([3]),\n                        possible_group_ids:\
    \ BTreeSet::from([3]),\n                        group_id_to_u8set: BTreeMap::new(),\n\
    \                    },\n                ],\n                start_state: 0,\n\
    \                non_greedy_finalizers: BTreeSet::new(),\n            },\n   \
    \     };\n        assert_eq!(_tokenizer, tokenizer);\n\n         let llm_tokens:\
    \ &[&[u8]] = &[b\"a\", b\"b\", b\"c\", b\"ab\", b\"bc\", b\"abc\"];\n        \
    \ let llm_token_map: BiBTreeMap<Vec<u8>, LLMTokenID> = llm_tokens.iter().enumerate().map(|(i,\
    \ token)| (token.to_vec(), LLMTokenID(i))).collect();\n\n        let max_llm_token_id\
    \ = llm_tokens.len() + 1;\n        let result = precompute(&tokenizer, &llm_token_map,\
    \ LLMTokenID(max_llm_token_id), max_llm_token_id);\n    }\n}\n```\n\n```rust id=\"\
    d0ec7314\"\nuse std::collections::BTreeMap;\n\n#[derive(Debug, Clone)]\npub(crate)\
    \ enum PrecomputeGSSNode<GrammarToken, Leaf> {\n    Internal(BTreeMap<GrammarToken,\
    \ PrecomputeGSSNode<GrammarToken, Leaf>>),\n    Leaf(Leaf),\n}\n\nimpl<GrammarToken,\
    \ Leaf> PrecomputeGSSNode<GrammarToken, Leaf>\nwhere\n    GrammarToken: Clone\
    \ + Ord,\n    Leaf: Clone,\n{\n    pub(crate) fn flatten(&self) -> BTreeMap<Vec<GrammarToken>,\
    \ Leaf> {\n        let mut result = BTreeMap::new();\n        self.flatten_recursive(&mut\
    \ result, Vec::new());\n        result\n    }\n\n    fn flatten_recursive(\n \
    \       &self,\n        result: &mut BTreeMap<Vec<GrammarToken>, Leaf>,\n    \
    \    path: Vec<GrammarToken>,\n    ) {\n         match self {\n            PrecomputeGSSNode::Internal(children)\
    \ => {\n                for (token, child) in children {\n                   \
    \ let mut new_path = path.clone();\n                    new_path.push(token.clone());\n\
    \                    child.flatten_recursive(result, new_path);\n            \
    \    }\n            }\n            PrecomputeGSSNode::Leaf(leaf) => {\n      \
    \          result.insert(path, leaf.clone());\n            }\n        }\n    }\n\
    \    \n    pub(crate) fn map<F, U>(&self, f: F) -> PrecomputeGSSNode<GrammarToken,\
    \ U>\n    where\n        F: Copy + Fn(&Leaf) -> U,\n    {\n        match self\
    \ {\n            PrecomputeGSSNode::Internal(children) => {\n                let\
    \ mapped_children = children\n                    .iter()\n                  \
    \  .map(|(token, child)| (token.clone(), child.map(f)))\n                    .collect();\n\
    \                PrecomputeGSSNode::Internal(mapped_children)\n            }\n\
    \            PrecomputeGSSNode::Leaf(leaf) => {\n                PrecomputeGSSNode::Leaf(f(leaf))\n\
    \            }\n        }\n    }\n}\n```\n\n```rust id=\"9ab8b5f8\"\nuse super::items::{compute_closure,\
    \ compute_goto, split_on_dot, Item};\nuse crate::glr::grammar::{compute_first_sets,\
    \ compute_follow_sets, NonTerminal, Production, Symbol, Terminal};\nuse crate::glr::parser::GLRParser;\n\
    use bimap::BiBTreeMap;\nuse std::collections::{HashMap, VecDeque};\nuse std::collections::{BTreeMap,\
    \ BTreeSet};\nuse std::fmt::Display;\nuse crate::analyze_grammar::{drop_dead,\
    \ validate};\n\ntype Stage1Table = BTreeMap<BTreeSet<Item>, Stage1Row>;\ntype\
    \ Stage2Table = BTreeMap<BTreeSet<Item>, Stage2Row>;\ntype Stage3Table = BTreeMap<BTreeSet<Item>,\
    \ Stage3Row>;\ntype Stage4Table = BTreeMap<BTreeSet<Item>, Stage4Row>;\ntype Stage5Table\
    \ = BTreeMap<BTreeSet<Item>, Stage5Row>;\ntype Stage6Table = BTreeMap<BTreeSet<Item>,\
    \ Stage6Row>;\npub type Stage7Table = BTreeMap<StateID, Stage7Row>;\n\ntype Stage1Row\
    \ = BTreeMap<Option<Symbol>, BTreeSet<Item>>;\n\n#[derive(Debug)]\nstruct Stage2Row\
    \ {\n    shifts: BTreeMap<Terminal, BTreeSet<Item>>,\n    gotos: BTreeMap<NonTerminal,\
    \ BTreeSet<Item>>,\n    reduces: BTreeSet<Item>,\n}\n\n#[derive(Debug)]\nstruct\
    \ Stage3Row {\n    shifts: BTreeMap<Terminal, BTreeSet<Item>>,\n    gotos: BTreeMap<NonTerminal,\
    \ BTreeSet<Item>>,\n    reduces: BTreeMap<Terminal, BTreeSet<Item>>,\n}\n\n#[derive(Debug)]\n\
    struct Stage4Row {\n    shifts: BTreeMap<Terminal, BTreeSet<Item>>,\n    gotos:\
    \ BTreeMap<NonTerminal, BTreeSet<Item>>,\n    reduces: BTreeMap<Terminal, BTreeSet<ProductionID>>,\n\
    }\n\ntype Stage5Row = Stage4Row;\n\n#[derive(Debug)]\nstruct Stage6Row {\n   \
    \ shifts_and_reduces: BTreeMap<Terminal, Stage6ShiftsAndReduces>,\n    gotos:\
    \ BTreeMap<NonTerminal, BTreeSet<Item>>,\n}\n\n#[derive(Debug)]\nenum Stage6ShiftsAndReduces\
    \ {\n    Shift(BTreeSet<Item>),\n    Reduce(ProductionID),\n    Split {\n    \
    \    shift: Option<BTreeSet<Item>>,\n        reduces: BTreeSet<ProductionID>,\n\
    \    },\n}\n\n#[derive(Debug, Clone)]\npub enum Stage7ShiftsAndReduces {\n   \
    \ Shift(StateID),\n    Reduce { production_id: ProductionID, nonterminal_id: NonTerminalID,\
    \ len: usize },\n    Split {\n        shift: Option<StateID>,\n        reduces:\
    \ BTreeMap<usize, BTreeMap<NonTerminalID, BTreeSet<ProductionID>>>,\n    },\n\
    }\n\n#[derive(Debug, Clone)]\npub struct Stage7Row {\n    pub shifts_and_reduces:\
    \ BTreeMap<TerminalID, Stage7ShiftsAndReduces>,\n    pub gotos: BTreeMap<NonTerminalID,\
    \ StateID>,\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\n\
    pub struct StateID(pub usize);\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq,\
    \ PartialOrd, Ord)]\npub struct ProductionID(pub usize);\n\n#[derive(Debug, Clone,\
    \ Copy, PartialEq, Eq, PartialOrd, Ord)]\npub struct NonTerminalID(pub usize);\n\
    \n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub struct TerminalID(pub\
    \ usize);\n\ntype Stage1Result = Stage1Table;\ntype Stage2Result = Stage2Table;\n\
    type Stage3Result = Stage3Table;\ntype Stage4Result = Stage4Table;\ntype Stage5Result\
    \ = Stage5Table;\ntype Stage6Result = Stage6Table;\ntype Stage7Result = (\n  \
    \  Stage7Table,\n    BiBTreeMap<BTreeSet<Item>, StateID>,\n    StateID,\n    TerminalID,\n\
    );\n\nfn stage_1(productions: &[Production], start_production_id: usize) -> Stage1Result\
    \ {\n    let initial_item = Item {\n        production: productions[start_production_id].clone(),\n\
    \        dot_position: 0,\n    };\n    let initial_closure = BTreeSet::from([initial_item]);\n\
    \    let mut worklist = VecDeque::from([initial_closure.clone()]);\n\n    let\
    \ mut transitions: BTreeMap<BTreeSet<Item>, BTreeMap<Option<Symbol>, BTreeSet<Item>>>\
    \ = BTreeMap::new();\n\n    while let Some(items) = worklist.pop_front() {\n \
    \       if transitions.contains_key(&items) {\n            continue;\n       \
    \ }\n\n        let closure = compute_closure(&items, productions);\n        let\
    \ splits = split_on_dot(&closure);\n        let mut row = BTreeMap::new();\n\n\
    \        for (symbol, items) in splits {\n            if symbol.is_none() {\n\
    \                continue;\n            }\n            let goto_set = compute_goto(&items);\n\
    \            row.insert(symbol.clone(), goto_set.clone());\n            worklist.push_back(goto_set);\n\
    \        }\n\n        transitions.insert(items.clone(), row);\n    }\n\n    transitions\n\
    }\n\nfn stage_2(stage_1_table: Stage1Table, productions: &[Production]) -> Stage2Result\
    \ {\n     let mut stage_2_table = BTreeMap::new();\n    for (item_set, transitions)\
    \ in stage_1_table {\n        let mut shifts = BTreeMap::new();\n        let mut\
    \ gotos = BTreeMap::new();\n        let mut reduces = BTreeSet::new();\n\n   \
    \     for item in &item_set {\n            if item.dot_position >= item.production.rhs.len()\
    \ {\n                reduces.insert(item.clone());\n            }\n        }\n\
    \n        for (symbol_opt, next_item_set) in &transitions {\n            if let\
    \ Some(symbol) = symbol_opt {\n                match symbol {\n              \
    \      Symbol::Terminal(t) => {\n                         shifts.insert(t.clone(),\
    \ next_item_set.clone());\n                    }\n                    Symbol::NonTerminal(nt)\
    \ => {\n                         gotos.insert(nt.clone(), next_item_set.clone());\n\
    \                    }\n                }\n            }\n        }\n\n      \
    \  stage_2_table.insert(\n            item_set,\n            Stage2Row {\n   \
    \             shifts,\n                gotos,\n                reduces,\n    \
    \        },\n        );\n    }\n    stage_2_table\n}\n\nfn stage_3(stage_2_table:\
    \ Stage2Table, productions: &[Production]) -> Stage3Result {\n    let follow_sets\
    \ = compute_follow_sets(productions);\n\n    let mut stage_3_table = BTreeMap::new();\n\
    \n    for (item_set, row) in stage_2_table {\n        let mut reduces: BTreeMap<Terminal,\
    \ BTreeSet<Item>> = BTreeMap::new();\n\n         for item in &row.reduces {\n\
    \            let lhs = &item.production.lhs;\n            let lookaheads = follow_sets.get(lhs).unwrap();\n\
    \n            for terminal in lookaheads {\n                 reduces\n       \
    \             .entry(terminal.clone())\n                    .or_default()\n  \
    \                  .insert(item.clone());\n            }\n        }\n\n      \
    \   stage_3_table.insert(\n            item_set,\n            Stage3Row {\n  \
    \              shifts: row.shifts,\n                gotos: row.gotos,\n      \
    \          reduces,\n            },\n        );\n    }\n\n    stage_3_table\n\
    }\n\nfn stage_4(stage_3_table: Stage3Table, productions: &[Production]) -> Stage4Result\
    \ {\n     let production_ids: BTreeMap<Production, ProductionID> = productions\n\
    \        .iter()\n        .enumerate()\n        .map(|(i, p)| (p.clone(), ProductionID(i)))\n\
    \        .collect();\n\n    let mut stage_4_table = BTreeMap::new();\n\n    for\
    \ (item_set, row) in stage_3_table {\n        let mut reduces = BTreeMap::new();\n\
    \n        for (terminal, items) in row.reduces {\n             let mut prod_ids\
    \ = BTreeSet::new();\n             for item in items {\n                 let prod_id\
    \ = production_ids.get(&item.production).unwrap();\n                prod_ids.insert(*prod_id);\n\
    \            }\n             reduces.insert(terminal.clone(), prod_ids);\n   \
    \     }\n\n        stage_4_table.insert(\n            item_set,\n            Stage4Row\
    \ {\n                shifts: row.shifts,\n                gotos: row.gotos,\n\
    \                 reduces,\n            },\n        );\n    }\n\n    stage_4_table\n\
    }\n\nfn stage_5(stage_4_table: Stage4Table, productions: &[Production]) -> Stage5Result\
    \ {\n    stage_4_table\n}\n\nfn stage_6(stage_5_table: Stage5Table) -> Stage6Result\
    \ {\n    let mut stage_6_table = BTreeMap::new();\n\n     for (item_set, row)\
    \ in stage_5_table {\n        let mut shifts_and_reduces = BTreeMap::new();\n\n\
    \        for (terminal, next_item_set) in row.shifts {\n            shifts_and_reduces.insert(terminal,\
    \ Stage6ShiftsAndReduces::Shift(next_item_set));\n        }\n\n        for (terminal,\
    \ mut production_ids) in row.reduces {\n            if let Some(mut existing)\
    \ = shifts_and_reduces.remove(&terminal) {\n                match existing {\n\
    \                    Stage6ShiftsAndReduces::Shift(shift_set) => {\n         \
    \               shifts_and_reduces.insert(terminal, Stage6ShiftsAndReduces::Split\
    \ {\n                            shift: Some(shift_set.clone()),\n           \
    \                 reduces: production_ids.clone(),\n                        });\n\
    \                    }\n                    Stage6ShiftsAndReduces::Reduce(existing_production_id)\
    \ => {\n                        production_ids.insert(existing_production_id);\n\
    \                         shifts_and_reduces.insert(terminal, Stage6ShiftsAndReduces::Split\
    \ {\n                            shift: None,\n                            reduces:\
    \ production_ids,\n                        });\n                    }\n      \
    \              Stage6ShiftsAndReduces::Split { shift, mut reduces } => {\n   \
    \                     reduces.extend(production_ids.into_iter());\n          \
    \               shifts_and_reduces.insert(terminal, Stage6ShiftsAndReduces::Split\
    \ { shift, reduces });\n                    }\n                }\n           \
    \ } else {\n                if production_ids.len() == 1 {\n                 \
    \   shifts_and_reduces.insert(terminal, Stage6ShiftsAndReduces::Reduce(production_ids.iter().next().unwrap().clone()));\n\
    \                } else {\n                    shifts_and_reduces.insert(terminal,\
    \ Stage6ShiftsAndReduces::Split { shift: None, reduces: production_ids });\n \
    \               }\n            }\n        }\n\n        stage_6_table.insert(\n\
    \            item_set,\n            Stage6Row {\n                 shifts_and_reduces,\n\
    \                gotos: row.gotos,\n            },\n        );\n    }\n\n    stage_6_table\n\
    }\n\nfn stage_7(stage_6_table: Stage6Table, productions: &[Production], start_production_id:\
    \ usize, terminal_map: &BiBTreeMap<Terminal, TerminalID>, non_terminal_map: &BiBTreeMap<NonTerminal,\
    \ NonTerminalID>) -> Stage7Result {\n    let mut item_set_map = BiBTreeMap::new();\n\
    \    let mut next_state_id = 0;\n\n    for (item_set, _) in &stage_6_table {\n\
    \        item_set_map.insert(item_set.clone(), StateID(next_state_id));\n    \
    \    next_state_id += 1;\n    }\n\n    let mut stage_7_table = BTreeMap::new();\n\
    \n    for (item_set, row) in stage_6_table {\n         let state_id = *item_set_map.get_by_left(&item_set).unwrap();\n\
    \        let mut shifts_and_reduces = BTreeMap::new();\n        let mut gotos\
    \ = BTreeMap::new();\n\n        for (terminal, action) in row.shifts_and_reduces\
    \ {\n             let terminal_id = *terminal_map.get_by_left(&terminal).expect(format!(\"\
    {:?} not found in terminal map {:?}\", terminal, terminal_map.left_values().map(|t|\
    \ t.0.clone()).collect::<Vec<String>>()).as_str());\n             let converted_action\
    \ = match action {\n                Stage6ShiftsAndReduces::Shift(next_item_set)\
    \ => {\n                     let next_state_id = *item_set_map.get_by_left(&next_item_set).unwrap();\n\
    \                    Stage7ShiftsAndReduces::Shift(next_state_id)\n          \
    \      }\n                Stage6ShiftsAndReduces::Reduce(production_id) => {\n\
    \                     let production = productions.get(production_id.0).unwrap();\n\
    \                    let nonterminal_id = *non_terminal_map.get_by_left(&production.lhs).unwrap();\n\
    \                     let len = production.rhs.len();\n                    Stage7ShiftsAndReduces::Reduce\
    \ { production_id, nonterminal_id, len }\n                }\n                \
    \ Stage6ShiftsAndReduces::Split { shift, reduces } => {\n                    \
    \ let shift_state_id = shift.as_ref().map(|set| *item_set_map.get_by_left(set).unwrap());\n\
    \                     let mut len_to_nt_to_production_id: BTreeMap<usize, BTreeMap<NonTerminalID,\
    \ BTreeSet<ProductionID>>> = BTreeMap::new();\n                     for production_id\
    \ in reduces {\n                        let production = productions.get(production_id.0).unwrap();\n\
    \                         let nonterminal_id = *non_terminal_map.get_by_left(&production.lhs).unwrap();\n\
    \                        let len = production.rhs.len();\n                   \
    \      len_to_nt_to_production_id.entry(len).or_default().entry(nonterminal_id).or_default().insert(production_id);\n\
    \                    }\n                    Stage7ShiftsAndReduces::Split { shift:\
    \ shift_state_id, reduces: len_to_nt_to_production_id }\n                }\n \
    \           };\n             shifts_and_reduces.insert(terminal_id, converted_action);\n\
    \        }\n\n        for (nonterminal, next_item_set) in row.gotos {\n      \
    \      let non_terminal_id = *non_terminal_map.get_by_left(&nonterminal).unwrap();\n\
    \            let next_state_id = *item_set_map.get_by_left(&next_item_set).unwrap();\n\
    \            gotos.insert(non_terminal_id, next_state_id);\n        }\n\n    \
    \    stage_7_table.insert(state_id, Stage7Row { shifts_and_reduces, gotos });\n\
    \    }\n\n    let start_item = Item {\n        production: productions[start_production_id].clone(),\n\
    \        dot_position: 0,\n    };\n    let start_state_id = *item_set_map.get_by_left(&BTreeSet::from([start_item])).unwrap();\n\
    \     let eof_terminal_id = *terminal_map.get_by_left(&Terminal(\"$\".to_string())).unwrap();\n\
    \n     (stage_7_table, item_set_map, start_state_id, eof_terminal_id)\n}\n\npub\
    \ fn generate_glr_parser_with_maps(productions: &[Production], start_production_id:\
    \ usize, mut terminal_map: BiBTreeMap<Terminal, TerminalID>, non_terminal_map:\
    \ BiBTreeMap<NonTerminal, NonTerminalID>) -> GLRParser {\n    crate::debug!(2,\
    \ \"Validating\");\n    validate(productions).expect(\"Validation error\");\n\n\
    \    assign_eof_terminal_id(&mut terminal_map);\n\n     crate::debug!(2, \"Stage\
    \ 1\");\n    let stage_1_table = stage_1(productions, start_production_id);\n\
    \    crate::debug!(2, \"Stage 2\");\n    let stage_2_table = stage_2(stage_1_table,\
    \ productions);\n    crate::debug!(2, \"Stage 3\");\n     let stage_3_table =\
    \ stage_3(stage_2_table, productions);\n    crate::debug!(2, \"Stage 4\");\n \
    \    let stage_4_table = stage_4(stage_3_table, productions);\n    crate::debug!(2,\
    \ \"Stage 5\");\n     let stage_5_table = stage_5(stage_4_table, productions);\n\
    \    crate::debug!(2, \"Stage 6\");\n    let stage_6_table = stage_6(stage_5_table);\n\
    \    crate::debug!(2, \"Stage 7\");\n     let (stage_7_table, item_set_map, start_state_id,\
    \ eof_terminal_id) = stage_7(stage_6_table, productions, start_production_id,\
    \ &terminal_map, &non_terminal_map);\n     crate::debug!(2, \"Stage 8\");\n\n\
    \     GLRParser::new(stage_7_table, productions.to_vec(), terminal_map, non_terminal_map,\
    \ item_set_map, start_state_id, eof_terminal_id)\n}\n\npub fn generate_glr_parser(productions:\
    \ &[Production], start_production_id: usize) -> GLRParser {\n    let terminal_map\
    \ = assign_terminal_ids(productions);\n     let non_terminal_map = assign_non_terminal_ids(productions);\n\
    \    generate_glr_parser_with_maps(productions, start_production_id, terminal_map,\
    \ non_terminal_map)\n}\n\npub fn assign_terminal_ids(productions: &[Production])\
    \ -> BiBTreeMap<Terminal, TerminalID> {\n     let mut terminal_map = BiBTreeMap::new();\n\
    \    let mut next_terminal_id = 0;\n\n     for p in productions {\n        for\
    \ symbol in &p.rhs {\n            if let Symbol::Terminal(t) = symbol {\n    \
    \            if !terminal_map.contains_left(t) {\n                    terminal_map.insert(t.clone(),\
    \ TerminalID(next_terminal_id));\n                     next_terminal_id += 1;\n\
    \                }\n            }\n        }\n    }\n\n     terminal_map\n}\n\n\
    pub fn assign_eof_terminal_id(terminal_map: &mut BiBTreeMap<Terminal, TerminalID>)\
    \ {\n     if !terminal_map.contains_left(&Terminal(\"$\".to_string())) {\n   \
    \     let max_terminal_id = terminal_map.right_values().max().unwrap().0;\n  \
    \      terminal_map.insert(Terminal(\"$\".to_string()), TerminalID(max_terminal_id\
    \ + 1));\n    }\n}\n\npub fn assign_non_terminal_ids(productions: &[Production])\
    \ -> BiBTreeMap<NonTerminal, NonTerminalID> {\n    let mut non_terminal_map =\
    \ BiBTreeMap::new();\n    let mut next_non_terminal_id = 0;\n\n    for p in productions\
    \ {\n        if !non_terminal_map.contains_left(&p.lhs) {\n            non_terminal_map.insert(p.lhs.clone(),\
    \ NonTerminalID(next_non_terminal_id));\n            next_non_terminal_id += 1;\n\
    \        }\n    }\n    non_terminal_map\n}\n```\n\n```rust id=\"727a024a\"\nuse\
    \ crate::glr::grammar::{nt, prod, t, Terminal};\nuse crate::glr::parser::GLRParser;\n\
    use crate::glr::table::{generate_glr_parser, TerminalID};\n\nfn create_simple_parser()\
    \ -> GLRParser {\n    let productions = vec![\n        prod(\"S\", vec![nt(\"\
    A\")]),\n        prod(\"A\", vec![nt(\"A\"), t(\"a\")]),\n        prod(\"A\",\
    \ vec![t(\"b\")]),\n    ];\n\n    generate_glr_parser(&productions, 0)\n}\n\n\
    fn create_expression_parser() -> GLRParser {\n    let productions = vec![\n  \
    \      prod(\"S\", vec![nt(\"E\")]),\n        prod(\"E\", vec![nt(\"E\"), t(\"\
    +\"), nt(\"T\")]),\n        prod(\"E\", vec![nt(\"T\")]),\n        prod(\"T\"\
    , vec![nt(\"T\"), t(\"*\"), nt(\"F\")]),\n        prod(\"T\", vec![nt(\"F\")]),\n\
    \        prod(\"F\", vec![t(\"(\"), nt(\"E\"), t(\")\")]),\n        prod(\"F\"\
    , vec![t(\"i\")]),\n    ];\n\n    generate_glr_parser(&productions, 0)\n}\n\n\
    fn tokenize(parser: &GLRParser, input: &str) -> Vec<TerminalID> {\n     input.chars()\n\
    \        .filter_map(|c| parser.terminal_map.get_by_left(&Terminal(c.to_string())).copied())\n\
    \        .collect()\n}\n\n#[test]\nfn test_simple_parse_table() {\n    let parser\
    \ = create_simple_parser();\n    \n    let test_cases = [\n        (\"b\", true),\n\
    \        (\"ba\", true),\n        (\"baa\", true),\n        (\"a\", false),\n\
    \        (\"bb\", false),\n    ];\n\n    for (input, expected_match) in test_cases\
    \ {\n        let result = parser.parse(&tokenize(&parser, input));\n        assert_eq!(\n\
    \            result.fully_matches(), \n            expected_match, \n        \
    \    \"Failed for input: {}\", input\n        );\n    }\n}\n\n#[test]\nfn test_parse_simple_expression()\
    \ {\n    let parser = create_expression_parser();\n    \n    let test_cases =\
    \ [\n        (\"i\", true),\n        (\"i+i*i\", true),\n        (\"i+i\", true),\n\
    \        (\"i*i\", true),\n        (\"(i+i)*i\", true),\n        (\"i+\", false),\n\
    \        (\"i++i\", false),\n        (\"\", false),\n        (\")\", false),\n\
    \    ];\n\n    for (input, expected_match) in test_cases {\n        let result\
    \ = parser.parse(&tokenize(&parser, input));\n        assert_eq!(\n          \
    \  result.fully_matches(), \n            expected_match, \n            \"Failed\
    \ for input: {}\", input\n        );\n    }\n}\n```\n\n```rust id=\"8111e0c4\"\
    \nuse crate::finite_automata::{Expr, QuantifierType};\nuse crate::u8set::U8Set;\n\
    \npub fn seq_fast(parsers: Vec<Expr>) -> Expr {\n    Expr::Seq(parsers)\n}\n\n\
    pub fn choice_fast(parsers: Vec<Expr>) -> Expr {\n    Expr::Choice(parsers)\n\
    }\n\npub fn opt_fast(parser: Expr) -> Expr {\n    Expr::Choice(vec![parser, Expr::Seq(vec![])])\n\
    }\n\npub fn repeat1_fast(parser: Expr) -> Expr {\n    Expr::Quantifier(Box::new(parser),\
    \ QuantifierType::OneOrMore)\n}\n\npub fn eat_u8_fast(byte: u8) -> Expr {\n  \
    \  Expr::U8Seq(vec![byte])\n}\n\npub fn eat_u8_negation_fast(byte: u8) -> Expr\
    \ {\n     Expr::U8Class(U8Set::from_byte(byte).complement())\n}\n\npub fn eat_u8_choice_fast(bytes:\
    \ &[u8]) -> Expr {\n     Expr::U8Class(U8Set::from_bytes(bytes))\n}\n\npub fn\
    \ eat_u8_negation_choice_fast(bytes: &[u8]) -> Expr {\n    Expr::U8Class(U8Set::from_bytes(bytes).complement())\n\
    }\n\npub fn eat_u8_range_fast(start: u8, end: u8) -> Expr {\n    Expr::U8Class(U8Set::from_byte_range(start..=end))\n\
    }\n\npub fn eat_char_fast(c: char) -> Expr {\n     Expr::U8Seq(vec![c as u8])\n\
    }\n\npub fn eat_char_negation_fast(c: char) -> Expr {\n    Expr::U8Class(U8Set::from_char(c).complement())\n\
    }\n\npub fn eat_char_choice_fast(s: &str) -> Expr {\n     Expr::U8Class(U8Set::from_chars(s))\n\
    }\n\npub fn eat_char_negation_choice_fast(s: &str) -> Expr {\n   Expr::U8Class(U8Set::from_chars(s).complement())\n\
    }\n\npub fn eat_string_fast(s: &str) -> Expr {\n    Expr::U8Seq(s.bytes().collect())\n\
    }\n\npub fn eat_byte_range_fast(start: u8, end: u8) -> Expr {\n    Expr::U8Class(U8Set::from_byte_range(start..=end))\n\
    }\n\npub fn eat_bytestring_choice_fast(bytestrings: Vec<Vec<u8>>) -> Expr {\n\
    \    let children: Vec<Expr> = bytestrings\n        .into_iter()\n        .map(eat_bytestring_fast)\n\
    \        .collect();\n    choice_fast(children)\n}\n\npub fn eat_bytestring_fast(bytes:\
    \ Vec<u8>) -> Expr {\n     Expr::U8Seq(bytes)\n}\n\npub fn eat_string_choice_fast(strings:\
    \ &[&str]) -> Expr {\n     choice_fast(strings.iter().map(|s| eat_string_fast(s)).collect())\n\
    }\n\npub fn repeat0_fast(parser: Expr) -> Expr {\n     opt_fast(repeat1_fast(parser))\n\
    }\n\npub fn seprep1_fast(a: Expr, b: Expr) -> Expr {\n    seq_fast(vec![a.clone(),\
    \ repeat0_fast(seq_fast(vec![b, a]))])\n}\n\npub fn seprep0_fast(a: Expr, b: Expr)\
    \ -> Expr {\n    opt_fast(seprep1_fast(a, b))\n}\n\npub fn repeatn_fast(n: usize,\
    \ parser: Expr) -> Expr {\n     if n == 0 {\n        return seq_fast(vec![]);\n\
    \    }\n     let parsers = std::iter::repeat(parser).take(n).collect();\n    seq_fast(parsers)\n\
    }\n\n#[macro_export]\nmacro_rules! seq_fast {\n    ($($x:expr),* $(,)?) => {\n\
    \        $crate::tokenizer_combinators::seq_fast(vec![$($x),*])\n    };\n}\n\n\
    #[macro_export]\nmacro_rules! choice_fast {\n    ($($x:expr),* $(,)?) => {\n \
    \       $crate::tokenizer_combinators::choice_fast(vec![$($x),*])\n    };\n}\n\
    ```\n\n```rust id=\"52a2f162\"\nuse std::collections::{HashMap, HashSet, VecDeque};\n\
    use std::collections::BTreeMap;\nuse std::fmt::Debug;\nuse std::rc::Rc;\nuse std::sync::{Arc,\
    \ Mutex};\nuse kdam::term::init;\n\n#[derive(Debug, Clone)]\npub struct TrieNode<E,\
    \ T> {\n    pub value: T,\n     children: BTreeMap<E, Arc<Mutex<TrieNode<E, T>>>>,\n\
    \    num_parents: usize,\n}\n\nimpl<T, E: Ord> TrieNode<E, T> {\n    pub fn new(value:\
    \ T) -> TrieNode<E, T> {\n        TrieNode {\n            value,\n           \
    \ children: BTreeMap::new(),\n            num_parents: 0,\n        }\n    }\n\n\
    \    pub fn insert(&mut self, edge: E, child: Arc<Mutex<TrieNode<E, T>>>) -> Option<Arc<Mutex<TrieNode<E,\
    \ T>>>> {\n         child.try_lock().unwrap().num_parents += 1;\n        if let\
    \ Some(existing_child) = self.children.insert(edge, child) {\n            existing_child.try_lock().unwrap().num_parents\
    \ -= 1;\n            Some(existing_child)\n         } else {\n            None\n\
    \        }\n    }\n\n    pub fn insert_with(&mut self, edge: E, child: Arc<Mutex<TrieNode<E,\
    \ T>>>, combine: impl FnOnce(&mut T, T)) {\n        todo!()\n    }\n\n    pub\
    \ fn get(&self, edge: &E) -> Option<Arc<Mutex<TrieNode<E, T>>>> {\n        self.children.get(edge).cloned()\n\
    \    }\n\n    pub fn children(&self) -> &BTreeMap<E, Arc<Mutex<TrieNode<E, T>>>>\
    \ {\n        &self.children\n    }\n\n    pub fn is_empty(&self) -> bool {\n \
    \       self.children.is_empty()\n    }\n\n    pub fn deep_clone(&self) -> Arc<Mutex<TrieNode<E,\
    \ T>>> where T: Clone, E: Clone {\n        let mut new_children = BTreeMap::new();\n\
    \        for (edge, child) in &self.children {\n            let new_child = child.try_lock().unwrap().deep_clone();\n\
    \            new_children.insert(edge.clone(), new_child);\n        }\n      \
    \  Arc::new(Mutex::new(TrieNode {\n            value: self.value.clone(),\n  \
    \          children: new_children,\n            num_parents: 0,\n        }))\n\
    \    }\n\n     pub fn shallow_clone(&self) -> Arc<Mutex<TrieNode<E, T>>> where\
    \ T: Clone, E: Clone {\n         let mut new_children = BTreeMap::new();\n   \
    \      for (edge, child) in self.children.clone() {\n            new_children.insert(edge,\
    \ child);\n        }\n         Arc::new(Mutex::new(TrieNode {\n             value:\
    \ self.value.clone(),\n             children: new_children,\n            num_parents:\
    \ 0,\n        }))\n    }\n\n    pub fn replace_child_with_clone(&mut self, edge:\
    \ &E) -> Arc<Mutex<TrieNode<E, T>>> where T: Clone, E: Clone {\n        let child\
    \ = self.children.get(edge).unwrap();\n         let new_child = child.try_lock().unwrap().shallow_clone();\n\
    \        self.insert(edge.clone(), new_child.clone());\n        new_child\n  \
    \  }\n\n    pub fn all_nodes(root: Arc<Mutex<TrieNode<E, T>>>) -> Vec<Arc<Mutex<TrieNode<E,\
    \ T>>>> {\n         let mut node_ptrs_in_order: Vec<*const TrieNode<E, T>> = Vec::new();\n\
    \        let mut nodes: BTreeMap<*const TrieNode<E, T>, Arc<Mutex<TrieNode<E,\
    \ T>>>> = BTreeMap::new();\n         let mut queue: VecDeque<Arc<Mutex<TrieNode<E,\
    \ T>>>> = VecDeque::new();\n        queue.push_back(root);\n        while let\
    \ Some(node) = queue.pop_front() {\n            if node_ptrs_in_order.contains(&(&*node.try_lock().unwrap()\
    \ as *const TrieNode<E, T>)) {\n                continue;\n            }\n   \
    \          node_ptrs_in_order.push(&*node.try_lock().unwrap() as *const TrieNode<E,\
    \ T>);\n             nodes.insert(&*node.try_lock().unwrap() as *const TrieNode<E,\
    \ T>, node.clone());\n            let node = node.try_lock().unwrap();\n     \
    \        for (_, child) in &node.children {\n                 queue.push_back(child.clone());\n\
    \            }\n        }\n         node_ptrs_in_order.into_iter().map(|ptr| nodes.get(&ptr).unwrap().clone()).collect()\n\
    \    }\n\n    pub fn map_t<F, U>(self, f: F) -> Arc<Mutex<TrieNode<E, U>>>\n \
    \   where\n        T: Clone,\n        E: Clone,\n        F: Copy + Fn(T) -> U,\n\
    \    {\n        let mut active_states: Vec<(Arc<Mutex<TrieNode<E, T>>>, Arc<Mutex<TrieNode<E,\
    \ U>>>)> = Vec::new();\n         let mut dormant_states: HashMap<*const TrieNode<E,\
    \ T>, (usize, Arc<Mutex<TrieNode<E, U>>>)> = HashMap::new();\n        let root\
    \ = Arc::new(Mutex::new(TrieNode::new(f(self.value.clone()))));\n        active_states.push((Arc::new(Mutex::new(self)),\
    \ root.clone()));\n\n         while let Some((node, new_node)) = active_states.pop()\
    \ {\n            let node = node.try_lock().unwrap();\n            for (edge,\
    \ child_arc) in &node.children {\n                 let child = child_arc.try_lock().unwrap();\n\
    \                if let Some((num_parents_seen, new_child)) = dormant_states.get_mut(&(&*child\
    \ as *const TrieNode<E, T>)) {\n                     new_node.try_lock().unwrap().insert(edge.clone(),\
    \ new_child.clone());\n                    *num_parents_seen += 1;\n         \
    \            if *num_parents_seen == child.num_parents {\n                   \
    \     active_states.push((child_arc.clone(), new_child.clone()));\n          \
    \          }\n                 } else {\n                    let new_child = Arc::new(Mutex::new(TrieNode::new(f(child.value.clone()))));\n\
    \                     new_node.try_lock().unwrap().insert(edge.clone(), new_child.clone());\n\
    \                     if child.num_parents == 1 {\n                        active_states.push((child_arc.clone(),\
    \ new_child.clone()));\n                    } else {\n                       \
    \ dormant_states.insert(&*child as *const TrieNode<E, T>, (1, new_child.clone()));\n\
    \                     }\n                }\n             }\n        }\n\n    \
    \     root\n    }\n\n    pub fn flatten<F>(&self, is_terminal: F) -> BTreeMap<Vec<E>,\
    \ T>\n    where\n        E: Clone,\n        T: Clone,\n        F: Copy + Fn(&T)\
    \ -> bool,\n    {\n        let mut result = BTreeMap::new();\n        self.flatten_recursive(&mut\
    \ result, Vec::new(), is_terminal);\n        result\n    }\n\n    fn flatten_recursive<F>(\n\
    \        &self,\n        result: &mut BTreeMap<Vec<E>, T>,\n        path: Vec<E>,\n\
    \        is_terminal: F,\n    )\n    where\n         E: Clone,\n        T: Clone,\n\
    \        F: Copy + Fn(&T) -> bool,\n    {\n        if is_terminal(&self.value)\
    \ {\n            result.insert(path.clone(), self.value.clone());\n        }\n\
    \        for (edge, child) in &self.children {\n            let mut new_path =\
    \ path.clone();\n             new_path.push(edge.clone());\n            child.try_lock().unwrap().flatten_recursive(result,\
    \ new_path, is_terminal);\n        }\n    }\n\n    fn can_reach(&self, target:\
    \ *const TrieNode<E, T>) -> bool {\n        let mut visited = HashSet::new();\n\
    \         let mut queue = VecDeque::new();\n\n        queue.push_back(self as\
    \ *const TrieNode<E, T>);\n        visited.insert(self as *const TrieNode<E, T>);\n\
    \n        while let Some(current_ptr) = queue.pop_front() {\n           if current_ptr\
    \ == target {\n                return true;\n            }\n\n             unsafe\
    \ {\n                if let Some(current) = current_ptr.as_ref() {\n         \
    \           for child in current.children.values() {\n                       \
    \  let child_ptr = &*child.try_lock().unwrap() as *const TrieNode<E, T>;\n   \
    \                     if visited.insert(child_ptr) {\n                       \
    \     queue.push_back(child_ptr);\n                        }\n               \
    \     }\n                }\n            }\n        }\n         false\n    }\n\
    }\n\n\nimpl<T: Clone, E: Ord + Clone> TrieNode<E, T> {\n   pub fn special_map<V>(\n\
    \        initial_node: Arc<Mutex<TrieNode<E, T>>>,\n        initial_value: V,\n\
    \        mut step: impl FnMut(&V, &E, &TrieNode<E, T>) -> V,\n        mut merge:\
    \ impl FnMut(Vec<V>) -> V,\n        mut process: impl FnMut(&T, &V),\n    ) where\n\
    \        V: Clone,\n        E: Ord,\n    {\n        let mut active_states: VecDeque<(Arc<Mutex<TrieNode<E,\
    \ T>>>, V)> = VecDeque::new();\n        let mut dormant_states: HashMap<*const\
    \ TrieNode<E, T>, Vec<V>> = HashMap::new();\n\n        active_states.push_back((initial_node.clone(),\
    \ initial_value));\n\n        while let Some((node_arc, value)) = active_states.pop_front()\
    \ {\n           let node = node_arc.try_lock().unwrap();\n\n           process(&node.value,\
    \ &value);\n          for (edge, child_arc) in &node.children {\n            \
    \    let child = child_arc.try_lock().unwrap();\n                let new_value\
    \ = step(&value, edge, &child);\n                let child_ptr = &*child as *const\
    \ TrieNode<E, T>;\n                let entry = dormant_states.entry(child_ptr).or_default();\n\
    \                entry.push(new_value.clone());\n\n                 if entry.len()\
    \ == child.num_parents {\n                     let merged_value = merge(entry.clone());\n\
    \                    dormant_states.remove(&child_ptr);\n                    active_states.push_back((child_arc.clone(),\
    \ merged_value));\n                }\n            }\n        }\n\n         if\
    \ !dormant_states.is_empty() {\n             for (node_ptr, values) in &dormant_states\
    \ {\n                println!(\"dormant state: {:?}\", node_ptr)\n           \
    \ }\n            panic!(\"Leftover dormant states\");\n        }\n    }\n\n  \
    \   pub fn merge<T2>(\n        node: Arc<Mutex<TrieNode<E, T>>>,\n        other:\
    \ Arc<Mutex<TrieNode<E, T2>>>,\n        t_merge: impl Fn(T, T2) -> T,\n      \
    \  t_init: impl Fn() -> T,\n    )\n    where\n        T2: Clone,\n    {\n    \
    \     let mut node_map: HashMap<*const TrieNode<E, T2>, Arc<Mutex<TrieNode<E,\
    \ T>>>> = HashMap::new();\n         let mut already_merged_values: HashSet<*const\
    \ TrieNode<E, T>> = HashSet::new();\n\n        let existing_value = node.try_lock().unwrap().value.clone();\n\
    \        let new_value = t_merge(existing_value, other.try_lock().unwrap().value.clone());\n\
    \        node.try_lock().unwrap().value = new_value;\n\n        TrieNode::special_map(\n\
    \             other.clone(),\n            vec![node.clone()],\n             |current_nodes:\
    \ &Vec<Arc<Mutex<TrieNode<E, T>>>>, edge: &E, dest_other_node: &TrieNode<E, T2>|\
    \ {\n                let mut new_nodes = Vec::new();\n\n                for current_self_node\
    \ in current_nodes {\n                     let mut current_self_node_guard = current_self_node.try_lock().unwrap();\n\
    \                    if let Some(child) = current_self_node_guard.get(edge) {\n\
    \                         if !already_merged_values.contains(&(&*child.try_lock().unwrap()\
    \ as *const TrieNode<E, T>)) {\n                            let child_value =\
    \ child.try_lock().unwrap().value.clone();\n                             let merged_value\
    \ = t_merge(child_value, dest_other_node.value.clone());\n                   \
    \         child.try_lock().unwrap().value = merged_value;\n                  \
    \      }\n                        new_nodes.push(child);\n                   \
    \ } else {\n                        let other_node_ptr = dest_other_node as *const\
    \ TrieNode<E, T2>;\n                        if let Some(mapped_node) = node_map.get(&other_node_ptr)\
    \ {\n                             current_self_node_guard.insert(edge.clone(),\
    \ mapped_node.clone());\n                             new_nodes.push(mapped_node.clone());\n\
    \                        } else {\n                            let new_node =\
    \ Arc::new(Mutex::new(TrieNode::new(t_merge(t_init(), dest_other_node.value.clone()))));\n\
    \                             current_self_node_guard.insert(edge.clone(), new_node.clone());\n\
    \                            node_map.insert(other_node_ptr, new_node.clone());\n\
    \                             new_nodes.push(new_node);\n                    \
    \    }\n                    }\n                }\n\n                new_nodes\n\
    \            },\n             |values: Vec<Vec<Arc<Mutex<TrieNode<E, T>>>>>| {\n\
    \                let mut merged_nodes = Vec::new();\n                 let mut\
    \ seen = HashSet::new();\n\n                for value in values {\n          \
    \           for node in value {\n                         let node_ptr = Arc::as_ptr(&node);\n\
    \                         if seen.insert(node_ptr) {\n                       \
    \      merged_nodes.push(node);\n                        }\n                 \
    \   }\n                }\n\n                 merged_nodes\n            },\n  \
    \          |_, _| {}\n        );\n    }\n}\n\npub(crate) fn dump_structure<E,\
    \ T>(root: Arc<Mutex<TrieNode<E, T>>>) where E: Debug, T: Debug {\n     let mut\
    \ queue: VecDeque<Arc<Mutex<TrieNode<E, T>>>> = VecDeque::new();\n    let mut\
    \ seen: HashSet<*const TrieNode<E, T>> = HashSet::new();\n\n    queue.push_back(root);\n\
    \n     while let Some(node) = queue.pop_front() {\n        let node = node.try_lock().unwrap();\n\
    \         let node_ptr = &*node as *const TrieNode<E, T>;\n        println!(\"\
    {:?}: num_parents: {}\", node_ptr, node.num_parents);\n         for (edge, child)\
    \ in &node.children {\n            let child_ptr = &*child.try_lock().unwrap()\
    \ as *const TrieNode<E, T>;\n            println!(\"  - {:?} -> {:?}\", edge,\
    \ child_ptr);\n             if !seen.contains(&child_ptr) {\n                seen.insert(child_ptr);\n\
    \                 queue.push_back(child.clone());\n            }\n        }\n\
    \    }\n}\n\n#[cfg(test)]\nmod tests {\n     use std::sync::{Arc, Mutex};\n  \
    \  use crate::trie::{dump_structure, TrieNode};\n\n     #[test]\n    fn test_trie()\
    \ {\n        let mut a = TrieNode::new(\"a\");\n         let mut b = TrieNode::new(\"\
    b\");\n        let c = TrieNode::new(\"c\");\n\n         b.insert(\"b->c\", Arc::new(Mutex::new(c)));\n\
    \        a.insert(\"a->b\", Arc::new(Mutex::new(b)));\n\n        let mut a2 =\
    \ TrieNode::new(\"a\");\n         let mut b2 = TrieNode::new(\"b\");\n       \
    \ let d = TrieNode::new(\"d\");\n\n         b2.insert(\"b->d\", Arc::new(Mutex::new(d)));\n\
    \         a2.insert(\"a->b\", Arc::new(Mutex::new(b2)));\n\n        let a = Arc::new(Mutex::new(a));\n\
    \         let a2 = Arc::new(Mutex::new(a2));\n\n        println!(\"a structure\
    \ (before)\");\n        dump_structure(a.clone());\n        println!(\"a2 structure\
    \ (before)\");\n        dump_structure(a2.clone());\n\n        let merged = TrieNode::merge(\n\
    \             a.clone(),\n             a2.clone(),\n             |x, y| { if x.is_empty()\
    \ { y } else { assert_eq!(x, y); x } },\n             || { \"\" }\n        );\n\
    \n        println!(\"a structure (after)\");\n        dump_structure(a.clone());\n\
    \         println!(\"a2 structure (after)e\");\n         dump_structure(a2.clone());\n\
    \    }\n}\n```\n\n```rust id=\"f4c5f846\"\n#[macro_export]\nmacro_rules! debug\
    \ {\n    ($level:expr, $($arg:tt)*) => {{\n        pub const DEBUG_LEVEL: usize\
    \ = 1;\n        if $level <= DEBUG_LEVEL {\n            #[cfg(feature = \"debug\"\
    )]\n            println!(\"[DEBUG {}] {}\", $level, format!($($arg)*));\n    \
    \    }\n    }};\n}\n```\n\n**Summary of Changes:**\n\n*   **Removed Redundancy:**\
    \  Simplified loops and logic to achieve the same results with fewer lines of\
    \ code.\n*   **Clarity:** Improved variable names and used more idiomatic Rust\
    \ constructs.\n*   **Conciseness:** Replaced verbose code with more concise alternatives\
    \ (e.g., using iterators with `map`, `filter_map`, `collect` for transformations).\n\
    *   **Readability:** Improved formatting to enhance readability.\n*   **Efficiency:**\
    \ Made minor improvements to avoid unnecessary clones where possible.\n\n**Key\
    \ Refinements:**\n\n*   **`analyze_grammar.rs`:** Replaced manual iteration with\
    \ higher-level iterator operations for computing non-terminal sets.\n*   **`constraint.rs`:**\
    \  Streamlined the `get_mask` and `commit` methods, avoiding unnecessary temporary\
    \ variables and making better use of iterators.\n*    **`grammar.rs`:** Improved\
    \ readability of FIRST and FOLLOW set computation\n*   **`gss.rs`:**  Removed\
    \ redundant `clone` calls and made `BulkMerge` more readable.\n*   **`interface.rs`:**\
    \  Removed unnecessary intermediate variables when building the tokenizer.\n*\
    \   **`parser.rs`:** Made minor adjustments to `GLRParserState` to avoid redundant\
    \ code and use `clone()` more efficiently.\n*   **`precompute.rs`:**  Simplified\
    \ the loop in `precompute_add_eof`.\n*   **`trie.rs`:** Code cleanup and used\
    \ more functional idioms.\n\nThis refactoring aims to make the codebase more maintainable\
    \ and easier to understand, without changing the core functionality.  The todos\
    \ are preserved as comments for future implementation."
